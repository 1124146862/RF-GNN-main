{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839f95dd4785f5af",
   "metadata": {},
   "source": [
    "### util function"
   ]
  },
  {
   "cell_type": "code",
   "id": "2569111fcf584f28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T16:05:44.847023Z",
     "start_time": "2025-02-12T16:05:44.667473Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Standardize the input data\n",
    "def standard_input(X):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    # 保留原有索引，便于后续处理\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "    return X_scaled_df\n",
    "\n",
    "\n",
    "def load_data_DEF(random_state=42):\n",
    "    \"\"\"\n",
    "    从 CSV 文件中加载数据，并划分训练、验证、测试集，同时构造节点 mask\n",
    "    \"\"\"\n",
    "    # CSV 文件路径（请根据实际情况修改）\n",
    "    path = '/home/gehongfei/project/TabGNN/dataset/DEF.csv'\n",
    "    df = pd.read_csv(path, sep=',')\n",
    "    \n",
    "    target_col = 'label'\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"Error: '{target_col}' column not found in the dataset.\")\n",
    "        return None, None, None, None, None, None, None, None, None, None, None\n",
    "    \n",
    "    y = df[target_col]\n",
    "    if \"ID\" in df.columns:\n",
    "        X = df.drop(columns=[\"ID\", target_col])\n",
    "    else:\n",
    "        X = df.drop(columns=[target_col])\n",
    "    \n",
    "    # 划分训练、验证和测试集（采用 stratify 保证标签分布均衡）\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=random_state, stratify=y\n",
    "    )\n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=2/3, random_state=random_state, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # 创建节点 mask（假设每一行数据代表图中的一个节点）\n",
    "    num_nodes = len(df)\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask   = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask  = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[X_train.index] = True\n",
    "    val_mask[X_valid.index]   = True\n",
    "    test_mask[X_test.index]   = True\n",
    "    \n",
    "    # 标准化数据\n",
    "    X = standard_input(X)\n",
    "    X_train = standard_input(X_train)\n",
    "    X_valid = standard_input(X_valid)\n",
    "    X_test  = standard_input(X_test)\n",
    "    \n",
    "    return X, y, X_train, X_valid, X_test, y_train, y_valid, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "# ---------------------- 构造联合邻接矩阵（不增加额外原型节点） ----------------------\n",
    "def compute_adjacency_matrix_by_prototypes(X_train, X_valid, X_test, y_train, y_valid,\n",
    "                                           n_clusters=1000, n_estimators=50, max_depth=None,\n",
    "                                           random_state=42, cluster_threshold=0, proto_threshold=0):\n",
    "    \"\"\"\n",
    "    主要步骤：\n",
    "      1. 在训练+验证集上用 KMeans 聚类，得到 n_clusters 个簇中心，并打印簇内样本数分布；\n",
    "      2. 对测试集样本利用余弦相似度分配到各簇；\n",
    "      3. 构造全集 X_all（所有样本）及其 cluster_assignments（长度 N，与 X_all 行数一致）；\n",
    "      4. 对每个簇，从该簇中优先选取训练+验证样本中距离簇中心最近的节点作为该簇代表（原型），记录其索引（prototype_indices）；\n",
    "      5. 对于每个簇内部，利用该簇中训练+验证数据训练 RF，计算簇内所有节点之间的 RF 叶节点相似度（S_local），经过阈值过滤；\n",
    "      6. 对每个簇，强制将簇内所有节点与代表节点连边，构成连接矩阵 S_connect（边权设为 1）；\n",
    "      7. 对所有簇的代表节点，利用其原始特征训练 RF，计算代表节点之间的 RF 相似度（S_proto），经过 proto_threshold 过滤后构成 S_proto_edges；\n",
    "      8. 最终联合邻接矩阵 A 为：A = max(S_local, S_connect) 与 S_proto_edges 的并集，且 A 的尺寸为 N×N（N 为所有样本数）。\n",
    "      \n",
    "    返回：\n",
    "      - adj_matrix: csr_matrix 格式，形状为 (N, N)\n",
    "      - cluster_assignments: ndarray，长度 N，每个样本的聚类编号\n",
    "      - prototype_indices: ndarray，长度 n_clusters，每个簇代表的样本在 X_all 中的索引（注意：这些索引为位置索引）\n",
    "      - n_clusters: 簇的数量\n",
    "    \"\"\"\n",
    "    # -------------（1）在训练+验证上聚类 -------------\n",
    "    X_tv = pd.concat([X_train, X_valid])\n",
    "    y_tv = pd.concat([y_train, y_valid])\n",
    "    X_tv_np = X_tv.values if isinstance(X_tv, pd.DataFrame) else np.array(X_tv)\n",
    "    \n",
    "    print(\"开始对训练+验证集进行 KMeans 聚类 ...\")\n",
    "    # 显式设置 n_init=10 避免 FutureWarning\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)\n",
    "    kmeans.fit(X_tv_np)\n",
    "    cluster_labels_tv = kmeans.labels_   # 长度 = len(X_train)+len(X_valid)\n",
    "    centers = kmeans.cluster_centers_      # shape = (n_clusters, n_features)\n",
    "    \n",
    "    # 打印各簇样本数分布\n",
    "    cluster_counts = Counter(cluster_labels_tv)\n",
    "    print(\"训练+验证集聚类样本数分布：\")\n",
    "    for cid, cnt in sorted(cluster_counts.items()):\n",
    "        print(f\"  Cluster {cid}: {cnt} samples\")\n",
    "    \n",
    "    # -------------（2）对测试集样本分配簇 -------------\n",
    "    X_test_np = X_test.values if isinstance(X_test, pd.DataFrame) else np.array(X_test)\n",
    "    sims = cosine_similarity(X_test_np, centers)   # shape = (n_test, n_clusters)\n",
    "    test_cluster_assignments = np.argmax(sims, axis=1)\n",
    "    \n",
    "    # -------------（3）构造全集数据 X_all 与对应标签 -------------\n",
    "    # 这里不调用 reset_index，以保留原始索引信息，但后续的索引操作均使用 .iloc（位置索引）\n",
    "    X_all = pd.concat([X_train, X_valid, X_test]).sort_index()\n",
    "    dummy_y_test = pd.Series([-1] * len(X_test), index=X_test.index)\n",
    "    y_all = pd.concat([y_train, y_valid, dummy_y_test]).sort_index()\n",
    "    N = len(X_all)\n",
    "    \n",
    "    # 构造 cluster_assignments，利用 X_all 的原始索引（标签）\n",
    "    cluster_dict = {}\n",
    "    for idx, label in zip(X_tv.index, cluster_labels_tv):\n",
    "        cluster_dict[idx] = label\n",
    "    for idx, label in zip(X_test.index, test_cluster_assignments):\n",
    "        cluster_dict[idx] = label\n",
    "    # 根据 X_all 的索引构造 cluster_assignments 数组\n",
    "    cluster_assignments = np.array([cluster_dict[idx] for idx in X_all.index])\n",
    "    \n",
    "    # 构造一个布尔数组 mask_tv 指示哪些样本属于训练+验证（用于 RF 训练）\n",
    "    # 注意：X_all 是按原始索引排序的，需将训练+验证集原始索引转换为位置索引\n",
    "    mask_tv = np.zeros(N, dtype=bool)\n",
    "    tv_indices = list(X_train.index) + list(X_valid.index)\n",
    "    tv_pos = [X_all.index.get_loc(idx) for idx in tv_indices]\n",
    "    mask_tv[tv_pos] = True\n",
    "\n",
    "    # -------------（4）选择每个簇的代表（原型）-------------\n",
    "    prototype_indices = np.zeros(n_clusters, dtype=int)\n",
    "    prototype_labels = []\n",
    "    X_all_np = X_all.values  # shape = (N, n_features)\n",
    "    for i in range(n_clusters):\n",
    "        idx_in_cluster = np.where(cluster_assignments == i)[0]\n",
    "        if len(idx_in_cluster) == 0:\n",
    "            # 极少情况：若簇为空，随便选择一个索引（后续可做特殊处理）\n",
    "            prototype_indices[i] = 0\n",
    "            prototype_labels.append(y_all.iloc[0])\n",
    "            continue\n",
    "        # 优先选择训练+验证中的样本\n",
    "        idx_in_tv = [j for j in idx_in_cluster if mask_tv[j]]\n",
    "        if len(idx_in_tv) == 0:\n",
    "            idx_in_tv = idx_in_cluster  # 若该簇中无训练数据，则全部考虑\n",
    "        # 计算各候选样本与该簇中心的欧式距离\n",
    "        center = centers[i]\n",
    "        candidates = X_all_np[idx_in_tv]\n",
    "        dists = np.linalg.norm(candidates - center, axis=1)\n",
    "        best_local_idx = idx_in_tv[np.argmin(dists)]\n",
    "        prototype_indices[i] = best_local_idx\n",
    "        # 多数投票获得该簇的标签（仅考虑训练+验证）\n",
    "        labels = y_all.iloc[idx_in_tv].values\n",
    "        most_common = Counter(labels).most_common(1)[0][0]\n",
    "        prototype_labels.append(most_common)\n",
    "    prototype_labels = np.array(prototype_labels)\n",
    "    \n",
    "    # -------------（5）计算簇内局部相似度 S_local -------------\n",
    "    S_local = np.zeros((N, N))\n",
    "    print(\"计算各簇内部相似度 S_local ...\")\n",
    "    for c in tqdm(range(n_clusters), desc=\"簇内 S_local\"):\n",
    "        idx = np.where(cluster_assignments == c)[0]\n",
    "        if len(idx) < 2:\n",
    "            continue\n",
    "        # 仅使用训练+验证数据训练 RF\n",
    "        idx_tv = [j for j in idx if mask_tv[j]]\n",
    "        if len(idx_tv) < 1:\n",
    "            continue\n",
    "        # 注意：这里使用 .iloc 按位置索引\n",
    "        X_cluster_train = X_all.iloc[idx_tv].values\n",
    "        y_cluster_train = y_all.iloc[idx_tv].values\n",
    "        rf_cluster = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n",
    "        rf_cluster.fit(X_cluster_train, y_cluster_train)\n",
    "        # 对簇内所有样本（包括测试）计算 RF 叶节点编号\n",
    "        X_cluster_all = X_all.iloc[idx].values\n",
    "        leaves = rf_cluster.apply(X_cluster_all)  # shape = (n_samples_in_cluster, n_estimators)\n",
    "        sim_matrix = np.zeros((len(idx), len(idx)))\n",
    "        for tree in range(n_estimators):\n",
    "            tree_leaves = leaves[:, tree]\n",
    "            sim_matrix += (tree_leaves[:, None] == tree_leaves[None, :]).astype(float)\n",
    "        sim_matrix /= n_estimators\n",
    "        # 过滤低于 cluster_threshold 的相似度\n",
    "        sim_matrix = np.where(sim_matrix > cluster_threshold, sim_matrix, 0)\n",
    "        # 写入 S_local 的对应子矩阵\n",
    "        S_local[np.ix_(idx, idx)] = sim_matrix\n",
    "    \n",
    "    # -------------（6）构造簇内原型–成员连接 S_connect -------------\n",
    "    S_connect = np.zeros((N, N))\n",
    "    for c in range(n_clusters):\n",
    "        idx = np.where(cluster_assignments == c)[0]\n",
    "        p = prototype_indices[c]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        # 将该簇内所有节点与代表节点连边（边权 1）\n",
    "        S_connect[p, idx] = 1.0\n",
    "        S_connect[idx, p] = 1.0\n",
    "        S_connect[p, p] = 0.0  # 排除自环\n",
    "    \n",
    "    # -------------（7）计算代表节点之间的相似度 S_proto_edges -------------\n",
    "    # 注意：prototype_indices 中保存的是位置索引，故需用 .iloc 访问\n",
    "    X_proto = X_all.iloc[prototype_indices].values\n",
    "    rf_proto = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n",
    "    rf_proto.fit(X_proto, prototype_labels)\n",
    "    leaves_proto = rf_proto.apply(X_proto)  # shape = (n_clusters, n_estimators)\n",
    "    S_proto = np.zeros((n_clusters, n_clusters))\n",
    "    for tree in range(n_estimators):\n",
    "        tree_leaves = leaves_proto[:, tree]\n",
    "        S_proto += (tree_leaves[:, None] == tree_leaves[None, :]).astype(float)\n",
    "    S_proto /= n_estimators\n",
    "    # 过滤低于 proto_threshold 的相似度，并构造 S_proto_edges（映射到原始节点位置）\n",
    "    S_proto_edges = np.zeros((N, N))\n",
    "    for i in range(n_clusters):\n",
    "        for j in range(i+1, n_clusters):\n",
    "            if S_proto[i, j] > proto_threshold:\n",
    "                u = prototype_indices[i]\n",
    "                v = prototype_indices[j]\n",
    "                S_proto_edges[u, v] = S_proto[i, j]\n",
    "                S_proto_edges[v, u] = S_proto[i, j]\n",
    "    \n",
    "    # -------------（8）构造最终联合邻接矩阵 A -------------\n",
    "    A = np.maximum(S_local, S_connect)\n",
    "    A = np.maximum(A, S_proto_edges)\n",
    "    \n",
    "    adj_matrix = csr_matrix(A)\n",
    "    print(f\"联合邻接矩阵构造完成，尺寸为：{adj_matrix.shape}\")\n",
    "    return adj_matrix, cluster_assignments, prototype_indices, n_clusters\n",
    "\n",
    "    \n",
    "# ---------------------- 邻接矩阵转换为 edge_index ----------------------\n",
    "def adjacency_to_edge_index(adj_matrix, prototype_indices, proto_threshold=0.05, cluster_threshold=0.15):\n",
    "    \"\"\"\n",
    "    将联合邻接矩阵转换为 edge_index（图的边列表），\n",
    "    根据节点是否为原型采用不同阈值：\n",
    "      - 对于两个原型节点，边权需大于 proto_threshold；\n",
    "      - 对于两个非原型节点，边权需大于 cluster_threshold；\n",
    "      - 对于一个原型与一个非原型节点，只要边权大于 0 则保留。\n",
    "      \n",
    "    返回 edge_index: torch.tensor，形状为 [2, num_edges]\n",
    "    \"\"\"\n",
    "    A = adj_matrix.toarray()\n",
    "    N = A.shape[0]\n",
    "    binary_adj = np.zeros_like(A, dtype=int)\n",
    "    is_proto = np.zeros(N, dtype=bool)\n",
    "    is_proto[prototype_indices] = True\n",
    "    proto_idx = np.where(is_proto)[0]\n",
    "    nonproto_idx = np.where(~is_proto)[0]\n",
    "    \n",
    "    # 原型–原型部分\n",
    "    if len(proto_idx) > 0:\n",
    "        sub = A[np.ix_(proto_idx, proto_idx)]\n",
    "        binary_sub = (sub > proto_threshold).astype(int)\n",
    "        binary_adj[np.ix_(proto_idx, proto_idx)] = binary_sub\n",
    "    # 非原型–非原型部分\n",
    "    if len(nonproto_idx) > 0:\n",
    "        sub = A[np.ix_(nonproto_idx, nonproto_idx)]\n",
    "        binary_sub = (sub > cluster_threshold).astype(int)\n",
    "        binary_adj[np.ix_(nonproto_idx, nonproto_idx)] = binary_sub\n",
    "    # 原型–非原型部分\n",
    "    if len(proto_idx) > 0 and len(nonproto_idx) > 0:\n",
    "        sub = A[np.ix_(proto_idx, nonproto_idx)]\n",
    "        binary_sub = (sub > 0).astype(int)\n",
    "        binary_adj[np.ix_(proto_idx, nonproto_idx)] = binary_sub\n",
    "        binary_adj[np.ix_(nonproto_idx, proto_idx)] = binary_sub.T\n",
    "    \n",
    "    coo = csr_matrix(binary_adj).tocoo()\n",
    "    edge_index = torch.tensor(np.vstack((coo.row, coo.col)), dtype=torch.long)\n",
    "    print(\"邻接矩阵转换完成！Edge index 维度:\", edge_index.shape)\n",
    "    return edge_index\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T16:07:55.078709Z",
     "start_time": "2025-02-12T16:05:45.830597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载数据\n",
    "X, y, X_train, X_valid, X_test, y_train, y_valid, y_test, train_mask, valid_mask, test_mask = load_data_DEF(random_state=22)\n",
    "\n",
    "# 计算联合邻接矩阵（注意：最终节点数应与 X 的行数一致，如 30000×30000）\n",
    "print(\"开始计算联合邻接矩阵（不增加额外原型节点） ...\")\n",
    "adj_matrix, cluster_assignments, prototype_indices, n_clusters = compute_adjacency_matrix_by_prototypes(\n",
    "    X_train, X_valid, X_test, y_train, y_valid,\n",
    "    n_clusters=1000, n_estimators=50, max_depth=None, random_state=42,\n",
    "    cluster_threshold=0, proto_threshold=0\n",
    ")\n",
    "\n",
    "# 将联合邻接矩阵转换为 edge_index\n",
    "edge_index = adjacency_to_edge_index(adj_matrix, prototype_indices, proto_threshold=0, cluster_threshold=0.4)"
   ],
   "id": "40f5189e54f85a5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始计算联合邻接矩阵（不增加额外原型节点） ...\n",
      "开始对训练+验证集进行 KMeans 聚类 ...\n",
      "训练+验证集聚类样本数分布：\n",
      "  Cluster 0: 19 samples\n",
      "  Cluster 1: 45 samples\n",
      "  Cluster 2: 35 samples\n",
      "  Cluster 3: 46 samples\n",
      "  Cluster 4: 16 samples\n",
      "  Cluster 5: 27 samples\n",
      "  Cluster 6: 59 samples\n",
      "  Cluster 7: 30 samples\n",
      "  Cluster 8: 30 samples\n",
      "  Cluster 9: 37 samples\n",
      "  Cluster 10: 9 samples\n",
      "  Cluster 11: 4 samples\n",
      "  Cluster 12: 91 samples\n",
      "  Cluster 13: 75 samples\n",
      "  Cluster 14: 2 samples\n",
      "  Cluster 15: 5 samples\n",
      "  Cluster 16: 5 samples\n",
      "  Cluster 17: 7 samples\n",
      "  Cluster 18: 5 samples\n",
      "  Cluster 19: 1 samples\n",
      "  Cluster 20: 39 samples\n",
      "  Cluster 21: 59 samples\n",
      "  Cluster 22: 69 samples\n",
      "  Cluster 23: 70 samples\n",
      "  Cluster 24: 42 samples\n",
      "  Cluster 25: 64 samples\n",
      "  Cluster 26: 1 samples\n",
      "  Cluster 27: 13 samples\n",
      "  Cluster 28: 1 samples\n",
      "  Cluster 29: 4 samples\n",
      "  Cluster 30: 60 samples\n",
      "  Cluster 31: 14 samples\n",
      "  Cluster 32: 39 samples\n",
      "  Cluster 33: 39 samples\n",
      "  Cluster 34: 28 samples\n",
      "  Cluster 35: 14 samples\n",
      "  Cluster 36: 64 samples\n",
      "  Cluster 37: 2 samples\n",
      "  Cluster 38: 19 samples\n",
      "  Cluster 39: 29 samples\n",
      "  Cluster 40: 9 samples\n",
      "  Cluster 41: 58 samples\n",
      "  Cluster 42: 14 samples\n",
      "  Cluster 43: 19 samples\n",
      "  Cluster 44: 17 samples\n",
      "  Cluster 45: 91 samples\n",
      "  Cluster 46: 19 samples\n",
      "  Cluster 47: 21 samples\n",
      "  Cluster 48: 15 samples\n",
      "  Cluster 49: 36 samples\n",
      "  Cluster 50: 1 samples\n",
      "  Cluster 51: 83 samples\n",
      "  Cluster 52: 34 samples\n",
      "  Cluster 53: 2 samples\n",
      "  Cluster 54: 16 samples\n",
      "  Cluster 55: 3 samples\n",
      "  Cluster 56: 80 samples\n",
      "  Cluster 57: 39 samples\n",
      "  Cluster 58: 5 samples\n",
      "  Cluster 59: 40 samples\n",
      "  Cluster 60: 39 samples\n",
      "  Cluster 61: 1 samples\n",
      "  Cluster 62: 10 samples\n",
      "  Cluster 63: 17 samples\n",
      "  Cluster 64: 9 samples\n",
      "  Cluster 65: 10 samples\n",
      "  Cluster 66: 4 samples\n",
      "  Cluster 67: 63 samples\n",
      "  Cluster 68: 41 samples\n",
      "  Cluster 69: 4 samples\n",
      "  Cluster 70: 35 samples\n",
      "  Cluster 71: 23 samples\n",
      "  Cluster 72: 43 samples\n",
      "  Cluster 73: 117 samples\n",
      "  Cluster 74: 18 samples\n",
      "  Cluster 75: 1 samples\n",
      "  Cluster 76: 32 samples\n",
      "  Cluster 77: 28 samples\n",
      "  Cluster 78: 3 samples\n",
      "  Cluster 79: 1 samples\n",
      "  Cluster 80: 7 samples\n",
      "  Cluster 81: 18 samples\n",
      "  Cluster 82: 4 samples\n",
      "  Cluster 83: 29 samples\n",
      "  Cluster 84: 48 samples\n",
      "  Cluster 85: 14 samples\n",
      "  Cluster 86: 32 samples\n",
      "  Cluster 87: 29 samples\n",
      "  Cluster 88: 43 samples\n",
      "  Cluster 89: 43 samples\n",
      "  Cluster 90: 30 samples\n",
      "  Cluster 91: 5 samples\n",
      "  Cluster 92: 40 samples\n",
      "  Cluster 93: 38 samples\n",
      "  Cluster 94: 68 samples\n",
      "  Cluster 95: 18 samples\n",
      "  Cluster 96: 19 samples\n",
      "  Cluster 97: 40 samples\n",
      "  Cluster 98: 13 samples\n",
      "  Cluster 99: 18 samples\n",
      "  Cluster 100: 31 samples\n",
      "  Cluster 101: 7 samples\n",
      "  Cluster 102: 17 samples\n",
      "  Cluster 103: 3 samples\n",
      "  Cluster 104: 2 samples\n",
      "  Cluster 105: 1 samples\n",
      "  Cluster 106: 1 samples\n",
      "  Cluster 107: 228 samples\n",
      "  Cluster 108: 6 samples\n",
      "  Cluster 109: 1 samples\n",
      "  Cluster 110: 7 samples\n",
      "  Cluster 111: 26 samples\n",
      "  Cluster 112: 10 samples\n",
      "  Cluster 113: 1 samples\n",
      "  Cluster 114: 5 samples\n",
      "  Cluster 115: 18 samples\n",
      "  Cluster 116: 18 samples\n",
      "  Cluster 117: 7 samples\n",
      "  Cluster 118: 6 samples\n",
      "  Cluster 119: 96 samples\n",
      "  Cluster 120: 84 samples\n",
      "  Cluster 121: 42 samples\n",
      "  Cluster 122: 83 samples\n",
      "  Cluster 123: 60 samples\n",
      "  Cluster 124: 50 samples\n",
      "  Cluster 125: 35 samples\n",
      "  Cluster 126: 62 samples\n",
      "  Cluster 127: 152 samples\n",
      "  Cluster 128: 27 samples\n",
      "  Cluster 129: 20 samples\n",
      "  Cluster 130: 3 samples\n",
      "  Cluster 131: 1 samples\n",
      "  Cluster 132: 15 samples\n",
      "  Cluster 133: 34 samples\n",
      "  Cluster 134: 34 samples\n",
      "  Cluster 135: 1 samples\n",
      "  Cluster 136: 51 samples\n",
      "  Cluster 137: 19 samples\n",
      "  Cluster 138: 8 samples\n",
      "  Cluster 139: 43 samples\n",
      "  Cluster 140: 5 samples\n",
      "  Cluster 141: 95 samples\n",
      "  Cluster 142: 5 samples\n",
      "  Cluster 143: 48 samples\n",
      "  Cluster 144: 2 samples\n",
      "  Cluster 145: 74 samples\n",
      "  Cluster 146: 16 samples\n",
      "  Cluster 147: 42 samples\n",
      "  Cluster 148: 2 samples\n",
      "  Cluster 149: 30 samples\n",
      "  Cluster 150: 20 samples\n",
      "  Cluster 151: 22 samples\n",
      "  Cluster 152: 132 samples\n",
      "  Cluster 153: 16 samples\n",
      "  Cluster 154: 15 samples\n",
      "  Cluster 155: 14 samples\n",
      "  Cluster 156: 59 samples\n",
      "  Cluster 157: 20 samples\n",
      "  Cluster 158: 27 samples\n",
      "  Cluster 159: 1 samples\n",
      "  Cluster 160: 9 samples\n",
      "  Cluster 161: 72 samples\n",
      "  Cluster 162: 38 samples\n",
      "  Cluster 163: 2 samples\n",
      "  Cluster 164: 93 samples\n",
      "  Cluster 165: 1 samples\n",
      "  Cluster 166: 4 samples\n",
      "  Cluster 167: 32 samples\n",
      "  Cluster 168: 9 samples\n",
      "  Cluster 169: 78 samples\n",
      "  Cluster 170: 6 samples\n",
      "  Cluster 171: 1 samples\n",
      "  Cluster 172: 4 samples\n",
      "  Cluster 173: 9 samples\n",
      "  Cluster 174: 14 samples\n",
      "  Cluster 175: 26 samples\n",
      "  Cluster 176: 3 samples\n",
      "  Cluster 177: 12 samples\n",
      "  Cluster 178: 39 samples\n",
      "  Cluster 179: 47 samples\n",
      "  Cluster 180: 20 samples\n",
      "  Cluster 181: 27 samples\n",
      "  Cluster 182: 14 samples\n",
      "  Cluster 183: 2 samples\n",
      "  Cluster 184: 52 samples\n",
      "  Cluster 185: 49 samples\n",
      "  Cluster 186: 25 samples\n",
      "  Cluster 187: 9 samples\n",
      "  Cluster 188: 15 samples\n",
      "  Cluster 189: 24 samples\n",
      "  Cluster 190: 13 samples\n",
      "  Cluster 191: 8 samples\n",
      "  Cluster 192: 69 samples\n",
      "  Cluster 193: 1 samples\n",
      "  Cluster 194: 87 samples\n",
      "  Cluster 195: 27 samples\n",
      "  Cluster 196: 69 samples\n",
      "  Cluster 197: 1 samples\n",
      "  Cluster 198: 1 samples\n",
      "  Cluster 199: 17 samples\n",
      "  Cluster 200: 24 samples\n",
      "  Cluster 201: 45 samples\n",
      "  Cluster 202: 99 samples\n",
      "  Cluster 203: 29 samples\n",
      "  Cluster 204: 22 samples\n",
      "  Cluster 205: 71 samples\n",
      "  Cluster 206: 26 samples\n",
      "  Cluster 207: 101 samples\n",
      "  Cluster 208: 22 samples\n",
      "  Cluster 209: 27 samples\n",
      "  Cluster 210: 22 samples\n",
      "  Cluster 211: 12 samples\n",
      "  Cluster 212: 14 samples\n",
      "  Cluster 213: 33 samples\n",
      "  Cluster 214: 42 samples\n",
      "  Cluster 215: 5 samples\n",
      "  Cluster 216: 3 samples\n",
      "  Cluster 217: 41 samples\n",
      "  Cluster 218: 30 samples\n",
      "  Cluster 219: 3 samples\n",
      "  Cluster 220: 26 samples\n",
      "  Cluster 221: 31 samples\n",
      "  Cluster 222: 26 samples\n",
      "  Cluster 223: 73 samples\n",
      "  Cluster 224: 42 samples\n",
      "  Cluster 225: 6 samples\n",
      "  Cluster 226: 42 samples\n",
      "  Cluster 227: 28 samples\n",
      "  Cluster 228: 34 samples\n",
      "  Cluster 229: 33 samples\n",
      "  Cluster 230: 28 samples\n",
      "  Cluster 231: 18 samples\n",
      "  Cluster 232: 11 samples\n",
      "  Cluster 233: 23 samples\n",
      "  Cluster 234: 31 samples\n",
      "  Cluster 235: 6 samples\n",
      "  Cluster 236: 119 samples\n",
      "  Cluster 237: 19 samples\n",
      "  Cluster 238: 14 samples\n",
      "  Cluster 239: 49 samples\n",
      "  Cluster 240: 12 samples\n",
      "  Cluster 241: 59 samples\n",
      "  Cluster 242: 33 samples\n",
      "  Cluster 243: 44 samples\n",
      "  Cluster 244: 35 samples\n",
      "  Cluster 245: 57 samples\n",
      "  Cluster 246: 42 samples\n",
      "  Cluster 247: 19 samples\n",
      "  Cluster 248: 13 samples\n",
      "  Cluster 249: 9 samples\n",
      "  Cluster 250: 1 samples\n",
      "  Cluster 251: 18 samples\n",
      "  Cluster 252: 3 samples\n",
      "  Cluster 253: 102 samples\n",
      "  Cluster 254: 1 samples\n",
      "  Cluster 255: 14 samples\n",
      "  Cluster 256: 88 samples\n",
      "  Cluster 257: 57 samples\n",
      "  Cluster 258: 51 samples\n",
      "  Cluster 259: 16 samples\n",
      "  Cluster 260: 24 samples\n",
      "  Cluster 261: 24 samples\n",
      "  Cluster 262: 38 samples\n",
      "  Cluster 263: 48 samples\n",
      "  Cluster 264: 24 samples\n",
      "  Cluster 265: 1 samples\n",
      "  Cluster 266: 75 samples\n",
      "  Cluster 267: 65 samples\n",
      "  Cluster 268: 36 samples\n",
      "  Cluster 269: 29 samples\n",
      "  Cluster 270: 49 samples\n",
      "  Cluster 271: 9 samples\n",
      "  Cluster 272: 14 samples\n",
      "  Cluster 273: 48 samples\n",
      "  Cluster 274: 42 samples\n",
      "  Cluster 275: 21 samples\n",
      "  Cluster 276: 3 samples\n",
      "  Cluster 277: 28 samples\n",
      "  Cluster 278: 98 samples\n",
      "  Cluster 279: 35 samples\n",
      "  Cluster 280: 35 samples\n",
      "  Cluster 281: 31 samples\n",
      "  Cluster 282: 24 samples\n",
      "  Cluster 283: 8 samples\n",
      "  Cluster 284: 18 samples\n",
      "  Cluster 285: 3 samples\n",
      "  Cluster 286: 66 samples\n",
      "  Cluster 287: 32 samples\n",
      "  Cluster 288: 1 samples\n",
      "  Cluster 289: 15 samples\n",
      "  Cluster 290: 11 samples\n",
      "  Cluster 291: 32 samples\n",
      "  Cluster 292: 30 samples\n",
      "  Cluster 293: 44 samples\n",
      "  Cluster 294: 2 samples\n",
      "  Cluster 295: 21 samples\n",
      "  Cluster 296: 14 samples\n",
      "  Cluster 297: 43 samples\n",
      "  Cluster 298: 17 samples\n",
      "  Cluster 299: 2 samples\n",
      "  Cluster 300: 25 samples\n",
      "  Cluster 301: 2 samples\n",
      "  Cluster 302: 12 samples\n",
      "  Cluster 303: 29 samples\n",
      "  Cluster 304: 36 samples\n",
      "  Cluster 305: 3 samples\n",
      "  Cluster 306: 28 samples\n",
      "  Cluster 307: 14 samples\n",
      "  Cluster 308: 1 samples\n",
      "  Cluster 309: 27 samples\n",
      "  Cluster 310: 2 samples\n",
      "  Cluster 311: 2 samples\n",
      "  Cluster 312: 2 samples\n",
      "  Cluster 313: 9 samples\n",
      "  Cluster 314: 9 samples\n",
      "  Cluster 315: 17 samples\n",
      "  Cluster 316: 40 samples\n",
      "  Cluster 317: 3 samples\n",
      "  Cluster 318: 1 samples\n",
      "  Cluster 319: 15 samples\n",
      "  Cluster 320: 3 samples\n",
      "  Cluster 321: 15 samples\n",
      "  Cluster 322: 24 samples\n",
      "  Cluster 323: 20 samples\n",
      "  Cluster 324: 14 samples\n",
      "  Cluster 325: 26 samples\n",
      "  Cluster 326: 56 samples\n",
      "  Cluster 327: 128 samples\n",
      "  Cluster 328: 10 samples\n",
      "  Cluster 329: 30 samples\n",
      "  Cluster 330: 13 samples\n",
      "  Cluster 331: 30 samples\n",
      "  Cluster 332: 34 samples\n",
      "  Cluster 333: 2 samples\n",
      "  Cluster 334: 1 samples\n",
      "  Cluster 335: 26 samples\n",
      "  Cluster 336: 15 samples\n",
      "  Cluster 337: 1 samples\n",
      "  Cluster 338: 24 samples\n",
      "  Cluster 339: 36 samples\n",
      "  Cluster 340: 11 samples\n",
      "  Cluster 341: 15 samples\n",
      "  Cluster 342: 1 samples\n",
      "  Cluster 343: 20 samples\n",
      "  Cluster 344: 2 samples\n",
      "  Cluster 345: 64 samples\n",
      "  Cluster 346: 16 samples\n",
      "  Cluster 347: 83 samples\n",
      "  Cluster 348: 26 samples\n",
      "  Cluster 349: 46 samples\n",
      "  Cluster 350: 77 samples\n",
      "  Cluster 351: 71 samples\n",
      "  Cluster 352: 1 samples\n",
      "  Cluster 353: 52 samples\n",
      "  Cluster 354: 46 samples\n",
      "  Cluster 355: 18 samples\n",
      "  Cluster 356: 6 samples\n",
      "  Cluster 357: 12 samples\n",
      "  Cluster 358: 71 samples\n",
      "  Cluster 359: 11 samples\n",
      "  Cluster 360: 44 samples\n",
      "  Cluster 361: 2 samples\n",
      "  Cluster 362: 16 samples\n",
      "  Cluster 363: 2 samples\n",
      "  Cluster 364: 51 samples\n",
      "  Cluster 365: 52 samples\n",
      "  Cluster 366: 2 samples\n",
      "  Cluster 367: 11 samples\n",
      "  Cluster 368: 23 samples\n",
      "  Cluster 369: 12 samples\n",
      "  Cluster 370: 1 samples\n",
      "  Cluster 371: 32 samples\n",
      "  Cluster 372: 59 samples\n",
      "  Cluster 373: 3 samples\n",
      "  Cluster 374: 88 samples\n",
      "  Cluster 375: 19 samples\n",
      "  Cluster 376: 39 samples\n",
      "  Cluster 377: 134 samples\n",
      "  Cluster 378: 12 samples\n",
      "  Cluster 379: 7 samples\n",
      "  Cluster 380: 1 samples\n",
      "  Cluster 381: 16 samples\n",
      "  Cluster 382: 1 samples\n",
      "  Cluster 383: 2 samples\n",
      "  Cluster 384: 23 samples\n",
      "  Cluster 385: 68 samples\n",
      "  Cluster 386: 9 samples\n",
      "  Cluster 387: 46 samples\n",
      "  Cluster 388: 49 samples\n",
      "  Cluster 389: 48 samples\n",
      "  Cluster 390: 55 samples\n",
      "  Cluster 391: 10 samples\n",
      "  Cluster 392: 2 samples\n",
      "  Cluster 393: 23 samples\n",
      "  Cluster 394: 29 samples\n",
      "  Cluster 395: 1 samples\n",
      "  Cluster 396: 28 samples\n",
      "  Cluster 397: 53 samples\n",
      "  Cluster 398: 7 samples\n",
      "  Cluster 399: 1 samples\n",
      "  Cluster 400: 73 samples\n",
      "  Cluster 401: 1 samples\n",
      "  Cluster 402: 2 samples\n",
      "  Cluster 403: 12 samples\n",
      "  Cluster 404: 3 samples\n",
      "  Cluster 405: 26 samples\n",
      "  Cluster 406: 1 samples\n",
      "  Cluster 407: 1 samples\n",
      "  Cluster 408: 47 samples\n",
      "  Cluster 409: 1 samples\n",
      "  Cluster 410: 23 samples\n",
      "  Cluster 411: 50 samples\n",
      "  Cluster 412: 40 samples\n",
      "  Cluster 413: 56 samples\n",
      "  Cluster 414: 11 samples\n",
      "  Cluster 415: 25 samples\n",
      "  Cluster 416: 10 samples\n",
      "  Cluster 417: 24 samples\n",
      "  Cluster 418: 68 samples\n",
      "  Cluster 419: 1 samples\n",
      "  Cluster 420: 25 samples\n",
      "  Cluster 421: 40 samples\n",
      "  Cluster 422: 80 samples\n",
      "  Cluster 423: 45 samples\n",
      "  Cluster 424: 39 samples\n",
      "  Cluster 425: 7 samples\n",
      "  Cluster 426: 29 samples\n",
      "  Cluster 427: 2 samples\n",
      "  Cluster 428: 19 samples\n",
      "  Cluster 429: 2 samples\n",
      "  Cluster 430: 70 samples\n",
      "  Cluster 431: 24 samples\n",
      "  Cluster 432: 34 samples\n",
      "  Cluster 433: 5 samples\n",
      "  Cluster 434: 15 samples\n",
      "  Cluster 435: 35 samples\n",
      "  Cluster 436: 13 samples\n",
      "  Cluster 437: 6 samples\n",
      "  Cluster 438: 3 samples\n",
      "  Cluster 439: 43 samples\n",
      "  Cluster 440: 3 samples\n",
      "  Cluster 441: 15 samples\n",
      "  Cluster 442: 17 samples\n",
      "  Cluster 443: 48 samples\n",
      "  Cluster 444: 39 samples\n",
      "  Cluster 445: 23 samples\n",
      "  Cluster 446: 6 samples\n",
      "  Cluster 447: 2 samples\n",
      "  Cluster 448: 15 samples\n",
      "  Cluster 449: 1 samples\n",
      "  Cluster 450: 12 samples\n",
      "  Cluster 451: 16 samples\n",
      "  Cluster 452: 38 samples\n",
      "  Cluster 453: 6 samples\n",
      "  Cluster 454: 45 samples\n",
      "  Cluster 455: 2 samples\n",
      "  Cluster 456: 69 samples\n",
      "  Cluster 457: 26 samples\n",
      "  Cluster 458: 1 samples\n",
      "  Cluster 459: 1 samples\n",
      "  Cluster 460: 33 samples\n",
      "  Cluster 461: 108 samples\n",
      "  Cluster 462: 14 samples\n",
      "  Cluster 463: 65 samples\n",
      "  Cluster 464: 6 samples\n",
      "  Cluster 465: 2 samples\n",
      "  Cluster 466: 32 samples\n",
      "  Cluster 467: 21 samples\n",
      "  Cluster 468: 6 samples\n",
      "  Cluster 469: 16 samples\n",
      "  Cluster 470: 2 samples\n",
      "  Cluster 471: 9 samples\n",
      "  Cluster 472: 1 samples\n",
      "  Cluster 473: 10 samples\n",
      "  Cluster 474: 1 samples\n",
      "  Cluster 475: 2 samples\n",
      "  Cluster 476: 29 samples\n",
      "  Cluster 477: 64 samples\n",
      "  Cluster 478: 108 samples\n",
      "  Cluster 479: 50 samples\n",
      "  Cluster 480: 56 samples\n",
      "  Cluster 481: 21 samples\n",
      "  Cluster 482: 1 samples\n",
      "  Cluster 483: 1 samples\n",
      "  Cluster 484: 23 samples\n",
      "  Cluster 485: 15 samples\n",
      "  Cluster 486: 1 samples\n",
      "  Cluster 487: 55 samples\n",
      "  Cluster 488: 18 samples\n",
      "  Cluster 489: 12 samples\n",
      "  Cluster 490: 1 samples\n",
      "  Cluster 491: 1 samples\n",
      "  Cluster 492: 25 samples\n",
      "  Cluster 493: 3 samples\n",
      "  Cluster 494: 18 samples\n",
      "  Cluster 495: 10 samples\n",
      "  Cluster 496: 19 samples\n",
      "  Cluster 497: 1 samples\n",
      "  Cluster 498: 9 samples\n",
      "  Cluster 499: 19 samples\n",
      "  Cluster 500: 23 samples\n",
      "  Cluster 501: 13 samples\n",
      "  Cluster 502: 1 samples\n",
      "  Cluster 503: 16 samples\n",
      "  Cluster 504: 1 samples\n",
      "  Cluster 505: 29 samples\n",
      "  Cluster 506: 1 samples\n",
      "  Cluster 507: 7 samples\n",
      "  Cluster 508: 77 samples\n",
      "  Cluster 509: 2 samples\n",
      "  Cluster 510: 23 samples\n",
      "  Cluster 511: 25 samples\n",
      "  Cluster 512: 18 samples\n",
      "  Cluster 513: 3 samples\n",
      "  Cluster 514: 1 samples\n",
      "  Cluster 515: 26 samples\n",
      "  Cluster 516: 20 samples\n",
      "  Cluster 517: 10 samples\n",
      "  Cluster 518: 2 samples\n",
      "  Cluster 519: 26 samples\n",
      "  Cluster 520: 34 samples\n",
      "  Cluster 521: 6 samples\n",
      "  Cluster 522: 29 samples\n",
      "  Cluster 523: 25 samples\n",
      "  Cluster 524: 6 samples\n",
      "  Cluster 525: 38 samples\n",
      "  Cluster 526: 24 samples\n",
      "  Cluster 527: 20 samples\n",
      "  Cluster 528: 16 samples\n",
      "  Cluster 529: 26 samples\n",
      "  Cluster 530: 38 samples\n",
      "  Cluster 531: 31 samples\n",
      "  Cluster 532: 12 samples\n",
      "  Cluster 533: 5 samples\n",
      "  Cluster 534: 52 samples\n",
      "  Cluster 535: 18 samples\n",
      "  Cluster 536: 22 samples\n",
      "  Cluster 537: 12 samples\n",
      "  Cluster 538: 20 samples\n",
      "  Cluster 539: 135 samples\n",
      "  Cluster 540: 1 samples\n",
      "  Cluster 541: 36 samples\n",
      "  Cluster 542: 20 samples\n",
      "  Cluster 543: 12 samples\n",
      "  Cluster 544: 17 samples\n",
      "  Cluster 545: 16 samples\n",
      "  Cluster 546: 2 samples\n",
      "  Cluster 547: 4 samples\n",
      "  Cluster 548: 5 samples\n",
      "  Cluster 549: 83 samples\n",
      "  Cluster 550: 16 samples\n",
      "  Cluster 551: 13 samples\n",
      "  Cluster 552: 14 samples\n",
      "  Cluster 553: 3 samples\n",
      "  Cluster 554: 1 samples\n",
      "  Cluster 555: 1 samples\n",
      "  Cluster 556: 6 samples\n",
      "  Cluster 557: 6 samples\n",
      "  Cluster 558: 37 samples\n",
      "  Cluster 559: 66 samples\n",
      "  Cluster 560: 14 samples\n",
      "  Cluster 561: 48 samples\n",
      "  Cluster 562: 4 samples\n",
      "  Cluster 563: 12 samples\n",
      "  Cluster 564: 26 samples\n",
      "  Cluster 565: 8 samples\n",
      "  Cluster 566: 40 samples\n",
      "  Cluster 567: 3 samples\n",
      "  Cluster 568: 27 samples\n",
      "  Cluster 569: 20 samples\n",
      "  Cluster 570: 23 samples\n",
      "  Cluster 571: 75 samples\n",
      "  Cluster 572: 29 samples\n",
      "  Cluster 573: 24 samples\n",
      "  Cluster 574: 61 samples\n",
      "  Cluster 575: 7 samples\n",
      "  Cluster 576: 9 samples\n",
      "  Cluster 577: 39 samples\n",
      "  Cluster 578: 52 samples\n",
      "  Cluster 579: 25 samples\n",
      "  Cluster 580: 1 samples\n",
      "  Cluster 581: 26 samples\n",
      "  Cluster 582: 1 samples\n",
      "  Cluster 583: 9 samples\n",
      "  Cluster 584: 35 samples\n",
      "  Cluster 585: 9 samples\n",
      "  Cluster 586: 21 samples\n",
      "  Cluster 587: 54 samples\n",
      "  Cluster 588: 41 samples\n",
      "  Cluster 589: 5 samples\n",
      "  Cluster 590: 17 samples\n",
      "  Cluster 591: 9 samples\n",
      "  Cluster 592: 15 samples\n",
      "  Cluster 593: 19 samples\n",
      "  Cluster 594: 15 samples\n",
      "  Cluster 595: 12 samples\n",
      "  Cluster 596: 50 samples\n",
      "  Cluster 597: 33 samples\n",
      "  Cluster 598: 7 samples\n",
      "  Cluster 599: 5 samples\n",
      "  Cluster 600: 30 samples\n",
      "  Cluster 601: 14 samples\n",
      "  Cluster 602: 5 samples\n",
      "  Cluster 603: 2 samples\n",
      "  Cluster 604: 18 samples\n",
      "  Cluster 605: 19 samples\n",
      "  Cluster 606: 2 samples\n",
      "  Cluster 607: 6 samples\n",
      "  Cluster 608: 1 samples\n",
      "  Cluster 609: 1 samples\n",
      "  Cluster 610: 26 samples\n",
      "  Cluster 611: 38 samples\n",
      "  Cluster 612: 19 samples\n",
      "  Cluster 613: 16 samples\n",
      "  Cluster 614: 4 samples\n",
      "  Cluster 615: 25 samples\n",
      "  Cluster 616: 12 samples\n",
      "  Cluster 617: 6 samples\n",
      "  Cluster 618: 1 samples\n",
      "  Cluster 619: 3 samples\n",
      "  Cluster 620: 1 samples\n",
      "  Cluster 621: 3 samples\n",
      "  Cluster 622: 1 samples\n",
      "  Cluster 623: 22 samples\n",
      "  Cluster 624: 16 samples\n",
      "  Cluster 625: 6 samples\n",
      "  Cluster 626: 66 samples\n",
      "  Cluster 627: 14 samples\n",
      "  Cluster 628: 1 samples\n",
      "  Cluster 629: 16 samples\n",
      "  Cluster 630: 50 samples\n",
      "  Cluster 631: 1 samples\n",
      "  Cluster 632: 39 samples\n",
      "  Cluster 633: 139 samples\n",
      "  Cluster 634: 18 samples\n",
      "  Cluster 635: 19 samples\n",
      "  Cluster 636: 64 samples\n",
      "  Cluster 637: 39 samples\n",
      "  Cluster 638: 41 samples\n",
      "  Cluster 639: 63 samples\n",
      "  Cluster 640: 26 samples\n",
      "  Cluster 641: 2 samples\n",
      "  Cluster 642: 14 samples\n",
      "  Cluster 643: 36 samples\n",
      "  Cluster 644: 10 samples\n",
      "  Cluster 645: 18 samples\n",
      "  Cluster 646: 72 samples\n",
      "  Cluster 647: 24 samples\n",
      "  Cluster 648: 18 samples\n",
      "  Cluster 649: 24 samples\n",
      "  Cluster 650: 2 samples\n",
      "  Cluster 651: 17 samples\n",
      "  Cluster 652: 10 samples\n",
      "  Cluster 653: 1 samples\n",
      "  Cluster 654: 36 samples\n",
      "  Cluster 655: 8 samples\n",
      "  Cluster 656: 20 samples\n",
      "  Cluster 657: 53 samples\n",
      "  Cluster 658: 11 samples\n",
      "  Cluster 659: 35 samples\n",
      "  Cluster 660: 34 samples\n",
      "  Cluster 661: 27 samples\n",
      "  Cluster 662: 62 samples\n",
      "  Cluster 663: 18 samples\n",
      "  Cluster 664: 4 samples\n",
      "  Cluster 665: 1 samples\n",
      "  Cluster 666: 5 samples\n",
      "  Cluster 667: 10 samples\n",
      "  Cluster 668: 28 samples\n",
      "  Cluster 669: 70 samples\n",
      "  Cluster 670: 10 samples\n",
      "  Cluster 671: 34 samples\n",
      "  Cluster 672: 11 samples\n",
      "  Cluster 673: 20 samples\n",
      "  Cluster 674: 11 samples\n",
      "  Cluster 675: 1 samples\n",
      "  Cluster 676: 44 samples\n",
      "  Cluster 677: 14 samples\n",
      "  Cluster 678: 1 samples\n",
      "  Cluster 679: 25 samples\n",
      "  Cluster 680: 27 samples\n",
      "  Cluster 681: 44 samples\n",
      "  Cluster 682: 55 samples\n",
      "  Cluster 683: 3 samples\n",
      "  Cluster 684: 31 samples\n",
      "  Cluster 685: 3 samples\n",
      "  Cluster 686: 15 samples\n",
      "  Cluster 687: 40 samples\n",
      "  Cluster 688: 10 samples\n",
      "  Cluster 689: 78 samples\n",
      "  Cluster 690: 1 samples\n",
      "  Cluster 691: 16 samples\n",
      "  Cluster 692: 5 samples\n",
      "  Cluster 693: 18 samples\n",
      "  Cluster 694: 15 samples\n",
      "  Cluster 695: 5 samples\n",
      "  Cluster 696: 5 samples\n",
      "  Cluster 697: 15 samples\n",
      "  Cluster 698: 12 samples\n",
      "  Cluster 699: 24 samples\n",
      "  Cluster 700: 26 samples\n",
      "  Cluster 701: 14 samples\n",
      "  Cluster 702: 64 samples\n",
      "  Cluster 703: 8 samples\n",
      "  Cluster 704: 43 samples\n",
      "  Cluster 705: 8 samples\n",
      "  Cluster 706: 14 samples\n",
      "  Cluster 707: 18 samples\n",
      "  Cluster 708: 53 samples\n",
      "  Cluster 709: 53 samples\n",
      "  Cluster 710: 8 samples\n",
      "  Cluster 711: 20 samples\n",
      "  Cluster 712: 27 samples\n",
      "  Cluster 713: 26 samples\n",
      "  Cluster 714: 30 samples\n",
      "  Cluster 715: 15 samples\n",
      "  Cluster 716: 4 samples\n",
      "  Cluster 717: 18 samples\n",
      "  Cluster 718: 4 samples\n",
      "  Cluster 719: 12 samples\n",
      "  Cluster 720: 32 samples\n",
      "  Cluster 721: 6 samples\n",
      "  Cluster 722: 25 samples\n",
      "  Cluster 723: 8 samples\n",
      "  Cluster 724: 2 samples\n",
      "  Cluster 725: 5 samples\n",
      "  Cluster 726: 13 samples\n",
      "  Cluster 727: 1 samples\n",
      "  Cluster 728: 14 samples\n",
      "  Cluster 729: 39 samples\n",
      "  Cluster 730: 37 samples\n",
      "  Cluster 731: 14 samples\n",
      "  Cluster 732: 23 samples\n",
      "  Cluster 733: 16 samples\n",
      "  Cluster 734: 14 samples\n",
      "  Cluster 735: 15 samples\n",
      "  Cluster 736: 1 samples\n",
      "  Cluster 737: 3 samples\n",
      "  Cluster 738: 23 samples\n",
      "  Cluster 739: 1 samples\n",
      "  Cluster 740: 61 samples\n",
      "  Cluster 741: 17 samples\n",
      "  Cluster 742: 11 samples\n",
      "  Cluster 743: 11 samples\n",
      "  Cluster 744: 13 samples\n",
      "  Cluster 745: 49 samples\n",
      "  Cluster 746: 24 samples\n",
      "  Cluster 747: 8 samples\n",
      "  Cluster 748: 37 samples\n",
      "  Cluster 749: 15 samples\n",
      "  Cluster 750: 10 samples\n",
      "  Cluster 751: 25 samples\n",
      "  Cluster 752: 19 samples\n",
      "  Cluster 753: 4 samples\n",
      "  Cluster 754: 1 samples\n",
      "  Cluster 755: 21 samples\n",
      "  Cluster 756: 1 samples\n",
      "  Cluster 757: 9 samples\n",
      "  Cluster 758: 38 samples\n",
      "  Cluster 759: 31 samples\n",
      "  Cluster 760: 1 samples\n",
      "  Cluster 761: 23 samples\n",
      "  Cluster 762: 22 samples\n",
      "  Cluster 763: 13 samples\n",
      "  Cluster 764: 41 samples\n",
      "  Cluster 765: 3 samples\n",
      "  Cluster 766: 66 samples\n",
      "  Cluster 767: 17 samples\n",
      "  Cluster 768: 13 samples\n",
      "  Cluster 769: 15 samples\n",
      "  Cluster 770: 12 samples\n",
      "  Cluster 771: 11 samples\n",
      "  Cluster 772: 36 samples\n",
      "  Cluster 773: 15 samples\n",
      "  Cluster 774: 48 samples\n",
      "  Cluster 775: 14 samples\n",
      "  Cluster 776: 21 samples\n",
      "  Cluster 777: 2 samples\n",
      "  Cluster 778: 17 samples\n",
      "  Cluster 779: 6 samples\n",
      "  Cluster 780: 10 samples\n",
      "  Cluster 781: 13 samples\n",
      "  Cluster 782: 42 samples\n",
      "  Cluster 783: 25 samples\n",
      "  Cluster 784: 35 samples\n",
      "  Cluster 785: 5 samples\n",
      "  Cluster 786: 2 samples\n",
      "  Cluster 787: 26 samples\n",
      "  Cluster 788: 102 samples\n",
      "  Cluster 789: 9 samples\n",
      "  Cluster 790: 1 samples\n",
      "  Cluster 791: 37 samples\n",
      "  Cluster 792: 2 samples\n",
      "  Cluster 793: 1 samples\n",
      "  Cluster 794: 46 samples\n",
      "  Cluster 795: 1 samples\n",
      "  Cluster 796: 1 samples\n",
      "  Cluster 797: 11 samples\n",
      "  Cluster 798: 3 samples\n",
      "  Cluster 799: 38 samples\n",
      "  Cluster 800: 1 samples\n",
      "  Cluster 801: 3 samples\n",
      "  Cluster 802: 23 samples\n",
      "  Cluster 803: 1 samples\n",
      "  Cluster 804: 12 samples\n",
      "  Cluster 805: 3 samples\n",
      "  Cluster 806: 1 samples\n",
      "  Cluster 807: 29 samples\n",
      "  Cluster 808: 8 samples\n",
      "  Cluster 809: 21 samples\n",
      "  Cluster 810: 26 samples\n",
      "  Cluster 811: 1 samples\n",
      "  Cluster 812: 51 samples\n",
      "  Cluster 813: 1 samples\n",
      "  Cluster 814: 19 samples\n",
      "  Cluster 815: 5 samples\n",
      "  Cluster 816: 6 samples\n",
      "  Cluster 817: 10 samples\n",
      "  Cluster 818: 37 samples\n",
      "  Cluster 819: 28 samples\n",
      "  Cluster 820: 3 samples\n",
      "  Cluster 821: 20 samples\n",
      "  Cluster 822: 1 samples\n",
      "  Cluster 823: 7 samples\n",
      "  Cluster 824: 1 samples\n",
      "  Cluster 825: 5 samples\n",
      "  Cluster 826: 32 samples\n",
      "  Cluster 827: 32 samples\n",
      "  Cluster 828: 12 samples\n",
      "  Cluster 829: 21 samples\n",
      "  Cluster 830: 1 samples\n",
      "  Cluster 831: 29 samples\n",
      "  Cluster 832: 45 samples\n",
      "  Cluster 833: 9 samples\n",
      "  Cluster 834: 59 samples\n",
      "  Cluster 835: 60 samples\n",
      "  Cluster 836: 12 samples\n",
      "  Cluster 837: 6 samples\n",
      "  Cluster 838: 21 samples\n",
      "  Cluster 839: 4 samples\n",
      "  Cluster 840: 33 samples\n",
      "  Cluster 841: 1 samples\n",
      "  Cluster 842: 16 samples\n",
      "  Cluster 843: 1 samples\n",
      "  Cluster 844: 24 samples\n",
      "  Cluster 845: 46 samples\n",
      "  Cluster 846: 22 samples\n",
      "  Cluster 847: 1 samples\n",
      "  Cluster 848: 14 samples\n",
      "  Cluster 849: 1 samples\n",
      "  Cluster 850: 1 samples\n",
      "  Cluster 851: 29 samples\n",
      "  Cluster 852: 8 samples\n",
      "  Cluster 853: 24 samples\n",
      "  Cluster 854: 8 samples\n",
      "  Cluster 855: 2 samples\n",
      "  Cluster 856: 2 samples\n",
      "  Cluster 857: 2 samples\n",
      "  Cluster 858: 30 samples\n",
      "  Cluster 859: 35 samples\n",
      "  Cluster 860: 44 samples\n",
      "  Cluster 861: 38 samples\n",
      "  Cluster 862: 21 samples\n",
      "  Cluster 863: 14 samples\n",
      "  Cluster 864: 13 samples\n",
      "  Cluster 865: 17 samples\n",
      "  Cluster 866: 23 samples\n",
      "  Cluster 867: 29 samples\n",
      "  Cluster 868: 16 samples\n",
      "  Cluster 869: 7 samples\n",
      "  Cluster 870: 25 samples\n",
      "  Cluster 871: 13 samples\n",
      "  Cluster 872: 11 samples\n",
      "  Cluster 873: 13 samples\n",
      "  Cluster 874: 27 samples\n",
      "  Cluster 875: 1 samples\n",
      "  Cluster 876: 32 samples\n",
      "  Cluster 877: 36 samples\n",
      "  Cluster 878: 8 samples\n",
      "  Cluster 879: 22 samples\n",
      "  Cluster 880: 4 samples\n",
      "  Cluster 881: 1 samples\n",
      "  Cluster 882: 1 samples\n",
      "  Cluster 883: 47 samples\n",
      "  Cluster 884: 26 samples\n",
      "  Cluster 885: 1 samples\n",
      "  Cluster 886: 1 samples\n",
      "  Cluster 887: 53 samples\n",
      "  Cluster 888: 19 samples\n",
      "  Cluster 889: 33 samples\n",
      "  Cluster 890: 30 samples\n",
      "  Cluster 891: 8 samples\n",
      "  Cluster 892: 22 samples\n",
      "  Cluster 893: 8 samples\n",
      "  Cluster 894: 4 samples\n",
      "  Cluster 895: 39 samples\n",
      "  Cluster 896: 2 samples\n",
      "  Cluster 897: 1 samples\n",
      "  Cluster 898: 13 samples\n",
      "  Cluster 899: 2 samples\n",
      "  Cluster 900: 22 samples\n",
      "  Cluster 901: 12 samples\n",
      "  Cluster 902: 1 samples\n",
      "  Cluster 903: 4 samples\n",
      "  Cluster 904: 123 samples\n",
      "  Cluster 905: 4 samples\n",
      "  Cluster 906: 1 samples\n",
      "  Cluster 907: 5 samples\n",
      "  Cluster 908: 1 samples\n",
      "  Cluster 909: 4 samples\n",
      "  Cluster 910: 6 samples\n",
      "  Cluster 911: 43 samples\n",
      "  Cluster 912: 14 samples\n",
      "  Cluster 913: 1 samples\n",
      "  Cluster 914: 5 samples\n",
      "  Cluster 915: 5 samples\n",
      "  Cluster 916: 1 samples\n",
      "  Cluster 917: 43 samples\n",
      "  Cluster 918: 33 samples\n",
      "  Cluster 919: 1 samples\n",
      "  Cluster 920: 10 samples\n",
      "  Cluster 921: 27 samples\n",
      "  Cluster 922: 50 samples\n",
      "  Cluster 923: 44 samples\n",
      "  Cluster 924: 10 samples\n",
      "  Cluster 925: 7 samples\n",
      "  Cluster 926: 21 samples\n",
      "  Cluster 927: 17 samples\n",
      "  Cluster 928: 49 samples\n",
      "  Cluster 929: 8 samples\n",
      "  Cluster 930: 1 samples\n",
      "  Cluster 931: 8 samples\n",
      "  Cluster 932: 16 samples\n",
      "  Cluster 933: 8 samples\n",
      "  Cluster 934: 17 samples\n",
      "  Cluster 935: 49 samples\n",
      "  Cluster 936: 1 samples\n",
      "  Cluster 937: 6 samples\n",
      "  Cluster 938: 1 samples\n",
      "  Cluster 939: 3 samples\n",
      "  Cluster 940: 1 samples\n",
      "  Cluster 941: 7 samples\n",
      "  Cluster 942: 21 samples\n",
      "  Cluster 943: 14 samples\n",
      "  Cluster 944: 15 samples\n",
      "  Cluster 945: 16 samples\n",
      "  Cluster 946: 31 samples\n",
      "  Cluster 947: 33 samples\n",
      "  Cluster 948: 13 samples\n",
      "  Cluster 949: 1 samples\n",
      "  Cluster 950: 88 samples\n",
      "  Cluster 951: 1 samples\n",
      "  Cluster 952: 6 samples\n",
      "  Cluster 953: 29 samples\n",
      "  Cluster 954: 20 samples\n",
      "  Cluster 955: 8 samples\n",
      "  Cluster 956: 13 samples\n",
      "  Cluster 957: 1 samples\n",
      "  Cluster 958: 61 samples\n",
      "  Cluster 959: 10 samples\n",
      "  Cluster 960: 1 samples\n",
      "  Cluster 961: 26 samples\n",
      "  Cluster 962: 12 samples\n",
      "  Cluster 963: 10 samples\n",
      "  Cluster 964: 30 samples\n",
      "  Cluster 965: 7 samples\n",
      "  Cluster 966: 25 samples\n",
      "  Cluster 967: 22 samples\n",
      "  Cluster 968: 52 samples\n",
      "  Cluster 969: 1 samples\n",
      "  Cluster 970: 17 samples\n",
      "  Cluster 971: 16 samples\n",
      "  Cluster 972: 43 samples\n",
      "  Cluster 973: 19 samples\n",
      "  Cluster 974: 21 samples\n",
      "  Cluster 975: 22 samples\n",
      "  Cluster 976: 2 samples\n",
      "  Cluster 977: 3 samples\n",
      "  Cluster 978: 47 samples\n",
      "  Cluster 979: 92 samples\n",
      "  Cluster 980: 43 samples\n",
      "  Cluster 981: 28 samples\n",
      "  Cluster 982: 19 samples\n",
      "  Cluster 983: 1 samples\n",
      "  Cluster 984: 1 samples\n",
      "  Cluster 985: 10 samples\n",
      "  Cluster 986: 35 samples\n",
      "  Cluster 987: 15 samples\n",
      "  Cluster 988: 66 samples\n",
      "  Cluster 989: 19 samples\n",
      "  Cluster 990: 18 samples\n",
      "  Cluster 991: 1 samples\n",
      "  Cluster 992: 19 samples\n",
      "  Cluster 993: 28 samples\n",
      "  Cluster 994: 21 samples\n",
      "  Cluster 995: 231 samples\n",
      "  Cluster 996: 13 samples\n",
      "  Cluster 997: 35 samples\n",
      "  Cluster 998: 26 samples\n",
      "  Cluster 999: 8 samples\n",
      "计算各簇内部相似度 S_local ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "簇内 S_local: 100%|████████████████████████████| 1000/1000 [00:57<00:00, 17.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "联合邻接矩阵构造完成，尺寸为：(30000, 30000)\n",
      "邻接矩阵转换完成！Edge index 维度: torch.Size([2, 1394636])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T16:39:13.124113Z",
     "start_time": "2025-02-12T16:39:13.097086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print(X_train.head())  # 查看训练集的前几行\n",
    "print(X_valid.head())  # 查看验证集的前几行\n",
    "print(X_test.head())  # 查看测试集的前几行\n"
   ],
   "id": "f0b7822dcc809aeb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       LIMIT_BAL       SEX  EDUCATION  MARRIAGE       AGE     PAY_0     PAY_2  \\\n",
      "21180   0.096863 -1.233905  -1.076145 -1.059394  1.147069  0.019367  0.113228   \n",
      "15163  -1.058653  0.810435   0.187360  0.854679 -1.464664 -0.868623  0.113228   \n",
      "11873  -0.519412 -1.233905   0.187360 -1.059394 -0.702908  1.795345  1.787144   \n",
      "17953   0.250932 -1.233905   1.450864 -1.059394  1.147069 -1.756612 -1.560687   \n",
      "15107   0.327966 -1.233905  -2.339649  0.854679  1.038247 -1.756612 -1.560687   \n",
      "\n",
      "          PAY_3     PAY_4     PAY_5  ...  BILL_AMT3  BILL_AMT4  BILL_AMT5  \\\n",
      "21180  0.139950  0.192226  0.232958  ...   1.288533   0.997997   0.264811   \n",
      "15163  0.139950  0.192226  0.232958  ...  -0.258728  -0.276564  -0.235182   \n",
      "11873  1.815039  2.769422  2.882121  ...   0.627960   0.746747   0.810314   \n",
      "17953 -1.535140 -1.525905 -1.533151  ...  -0.676808  -0.670086  -0.661071   \n",
      "15107 -1.535140 -1.525905 -1.533151  ...  -0.661849  -0.657838  -0.603502   \n",
      "\n",
      "       BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \n",
      "21180   0.318231 -0.042999  0.045256 -0.005454 -0.178731 -0.177756 -0.184814  \n",
      "15163  -0.226828 -0.268779  0.865144 -0.206713 -0.168020 -0.246732 -0.243107  \n",
      "11873   0.882343 -0.042999  0.036573 -0.097925 -0.304752 -0.090561 -0.097374  \n",
      "17953  -0.649614 -0.336754 -0.258648 -0.288304 -0.304752 -0.311803 -0.301400  \n",
      "15107  -0.592963  0.007174 -0.213844 -0.245442 -0.080120 -0.092383  0.595205  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "       LIMIT_BAL       SEX  EDUCATION  MARRIAGE       AGE     PAY_0     PAY_2  \\\n",
      "28533  -0.361560  0.804079   0.192694 -1.056673 -0.474517  0.850536  1.722324   \n",
      "23795  -0.361560  0.804079   0.192694 -1.056673  0.480459 -0.017358  0.091850   \n",
      "17077  -0.207460  0.804079   0.192694  0.884551 -1.217276 -0.017358  0.091850   \n",
      "17333   1.102391  0.804079   0.192694 -1.056673 -0.686734 -0.017358  0.091850   \n",
      "27821   0.254840  0.804079  -1.075030  0.884551 -1.005059 -0.017358  0.091850   \n",
      "\n",
      "          PAY_3     PAY_4     PAY_5  ...  BILL_AMT3  BILL_AMT4  BILL_AMT5  \\\n",
      "28533  1.765894  1.831962  0.217748  ...  -0.326979  -0.300750  -0.194835   \n",
      "23795  0.113724  0.153829  0.217748  ...   1.117101   1.228116   1.347774   \n",
      "17077  1.765894  1.831962  1.973782  ...   0.915672   0.970139   1.175366   \n",
      "17333  0.113724  0.153829  0.217748  ...  -0.316210  -0.336907  -0.324727   \n",
      "27821  0.113724  0.153829  0.217748  ...  -0.362479  -0.493257  -0.581086   \n",
      "\n",
      "       BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \n",
      "28533  -0.135961 -0.357736 -0.012985 -0.342930  0.012368 -0.120870 -0.273714  \n",
      "23795   1.388955  0.036307  0.089419  0.114111  0.012368  0.013683 -0.025079  \n",
      "17077   1.251906  0.364676  0.055284 -0.342930  0.203352 -0.053593 -0.025079  \n",
      "17333  -0.427860 -0.243989 -0.258208 -0.257920 -0.266342 -0.282333 -0.236518  \n",
      "27821  -0.609671 -0.226389 -0.233495 -0.202091 -0.248644 -0.201601 -0.195692  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "       LIMIT_BAL       SEX  EDUCATION  MARRIAGE       AGE     PAY_0     PAY_2  \\\n",
      "15047  -0.911018 -1.231147   1.447438 -1.050402  0.913568  0.015598  0.116769   \n",
      "10555   1.791888  0.812251  -1.093413  0.859419 -0.497627  0.015598 -0.723296   \n",
      "25242  -0.988244  0.812251  -1.093413  0.859419 -1.040394 -0.893040 -0.723296   \n",
      "28635  -0.524888  0.812251   1.447438 -1.050402 -0.063413  1.832875  1.796900   \n",
      "5733   -0.138759  0.812251  -1.093413 -1.050402 -0.063413  0.015598  0.116769   \n",
      "\n",
      "          PAY_3     PAY_4     PAY_5  ...  BILL_AMT3  BILL_AMT4  BILL_AMT5  \\\n",
      "15047  0.147801  0.194468  0.250448  ...  -0.433802  -0.399651  -0.365171   \n",
      "10555 -0.685662  0.194468 -0.632447  ...  -0.628629  -0.636313  -0.093868   \n",
      "25242 -0.685662  0.194468  0.250448  ...  -0.618855  -0.469575  -0.397093   \n",
      "28635  1.814726  1.896598  2.016239  ...   0.768395   0.848029   1.024442   \n",
      "5733   0.147801  0.194468  0.250448  ...  -0.173057  -0.153459  -0.122440   \n",
      "\n",
      "       BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \n",
      "15047  -0.342111 -0.274647 -0.182612 -0.244598 -0.287447 -0.274126 -0.238521  \n",
      "10555  -0.011317 -0.084439 -0.106424 -0.161857  2.023871  0.341034  0.931334  \n",
      "25242  -0.643912 -0.305973 -0.067550  0.298157  0.013443 -0.265093 -0.245115  \n",
      "28635   0.966144 -0.002103  0.077707 -0.315195  0.214707 -0.318302  0.180740  \n",
      "5733   -0.563559  0.945271  0.147612  0.788961  0.348884  0.052311 -0.280339  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Batch-Based Optimization",
   "id": "7bd858fd7f633aac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T21:36:32.340347Z",
     "start_time": "2025-02-12T21:34:57.954924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv, GATConv, GCNConv  # 增加 GCNConv 的导入\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "\n",
    "# 假设设备定义如下\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#########################################\n",
    "# 1. 定义模型\n",
    "#########################################\n",
    "# GraphSAGE 模型（包含残差结构、dropout 以及聚合邻居信息衰减控制，每层隐藏单元数递减至上一层的 3/4）\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout_rate=0.5, agg_decay=1.0):\n",
    "        \"\"\"\n",
    "        参数说明：\n",
    "          in_channels: 输入特征维度\n",
    "          hidden_channels: 第一层的隐藏单元数\n",
    "          out_channels: 输出类别数\n",
    "          num_layers: 图卷积层的总层数（至少为 1）\n",
    "          dropout_rate: dropout 概率\n",
    "          agg_decay: 邻居信息聚合时的衰减因子，第一层乘以 1，第二层乘以 agg_decay，第三层乘以 agg_decay^2，以此类推\n",
    "        \"\"\"\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.residuals = torch.nn.ModuleList()\n",
    "        # 第一层：从 in_channels 到 hidden_channels\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        if in_channels != hidden_channels:\n",
    "            self.residuals.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        else:\n",
    "            self.residuals.append(torch.nn.Identity())\n",
    "        current_hidden = hidden_channels\n",
    "        # 后续每一层：隐藏单元数为上一层的 3/4（向下取整，最小为 1）\n",
    "        for _ in range(num_layers - 1):\n",
    "            next_hidden = max(1, int(current_hidden * (1)))\n",
    "            self.convs.append(SAGEConv(current_hidden, next_hidden))\n",
    "            if current_hidden != next_hidden:\n",
    "                self.residuals.append(torch.nn.Linear(current_hidden, next_hidden))\n",
    "            else:\n",
    "                self.residuals.append(torch.nn.Identity())\n",
    "            current_hidden = next_hidden\n",
    "        # 全连接层：将最后一层的隐藏向量映射到输出类别\n",
    "        self.fc = torch.nn.Linear(current_hidden, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.agg_decay = agg_decay\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        \"\"\"依次通过图卷积层、残差连接、ReLU 和 dropout 提取节点表示，并对每一层的聚合信息乘以衰减因子\"\"\"\n",
    "        for i, (conv, res) in enumerate(zip(self.convs, self.residuals)):\n",
    "            out = conv(x, edge_index)\n",
    "            res_x = res(x)\n",
    "            decay_factor = self.agg_decay ** i  # 第一层: agg_decay^0 = 1，第二层: agg_decay^1，……\n",
    "            x = self.dropout(torch.relu(decay_factor * out + res_x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.encode(data.x, data.edge_index)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# GAT 模型（包含残差结构、dropout 以及聚合邻居信息衰减控制，每层隐藏单元数递减至上一层的 3/4）\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout_rate=0.5, heads=1, concat=True, agg_decay=1.0):\n",
    "        super(GAT, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.residuals = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "        # 第一层\n",
    "        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout_rate, concat=concat))\n",
    "        out_dim = hidden_channels * heads if concat else hidden_channels\n",
    "        self.batch_norms.append(torch.nn.BatchNorm1d(out_dim))\n",
    "        if in_channels != out_dim:\n",
    "            self.residuals.append(torch.nn.Linear(in_channels, out_dim))\n",
    "        else:\n",
    "            self.residuals.append(torch.nn.Identity())\n",
    "        current_dim = out_dim\n",
    "        # 后续层\n",
    "        for _ in range(num_layers - 1):\n",
    "            next_hidden = max(1, int(current_dim * (1/3)))\n",
    "            self.convs.append(GATConv(current_dim, next_hidden, heads=heads, dropout=dropout_rate, concat=concat))\n",
    "            new_out_dim = next_hidden * heads if concat else next_hidden\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(new_out_dim))\n",
    "            if current_dim != new_out_dim:\n",
    "                self.residuals.append(torch.nn.Linear(current_dim, new_out_dim))\n",
    "            else:\n",
    "                self.residuals.append(torch.nn.Identity())\n",
    "            current_dim = new_out_dim\n",
    "        self.fc = torch.nn.Linear(current_dim, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.agg_decay = agg_decay\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        for i, (conv, res, bn) in enumerate(zip(self.convs, self.residuals, self.batch_norms)):\n",
    "            out = conv(x, edge_index)\n",
    "            res_x = res(x)\n",
    "            decay_factor = self.agg_decay ** i\n",
    "            x = torch.relu(decay_factor * out + res_x)\n",
    "            x = bn(x)\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.encode(data.x, data.edge_index)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout_rate=0.5, agg_decay=1.0):\n",
    "        super(GCN, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.residuals = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "        # 第一层\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "        self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        if in_channels != hidden_channels:\n",
    "            self.residuals.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        else:\n",
    "            self.residuals.append(torch.nn.Identity())\n",
    "        current_hidden = hidden_channels\n",
    "        # 后续层\n",
    "        for _ in range(num_layers - 1):\n",
    "            next_hidden = max(1, int(current_hidden * (1/3)))\n",
    "            self.convs.append(GCNConv(current_hidden, next_hidden))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(next_hidden))\n",
    "            if current_hidden != next_hidden:\n",
    "                self.residuals.append(torch.nn.Linear(current_hidden, next_hidden))\n",
    "            else:\n",
    "                self.residuals.append(torch.nn.Identity())\n",
    "            current_hidden = next_hidden\n",
    "        self.fc = torch.nn.Linear(current_hidden, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.agg_decay = agg_decay\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        for i, (conv, res, bn) in enumerate(zip(self.convs, self.residuals, self.batch_norms)):\n",
    "            out = conv(x, edge_index)\n",
    "            res_x = res(x)\n",
    "            decay_factor = self.agg_decay ** i\n",
    "            x = torch.relu(decay_factor * out + res_x)\n",
    "            x = bn(x)\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.encode(data.x, data.edge_index)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 2. 定义损失函数\n",
    "#########################################\n",
    "# Focal Loss（用于处理类别不平衡）\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None, reduction=\"mean\"):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  \n",
    "        self.reduction = reduction\n",
    "        self.ce = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        if self.alpha is not None:\n",
    "            if isinstance(self.alpha, (list, np.ndarray)):\n",
    "                alpha = inputs.new_tensor(self.alpha)\n",
    "            else:\n",
    "                alpha = self.alpha\n",
    "            at = alpha.gather(0, targets.data)\n",
    "            ce_loss = at * ce_loss\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean() if self.reduction == \"mean\" else focal_loss.sum()\n",
    "\n",
    "# 修改后的普通对比学习损失（不使用 mask）\n",
    "class SupConLoss(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            temperature: 温度参数\n",
    "        \"\"\"\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: [batch_size, n_views, feature_dim]\n",
    "                      要求每个样本至少有两个视图，其间互为正样本，其余为负样本。\n",
    "        Returns:\n",
    "            对比损失（InfoNCE Loss）\n",
    "        \"\"\"\n",
    "        device = features.device\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` 需要形状为 [batch_size, n_views, feature_dim]')\n",
    "        batch_size, n_views, feature_dim = features.shape\n",
    "\n",
    "        # 将多个视图拼接为 [batch_size*n_views, feature_dim] 并归一化\n",
    "        features = features.view(batch_size * n_views, feature_dim)\n",
    "        features = torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "\n",
    "        similarity_matrix = torch.matmul(features, features.T) / self.temperature\n",
    "\n",
    "        # 构造正样本掩码：同一原始样本的不同视图为正样本\n",
    "        labels = torch.arange(batch_size, device=device).repeat_interleave(n_views)\n",
    "        mask = torch.eq(labels.unsqueeze(1), labels.unsqueeze(0)).float()\n",
    "        self_mask = torch.eye(mask.shape[0], device=device)\n",
    "        mask = mask - self_mask\n",
    "\n",
    "        exp_sim = torch.exp(similarity_matrix) * (1 - self_mask)\n",
    "        denom = exp_sim.sum(dim=1, keepdim=True) + 1e-8\n",
    "\n",
    "        log_prob = similarity_matrix - torch.log(denom)\n",
    "        numerator = (mask * log_prob).sum(dim=1)\n",
    "        pos_count = mask.sum(dim=1) + 1e-8\n",
    "        loss = - (numerator / pos_count)\n",
    "        return loss.mean()\n",
    "\n",
    "#########################################\n",
    "# 3. 数据增强方法\n",
    "#########################################\n",
    "def perturb_features(features, noise_level=0.1):\n",
    "    \"\"\"对特征进行扰动，生成增强视图\"\"\"\n",
    "    noise = torch.randn_like(features) * noise_level\n",
    "    return features + noise\n",
    "\n",
    "def augment_node_drop(features, edge_index, drop_prob=0.1):\n",
    "    \"\"\"以一定概率丢弃节点及其相关边\"\"\"\n",
    "    if isinstance(drop_prob, (list, tuple)):\n",
    "        drop_prob = float(drop_prob[0])\n",
    "    num_nodes = features.shape[0]\n",
    "    keep_mask = (torch.rand(num_nodes, device=features.device) > drop_prob)\n",
    "    features_aug = features * keep_mask.unsqueeze(1).float()\n",
    "    src, dst = edge_index\n",
    "    valid_edge_mask = keep_mask[src] & keep_mask[dst]\n",
    "    edge_index_aug = edge_index[:, valid_edge_mask]\n",
    "    return features_aug, edge_index_aug\n",
    "\n",
    "def augment_edge_drop(features, edge_index, drop_prob=0.1):\n",
    "    \"\"\"以一定概率删除部分边（节点及其特征保持不变）\"\"\"\n",
    "    if isinstance(drop_prob, (list, tuple)):\n",
    "        drop_prob = float(drop_prob[0])\n",
    "    num_edges = edge_index.shape[1]\n",
    "    mask = (torch.rand(num_edges, device=edge_index.device) > drop_prob)\n",
    "    edge_index_aug = edge_index[:, mask]\n",
    "    return features, edge_index_aug\n",
    "\n",
    "def augment_edge_perturb(features, edge_index, drop_prob=0.1):\n",
    "    \"\"\"\n",
    "    先以一定概率删除部分边，再随机添加数量相当的新边\n",
    "    \"\"\"\n",
    "    if isinstance(drop_prob, (list, tuple)):\n",
    "        drop_prob = float(drop_prob[0])\n",
    "    num_edges = edge_index.shape[1]\n",
    "    num_nodes = features.shape[0]\n",
    "    mask = (torch.rand(num_edges, device=edge_index.device) > drop_prob)\n",
    "    edge_index_dropped = edge_index[:, mask]\n",
    "    num_dropped = num_edges - mask.sum().item()\n",
    "    if num_dropped > 0:\n",
    "        new_edges = torch.randint(0, num_nodes, (2, num_dropped), device=features.device)\n",
    "        edge_index_aug = torch.cat([edge_index_dropped, new_edges], dim=1)\n",
    "    else:\n",
    "        edge_index_aug = edge_index_dropped\n",
    "    return features, edge_index_aug\n",
    "\n",
    "def augment_data(data, aug_method=\"feature\", aug_ratio=0.1):\n",
    "    \"\"\"\n",
    "    根据指定增强方式对图数据进行增强，返回增强后的节点特征和 edge_index\n",
    "    \"\"\"\n",
    "    if aug_method == \"feature\":\n",
    "        x_aug = perturb_features(data.x, noise_level=aug_ratio)\n",
    "        edge_index_aug = data.edge_index  # 图结构保持不变\n",
    "        return x_aug, edge_index_aug\n",
    "    elif aug_method == \"node_drop\":\n",
    "        return augment_node_drop(data.x, data.edge_index, drop_prob=aug_ratio)\n",
    "    elif aug_method == \"edge_drop\":\n",
    "        return augment_edge_drop(data.x, data.edge_index, drop_prob=aug_ratio)\n",
    "    elif aug_method == \"edge_perturb\":\n",
    "        return augment_edge_perturb(data.x, data.edge_index, drop_prob=aug_ratio)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown augmentation method: {aug_method}\")\n",
    "\n",
    "#########################################\n",
    "# 3.1 辅助函数：提取 mini-batch 子图\n",
    "#########################################\n",
    "def get_mini_batch_data(data, batch_node_idx, num_hops):\n",
    "    \"\"\"\n",
    "    对一小批节点（batch_node_idx）提取 k-hop 子图，并 relabel 节点。\n",
    "    返回子图 Data 对象及原始目标节点在子图中的映射索引。\n",
    "    \"\"\"\n",
    "    subset, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "        node_idx=batch_node_idx, num_hops=num_hops, edge_index=data.edge_index, relabel_nodes=True)\n",
    "    sub_data = Data(x=data.x[subset], edge_index=sub_edge_index, y=data.y[subset])\n",
    "    if hasattr(data, 'train_mask'):\n",
    "        sub_data.train_mask = data.train_mask[subset]\n",
    "    if hasattr(data, 'val_mask'):\n",
    "        sub_data.val_mask = data.val_mask[subset]\n",
    "    if hasattr(data, 'test_mask'):\n",
    "        sub_data.test_mask = data.test_mask[subset]\n",
    "    return sub_data, mapping\n",
    "\n",
    "#########################################\n",
    "# 4. 训练函数（预训练 + 微调）——mini-batch 版（基于 k_hop_subgraph）\n",
    "#########################################\n",
    "def pretrain_model(data, model, optimizer, criterion_contrast, num_epochs=200, aug_method=\"feature\", aug_ratio=0.1, batch_size=64):\n",
    "    \"\"\"\n",
    "    预训练阶段：仅使用对比损失训练模型（不计算 Focal Loss）。\n",
    "    当 num_epochs==0 时直接跳过预训练，返回当前模型状态。\n",
    "    使用 mini-batch（基于 k_hop_subgraph）进行训练，且预训练阶段直接返回最后一个 epoch 的模型权重。\n",
    "    \"\"\"\n",
    "    if num_epochs == 0:\n",
    "        print(\"预训练轮次为 0，跳过预训练阶段\")\n",
    "        return model.state_dict()\n",
    "\n",
    "    train_idx = data.train_mask.nonzero(as_tuple=False).view(-1).tolist()\n",
    "    num_hops = len(model.convs)  # 使用模型层数作为子图的 hop 数\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        count = 0\n",
    "        loader = DataLoader(train_idx, batch_size=batch_size, shuffle=True)\n",
    "        for batch in loader:\n",
    "            batch = batch.clone().detach().to(device)\n",
    "            sub_data, mapping = get_mini_batch_data(data, batch, num_hops)\n",
    "            sub_data = sub_data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            x_aug1, edge_index1 = augment_data(sub_data, aug_method, aug_ratio)\n",
    "            x_aug2, edge_index2 = augment_data(sub_data, aug_method, aug_ratio)\n",
    "            embedding_aug1 = model.encode(x_aug1, edge_index1)\n",
    "            embedding_aug2 = model.encode(x_aug2, edge_index2)\n",
    "            # 仅计算 mini-batch 中目标节点的增强表示\n",
    "            target_emb1 = embedding_aug1[mapping]\n",
    "            target_emb2 = embedding_aug2[mapping]\n",
    "            features_aug = torch.stack([target_emb1, target_emb2], dim=1)\n",
    "            loss = criterion_contrast(features_aug)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * len(batch)\n",
    "            count += len(batch)\n",
    "        avg_loss = total_loss / count\n",
    "        print(f\"Pretrain Epoch {epoch+1}/{num_epochs}, Contrast Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return model.state_dict()\n",
    "\n",
    "def fine_tune_model(data, model, optimizer, criterion_focal, num_epochs=50, batch_size=64):\n",
    "    \"\"\"\n",
    "    微调阶段：仅使用 Focal Loss 进行训练（不计算对比损失）。\n",
    "    采用 mini-batch 方式（基于 k_hop_subgraph），仅在目标节点上计算损失。\n",
    "    采用验证集准确率（val_acc）作为保存最佳模型参数的依据，同时在打印时也输出测试集准确率。\n",
    "    \"\"\"\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    train_idx = data.train_mask.nonzero(as_tuple=False).view(-1).tolist()\n",
    "    num_hops = len(model.convs)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        count = 0\n",
    "        loader = DataLoader(train_idx, batch_size=batch_size, shuffle=True)\n",
    "        for batch in loader:\n",
    "            batch = batch.clone().detach().to(device)\n",
    "            sub_data, mapping = get_mini_batch_data(data, batch, num_hops)\n",
    "            sub_data = sub_data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(sub_data)\n",
    "            loss = criterion_focal(out[mapping], sub_data.y[mapping])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * len(batch)\n",
    "            count += len(batch)\n",
    "        avg_loss = total_loss / count\n",
    "        \n",
    "        # 计算验证集和测试集上的准确率\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out_full = model(data)\n",
    "            # 验证集准确率\n",
    "            preds_val = out_full[data.val_mask].argmax(dim=1)\n",
    "            true_val = data.y[data.val_mask]\n",
    "            val_acc = accuracy_score(true_val.cpu(), preds_val.cpu())\n",
    "            \n",
    "            # 测试集准确率\n",
    "            preds_test = out_full[data.test_mask].argmax(dim=1)\n",
    "            true_test = data.y[data.test_mask]\n",
    "            test_acc = accuracy_score(true_test.cpu(), preds_test.cpu())\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Fine-tune Epoch {epoch+1}/{num_epochs}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}, Focal Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict()\n",
    "            \n",
    "    return best_model_state\n",
    "\n",
    "\n",
    "def two_stage_train_model(data, model, optimizer, optimizer_ft, criterion_focal, criterion_contrast,\n",
    "                          pretrain_epochs, finetune_epochs, aug_method=\"feature\", aug_ratio=0.1,\n",
    "                          batch_size=64):\n",
    "    \"\"\"\n",
    "    两阶段训练：\n",
    "      第一阶段：预训练（仅用对比损失，mini-batch 方式）；当 pretrain_epochs 为 0 时跳过预训练阶段\n",
    "      第二阶段：微调（仅用分类损失，mini-batch 方式）。\n",
    "    \"\"\"\n",
    "    if pretrain_epochs > 0:\n",
    "        print(\"========== 开始预训练阶段 ==========\")\n",
    "        pretrain_state = pretrain_model(data, model, optimizer, criterion_contrast,\n",
    "                                        num_epochs=pretrain_epochs,\n",
    "                                        aug_method=aug_method, aug_ratio=aug_ratio,\n",
    "                                        batch_size=batch_size)\n",
    "        model.load_state_dict(pretrain_state)\n",
    "    else:\n",
    "        print(\"========== 跳过预训练阶段 ==========\")\n",
    "\n",
    "    print(\"========== 开始微调阶段 ==========\")\n",
    "    best_finetune_state = fine_tune_model(data, model, optimizer_ft,\n",
    "                                          criterion_focal, num_epochs=finetune_epochs,\n",
    "                                          batch_size=batch_size)\n",
    "    return best_finetune_state\n",
    "\n",
    "#########################################\n",
    "# 5. 封装随机采样超参数组合的函数\n",
    "#########################################\n",
    "def get_continuous_candidates(start, stop, step, decimals):\n",
    "    \"\"\"\n",
    "    生成从 start 到 stop（含）之间，以 step 为步长的候选列表，并保留指定小数位数。\n",
    "    \"\"\"\n",
    "    num_steps = int((stop - start) / step) + 1\n",
    "    return [round(start + i * step, decimals) for i in range(num_steps)]\n",
    "\n",
    "def get_random_hyperparameter_combinations(n_iter):\n",
    "    \"\"\"\n",
    "    随机生成 n_iter 个超参数组合，每个组合包含：\n",
    "      (threshold, random_state, num_layers, hidden_channels, finetune_lr, pretrain_lr,\n",
    "       gamma, alpha_value, aug_method, aug_ratio, pretrain_epochs, temperature,\n",
    "       model_type, dropout_rate, agg_decay)\n",
    "    \"\"\"\n",
    "    # 离散变量候选列表\n",
    "    discrete_candidates = {\n",
    "        'random_state': [3333],\n",
    "        'num_layers': [2],\n",
    "        'hidden_channels': [1024],\n",
    "        'finetune_lr': [0.001],\n",
    "        'pretrain_lr': [0.001, 0.0001, 0.00001],\n",
    "        'aug_method': [\"feature\", \"node_drop\", \"edge_drop\", \"edge_perturb\"],\n",
    "        'pretrain_epochs': [0],  # 允许预训练轮次为 0\n",
    "        'model_type': [\"GCN\"], # \"GraphSAGE\", \"GAT\", \n",
    "        'dropout_rate': [0.6],\n",
    "    }\n",
    "\n",
    "    # 连续变量候选区间及步长\n",
    "    continuous_candidates = {\n",
    "        'threshold': [0.8],\n",
    "        'gamma': [3],\n",
    "        'alpha_value': [0.25],\n",
    "        'aug_ratio': get_continuous_candidates(0.05, 0.25, 0.01, 2),\n",
    "        'temperature': get_continuous_candidates(0.05, 0.1, 0.01, 2),\n",
    "        'agg_decay': [0.3]  # 新增参数，控制邻居信息聚合衰减\n",
    "    }\n",
    "\n",
    "    combinations = []\n",
    "    for _ in range(n_iter):\n",
    "        # 随机采样连续变量\n",
    "        threshold   = random.choice(continuous_candidates['threshold'])\n",
    "        gamma       = random.choice(continuous_candidates['gamma'])\n",
    "        alpha_value = random.choice(continuous_candidates['alpha_value'])\n",
    "        aug_ratio   = random.choice(continuous_candidates['aug_ratio'])\n",
    "        temperature = random.choice(continuous_candidates['temperature'])\n",
    "        agg_decay   = random.choice(continuous_candidates['agg_decay'])\n",
    "        \n",
    "        # 随机采样离散变量\n",
    "        random_state    = random.choice(discrete_candidates['random_state'])\n",
    "        num_layers      = random.choice(discrete_candidates['num_layers'])\n",
    "        hidden_channels = random.choice(discrete_candidates['hidden_channels'])\n",
    "        finetune_lr     = random.choice(discrete_candidates['finetune_lr'])\n",
    "        pretrain_lr     = random.choice(discrete_candidates['pretrain_lr'])\n",
    "        aug_method      = random.choice(discrete_candidates['aug_method'])\n",
    "        pretrain_epochs = random.choice(discrete_candidates['pretrain_epochs'])\n",
    "        model_type      = random.choice(discrete_candidates['model_type'])\n",
    "        dropout_rate    = random.choice(discrete_candidates['dropout_rate'])\n",
    "        \n",
    "        combination = (\n",
    "            threshold,      # 阈值\n",
    "            random_state,   # 随机种子\n",
    "            num_layers,     # 图卷积层数\n",
    "            hidden_channels,# 第一层隐藏单元数\n",
    "            finetune_lr,    # 微调学习率\n",
    "            pretrain_lr,    # 预训练学习率\n",
    "            gamma,          # gamma 参数\n",
    "            alpha_value,    # alpha 参数\n",
    "            aug_method,     # 增强方式\n",
    "            aug_ratio,      # 增强比例\n",
    "            pretrain_epochs,# 预训练轮数（允许 0 轮次，表示跳过预训练）\n",
    "            temperature,    # 对比学习温度\n",
    "            model_type,     # 模型类型\n",
    "            dropout_rate,   # dropout 概率\n",
    "            agg_decay       # 邻居信息聚合衰减因子\n",
    "        )\n",
    "        combinations.append(combination)\n",
    "    return combinations\n",
    "\n",
    "#########################################\n",
    "# 6. 随机搜索超参数并评估模型\n",
    "#########################################\n",
    "def grid_search(X, y, train_mask, valid_mask, test_mask, n_iter):\n",
    "    best_acc = 0.0\n",
    "    best_overall_model_state = None\n",
    "    best_overall_params = None\n",
    "\n",
    "    print(\"Start random search with {} combinations...\".format(n_iter))\n",
    "    hyperparam_combos = get_random_hyperparameter_combinations(n_iter)\n",
    "    for i, (cluster_threshold, random_state, num_layers, hidden_channels, finetune_lr,\n",
    "            pretrain_lr, gamma, alpha_value, aug_method, aug_ratio, pretrain_epochs, temperature,\n",
    "            model_type, dropout_rate, agg_decay) in enumerate(hyperparam_combos):\n",
    "        print(f\"\\nTesting combination {i+1}: cluster_threshold={cluster_threshold:.4f}, random_state={random_state}, \"\n",
    "              f\"layers={num_layers}, hidden_channels={hidden_channels}, finetune_lr={finetune_lr}, \"\n",
    "              f\"pretrain_lr={pretrain_lr}, gamma={gamma:.4f}, alpha={alpha_value:.4f}, aug_method={aug_method}, \"\n",
    "              f\"aug_ratio={aug_ratio:.4f}, pretrain_epochs={pretrain_epochs}, temperature={temperature:.4f}, \"\n",
    "              f\"model_type={model_type}, dropout_rate={dropout_rate:.4f}, agg_decay={agg_decay:.4f}\")\n",
    "\n",
    "        # 假设 adj_matrix 与 prototype_indices 已经初始化，此处调用自定义的 adjacency_to_edge_index 函数\n",
    "        edge_index = adjacency_to_edge_index(adj_matrix, prototype_indices, proto_threshold=0, cluster_threshold=cluster_threshold).to(device)\n",
    "        \n",
    "        X_tensor = torch.tensor(X.values, dtype=torch.float)\n",
    "        y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "        data = Data(x=X_tensor, y=y_tensor, edge_index=edge_index,\n",
    "                    train_mask=train_mask, val_mask=valid_mask, test_mask=test_mask).to(device)\n",
    "\n",
    "        # 根据 model_type 选择模型，并传入 agg_decay 参数\n",
    "        if model_type == \"GraphSAGE\":\n",
    "            model = GraphSAGE(in_channels=X.shape[1], hidden_channels=hidden_channels,\n",
    "                              out_channels=len(np.unique(y)), num_layers=num_layers, dropout_rate=dropout_rate,\n",
    "                              agg_decay=agg_decay).to(device)\n",
    "        elif model_type == \"GAT\":\n",
    "            model = GAT(in_channels=X.shape[1], hidden_channels=hidden_channels,\n",
    "                        out_channels=len(np.unique(y)), num_layers=num_layers, dropout_rate=dropout_rate,\n",
    "                        agg_decay=agg_decay).to(device)\n",
    "        elif model_type == \"GCN\":\n",
    "            model = GCN(in_channels=X.shape[1], hidden_channels=hidden_channels,\n",
    "                        out_channels=len(np.unique(y)), num_layers=num_layers, dropout_rate=dropout_rate,\n",
    "                        agg_decay=agg_decay).to(device)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=pretrain_lr, weight_decay=1e-3)\n",
    "        optimizer_ft = torch.optim.AdamW(model.parameters(), lr=finetune_lr, weight_decay=1e-3)\n",
    "               \n",
    "        # 设置 focal loss 中的 alpha 参数\n",
    "        alpha_list = [1 - alpha_value, alpha_value]\n",
    "        alpha_tensor = torch.tensor(alpha_list, dtype=torch.float).to(device)\n",
    "        criterion_focal = FocalLoss(gamma=gamma, alpha=alpha_tensor, reduction=\"mean\")\n",
    "        criterion_contrast = SupConLoss(temperature=temperature)\n",
    "\n",
    "        best_model_epoch = two_stage_train_model(data, model, optimizer, optimizer_ft, criterion_focal,\n",
    "                                                 criterion_contrast, pretrain_epochs=pretrain_epochs, finetune_epochs=500,\n",
    "                                                 aug_method=aug_method, aug_ratio=aug_ratio, batch_size=30000)\n",
    "        model.load_state_dict(best_model_epoch)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_out = model(data)\n",
    "            preds = test_out[data.test_mask].argmax(dim=1)\n",
    "            test_acc = accuracy_score(data.y[data.test_mask].cpu(), preds.cpu())\n",
    "        print(f\"Test Accuracy for current combination: {test_acc:.4f}\")\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_overall_model_state = best_model_epoch\n",
    "            best_overall_params = (cluster_threshold, random_state, num_layers, hidden_channels,\n",
    "                                   finetune_lr, pretrain_lr, gamma, alpha_value, aug_method, aug_ratio,\n",
    "                                   pretrain_epochs, temperature, model_type, dropout_rate, agg_decay)\n",
    "\n",
    "    return best_overall_params, best_overall_model_state\n",
    "\n",
    "#########################################\n",
    "# 7. 主程序：加载数据、随机搜索超参数、加载最佳模型并评估\n",
    "#########################################\n",
    "# 假设 X, y, train_mask, valid_mask, test_mask 已提前加载，且全局 adj_matrix 与 prototype_indices 已初始化\n",
    "best_params, best_model_state = grid_search(X, y, train_mask, valid_mask, test_mask, n_iter=10)\n",
    "print(\"\\nBest Hyperparameters:\", best_params)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "d4dd0293ef200878",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start random search with 10 combinations...\n",
      "\n",
      "Testing combination 1: cluster_threshold=0.8000, random_state=3333, layers=2, hidden_channels=1024, finetune_lr=0.001, pretrain_lr=1e-05, gamma=3.0000, alpha=0.2500, aug_method=feature, aug_ratio=0.0700, pretrain_epochs=0, temperature=0.0900, model_type=GCN, dropout_rate=0.6000, agg_decay=0.3000\n",
      "邻接矩阵转换完成！Edge index 维度: torch.Size([2, 955570])\n",
      "========== 跳过预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Fine-tune Epoch 10/500, Val Acc: 0.7733, Test Acc: 0.7728, Focal Loss: 0.0913\n",
      "Fine-tune Epoch 20/500, Val Acc: 0.8037, Test Acc: 0.8025, Focal Loss: 0.0575\n",
      "Fine-tune Epoch 30/500, Val Acc: 0.8020, Test Acc: 0.8057, Focal Loss: 0.0502\n",
      "Fine-tune Epoch 40/500, Val Acc: 0.8117, Test Acc: 0.8110, Focal Loss: 0.0449\n",
      "Fine-tune Epoch 50/500, Val Acc: 0.8087, Test Acc: 0.8103, Focal Loss: 0.0399\n",
      "Fine-tune Epoch 60/500, Val Acc: 0.8110, Test Acc: 0.8095, Focal Loss: 0.0381\n",
      "Fine-tune Epoch 70/500, Val Acc: 0.8070, Test Acc: 0.8088, Focal Loss: 0.0348\n",
      "Fine-tune Epoch 80/500, Val Acc: 0.8083, Test Acc: 0.8088, Focal Loss: 0.0333\n",
      "Fine-tune Epoch 90/500, Val Acc: 0.8090, Test Acc: 0.8083, Focal Loss: 0.0317\n",
      "Fine-tune Epoch 100/500, Val Acc: 0.8070, Test Acc: 0.8063, Focal Loss: 0.0316\n",
      "Fine-tune Epoch 110/500, Val Acc: 0.8090, Test Acc: 0.8065, Focal Loss: 0.0294\n",
      "Fine-tune Epoch 120/500, Val Acc: 0.8043, Test Acc: 0.8042, Focal Loss: 0.0288\n",
      "Fine-tune Epoch 130/500, Val Acc: 0.8073, Test Acc: 0.8047, Focal Loss: 0.0275\n",
      "Fine-tune Epoch 140/500, Val Acc: 0.8050, Test Acc: 0.8028, Focal Loss: 0.0270\n",
      "Fine-tune Epoch 150/500, Val Acc: 0.7983, Test Acc: 0.7988, Focal Loss: 0.0267\n",
      "Fine-tune Epoch 160/500, Val Acc: 0.8070, Test Acc: 0.8033, Focal Loss: 0.0262\n",
      "Fine-tune Epoch 170/500, Val Acc: 0.8013, Test Acc: 0.8000, Focal Loss: 0.0250\n",
      "Fine-tune Epoch 180/500, Val Acc: 0.8007, Test Acc: 0.7990, Focal Loss: 0.0252\n",
      "Fine-tune Epoch 190/500, Val Acc: 0.8010, Test Acc: 0.7987, Focal Loss: 0.0258\n",
      "Fine-tune Epoch 200/500, Val Acc: 0.7997, Test Acc: 0.7968, Focal Loss: 0.0242\n",
      "Fine-tune Epoch 210/500, Val Acc: 0.8007, Test Acc: 0.7980, Focal Loss: 0.0241\n",
      "Fine-tune Epoch 220/500, Val Acc: 0.8017, Test Acc: 0.7972, Focal Loss: 0.0237\n",
      "Fine-tune Epoch 230/500, Val Acc: 0.8017, Test Acc: 0.7973, Focal Loss: 0.0235\n",
      "Fine-tune Epoch 240/500, Val Acc: 0.8003, Test Acc: 0.7975, Focal Loss: 0.0233\n",
      "Fine-tune Epoch 250/500, Val Acc: 0.8007, Test Acc: 0.7937, Focal Loss: 0.0235\n",
      "Fine-tune Epoch 260/500, Val Acc: 0.8007, Test Acc: 0.7963, Focal Loss: 0.0226\n",
      "Fine-tune Epoch 270/500, Val Acc: 0.8000, Test Acc: 0.7938, Focal Loss: 0.0228\n",
      "Fine-tune Epoch 280/500, Val Acc: 0.7980, Test Acc: 0.7940, Focal Loss: 0.0223\n",
      "Fine-tune Epoch 290/500, Val Acc: 0.8007, Test Acc: 0.7972, Focal Loss: 0.0225\n",
      "Fine-tune Epoch 300/500, Val Acc: 0.7980, Test Acc: 0.7967, Focal Loss: 0.0223\n",
      "Fine-tune Epoch 310/500, Val Acc: 0.7997, Test Acc: 0.7948, Focal Loss: 0.0220\n",
      "Fine-tune Epoch 320/500, Val Acc: 0.7997, Test Acc: 0.7947, Focal Loss: 0.0219\n",
      "Fine-tune Epoch 330/500, Val Acc: 0.8007, Test Acc: 0.7950, Focal Loss: 0.0223\n",
      "Fine-tune Epoch 340/500, Val Acc: 0.7977, Test Acc: 0.7945, Focal Loss: 0.0219\n",
      "Fine-tune Epoch 350/500, Val Acc: 0.7993, Test Acc: 0.7955, Focal Loss: 0.0215\n",
      "Fine-tune Epoch 360/500, Val Acc: 0.7983, Test Acc: 0.7962, Focal Loss: 0.0215\n",
      "Fine-tune Epoch 370/500, Val Acc: 0.7987, Test Acc: 0.7938, Focal Loss: 0.0214\n",
      "Fine-tune Epoch 380/500, Val Acc: 0.7990, Test Acc: 0.7942, Focal Loss: 0.0215\n",
      "Fine-tune Epoch 390/500, Val Acc: 0.7983, Test Acc: 0.7930, Focal Loss: 0.0212\n",
      "Fine-tune Epoch 400/500, Val Acc: 0.7970, Test Acc: 0.7935, Focal Loss: 0.0209\n",
      "Fine-tune Epoch 410/500, Val Acc: 0.7983, Test Acc: 0.7943, Focal Loss: 0.0209\n",
      "Fine-tune Epoch 420/500, Val Acc: 0.8000, Test Acc: 0.7933, Focal Loss: 0.0208\n",
      "Fine-tune Epoch 430/500, Val Acc: 0.7987, Test Acc: 0.7942, Focal Loss: 0.0209\n",
      "Fine-tune Epoch 440/500, Val Acc: 0.7970, Test Acc: 0.7930, Focal Loss: 0.0208\n",
      "Fine-tune Epoch 450/500, Val Acc: 0.7987, Test Acc: 0.7950, Focal Loss: 0.0207\n",
      "Fine-tune Epoch 460/500, Val Acc: 0.7997, Test Acc: 0.7947, Focal Loss: 0.0208\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[54], line 592\u001B[0m\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m best_overall_params, best_overall_model_state\n\u001B[1;32m    588\u001B[0m \u001B[38;5;66;03m#########################################\u001B[39;00m\n\u001B[1;32m    589\u001B[0m \u001B[38;5;66;03m# 7. 主程序：加载数据、随机搜索超参数、加载最佳模型并评估\u001B[39;00m\n\u001B[1;32m    590\u001B[0m \u001B[38;5;66;03m#########################################\u001B[39;00m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;66;03m# 假设 X, y, train_mask, valid_mask, test_mask 已提前加载，且全局 adj_matrix 与 prototype_indices 已初始化\u001B[39;00m\n\u001B[0;32m--> 592\u001B[0m best_params, best_model_state \u001B[38;5;241m=\u001B[39m \u001B[43mgrid_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    593\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mBest Hyperparameters:\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_params)\n",
      "Cell \u001B[0;32mIn[54], line 569\u001B[0m, in \u001B[0;36mgrid_search\u001B[0;34m(X, y, train_mask, valid_mask, test_mask, n_iter)\u001B[0m\n\u001B[1;32m    566\u001B[0m criterion_focal \u001B[38;5;241m=\u001B[39m FocalLoss(gamma\u001B[38;5;241m=\u001B[39mgamma, alpha\u001B[38;5;241m=\u001B[39malpha_tensor, reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmean\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    567\u001B[0m criterion_contrast \u001B[38;5;241m=\u001B[39m SupConLoss(temperature\u001B[38;5;241m=\u001B[39mtemperature)\n\u001B[0;32m--> 569\u001B[0m best_model_epoch \u001B[38;5;241m=\u001B[39m \u001B[43mtwo_stage_train_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_ft\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion_focal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    570\u001B[0m \u001B[43m                                         \u001B[49m\u001B[43mcriterion_contrast\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpretrain_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpretrain_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinetune_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    571\u001B[0m \u001B[43m                                         \u001B[49m\u001B[43maug_method\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maug_method\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maug_ratio\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maug_ratio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m30000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    572\u001B[0m model\u001B[38;5;241m.\u001B[39mload_state_dict(best_model_epoch)\n\u001B[1;32m    573\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n",
      "Cell \u001B[0;32mIn[54], line 431\u001B[0m, in \u001B[0;36mtwo_stage_train_model\u001B[0;34m(data, model, optimizer, optimizer_ft, criterion_focal, criterion_contrast, pretrain_epochs, finetune_epochs, aug_method, aug_ratio, batch_size)\u001B[0m\n\u001B[1;32m    428\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m========== 跳过预训练阶段 ==========\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    430\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m========== 开始微调阶段 ==========\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 431\u001B[0m best_finetune_state \u001B[38;5;241m=\u001B[39m \u001B[43mfine_tune_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_ft\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    432\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mcriterion_focal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfinetune_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    433\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m best_finetune_state\n",
      "Cell \u001B[0;32mIn[54], line 384\u001B[0m, in \u001B[0;36mfine_tune_model\u001B[0;34m(data, model, optimizer, criterion_focal, num_epochs, batch_size)\u001B[0m\n\u001B[1;32m    381\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m    382\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m--> 384\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch)\n\u001B[1;32m    385\u001B[0m     count \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch)\n\u001B[1;32m    386\u001B[0m avg_loss \u001B[38;5;241m=\u001B[39m total_loss \u001B[38;5;241m/\u001B[39m count\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "Testing combination 1: cluster_threshold=0.4000, random_state=333, layers=2, hidden_channels=200, finetune_lr=0.01, pretrain_lr=0.0001, gamma=3.0000, alpha=0.2500, aug_method=node_drop, aug_ratio=0.1700, pretrain_epochs=0, temperature=0.0500, model_type=GCN, dropout_rate=0.3000, agg_decay=0.3000\n",
    "Fine-tune Epoch 400/500, Val Acc: 0.8517, Test Acc: 0.7848, Focal Loss: 0.0086"
   ],
   "id": "6ce2213279d2363d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GPT给的缓解过拟合建议",
   "id": "9b81ea0f437da1c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5acf0643ef32fe44"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
