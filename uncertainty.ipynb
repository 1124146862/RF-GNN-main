{
 "cells": [
  {
   "cell_type": "code",
   "id": "92b3eea7-e868-4e45-bbbe-04e907049cce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T20:37:30.512019Z",
     "start_time": "2025-02-07T20:37:30.485384Z"
    }
   },
   "source": [
    "# util function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.sparse import csr_matrix\n",
    "import torch\n",
    "\n",
    "def standard_input(X):\n",
    "    # 标准化输入\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "def load_data_SGER_RAW(random_state=42):\n",
    "    # 读取以空格分隔的SGER CSV文件\n",
    "    path = '/home/gehongfei/project/TabGNN/dataset/SGER1000.csv'\n",
    "    df = pd.read_csv(path, sep='\\s+')\n",
    "    # 确保 'kredit' 列存在\n",
    "    if 'kredit' not in df.columns:\n",
    "        print(\"Error: 'kredit' column not found.\")\n",
    "        return None, None, None, None, None, None\n",
    "    # 目标变量和特征\n",
    "    y = df['kredit']\n",
    "    X = df.drop(columns=['kredit'])\n",
    "    # 划分训练集、验证集和测试集\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=random_state, stratify=y)\n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=2/3, random_state=random_state, stratify=y_temp)\n",
    "    # 计算节点数并创建 mask\n",
    "    num_nodes = len(df)\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    # 获取索引并设置 mask\n",
    "    train_mask[X_train.index] = True\n",
    "    val_mask[X_valid.index] = True\n",
    "    test_mask[X_test.index] = True\n",
    "    # 标准化输入\n",
    "    X = standard_input(X)\n",
    "    X_train = standard_input(X_train)\n",
    "    X_valid = standard_input(X_valid)\n",
    "    X_test = standard_input(X_test)\n",
    "    return X, y, X_train, X_valid, X_test, y_train, y_valid, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "X, y, X_train, X_valid, X_test, y_train, y_valid, y_test, train_mask, val_mask, test_mask = load_data_SGER_RAW()\n",
    "X.shape\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "9b08195c-9e5f-4d48-864d-557e8b3e5e58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T20:53:52.436771Z",
     "start_time": "2025-02-07T20:53:34.717514Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import copy\n",
    "import itertools\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, classification_report\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----------------------------\n",
    "# 1. 构造数据集\n",
    "# 请确保 load_data_SGER_RAW() 已定义，返回以下变量：\n",
    "# X, y, X_train, X_valid, X_test, y_train, y_valid, y_test, train_mask, val_mask, test_mask\n",
    "X, y, X_train, X_valid, X_test, y_train, y_valid, y_test, train_mask, val_mask, test_mask = load_data_SGER_RAW()\n",
    "\n",
    "# ----------------------------\n",
    "# 2. 定义残差网络模型\n",
    "# ----------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dropout_rate):\n",
    "        \"\"\"\n",
    "        一个残差块，计算：\n",
    "            out = dropout( ReLU( Linear(x) ) )\n",
    "        并添加（可能经过投影）的残差连接。\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # 当输入输出维度不同时，对输入做投影\n",
    "        if in_dim != out_dim:\n",
    "            self.residual_transform = nn.Linear(in_dim, out_dim)\n",
    "        else:\n",
    "            self.residual_transform = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.linear(x))\n",
    "        out = self.dropout(out)\n",
    "        if self.residual_transform is not None:\n",
    "            residual = self.residual_transform(x)\n",
    "        else:\n",
    "            residual = x\n",
    "        out = out + residual\n",
    "        return out\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, num_layers, dropout_rate):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: 输入特征数。\n",
    "            hidden_size: 第一层隐藏层神经元个数。\n",
    "            num_layers: 总层数，包括输入层与后续残差块。（例如 num_layers=4 表示输入层+3 个残差块）\n",
    "            dropout_rate: 每个块的 dropout 概率。\n",
    "        \"\"\"\n",
    "        super(ResidualMLP, self).__init__()\n",
    "        # 输入层：投影 input_dim -> hidden_size\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_size)\n",
    "        \n",
    "        # 构造若干个残差块，每个块使隐藏层维度降低 3/4\n",
    "        self.hidden_blocks = nn.ModuleList()\n",
    "        current_dim = hidden_size\n",
    "        for i in range(num_layers - 1):  # 输入层已计入，总共构造 num_layers-1 个块\n",
    "            new_dim = max(1, int(current_dim * 0.6))\n",
    "            block = ResidualBlock(in_dim=current_dim, out_dim=new_dim, dropout_rate=dropout_rate)\n",
    "            self.hidden_blocks.append(block)\n",
    "            current_dim = new_dim\n",
    "        \n",
    "        # 输出层：从最后一个隐藏层维度投影到 1\n",
    "        self.output_layer = nn.Linear(current_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.input_layer(x))\n",
    "        for block in self.hidden_blocks:\n",
    "            out = block(out)\n",
    "        out = torch.sigmoid(self.output_layer(out))\n",
    "        return out\n",
    "\n",
    "    def get_hidden_representation(self, x):\n",
    "        \"\"\"\n",
    "        返回经过输入层和残差块后的隐藏层表示（即输出层之前的结果）。\n",
    "        \"\"\"\n",
    "        out = F.relu(self.input_layer(x))\n",
    "        for block in self.hidden_blocks:\n",
    "            out = block(out)\n",
    "        return out\n",
    "\n",
    "# 定义分类器类，包含训练、评估、预测及不确定性估计等方法\n",
    "class BinaryClassifier:\n",
    "    def __init__(self, input_dim, hidden_size=64, num_layers=3, dropout_rate=0.5, learning_rate=0.001):\n",
    "        self.model = ResidualMLP(input_dim, hidden_size, num_layers, dropout_rate)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def train(self, X_train, y_train, X_valid, y_valid, epochs=50, batch_size=32):\n",
    "        train_dataset = TensorDataset(torch.tensor(X_train.values, dtype=torch.float32), \n",
    "                                      torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1))\n",
    "        valid_dataset = TensorDataset(torch.tensor(X_valid.values, dtype=torch.float32), \n",
    "                                      torch.tensor(y_valid.values, dtype=torch.float32).unsqueeze(1))\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        best_model_state = None\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(X_batch)\n",
    "                loss = self.criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            valid_loss = self.evaluate(valid_loader)\n",
    "\n",
    "            # 计算验证集上的 F1 得分\n",
    "            self.model.eval()\n",
    "            val_preds = []\n",
    "            val_targets = []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in valid_loader:\n",
    "                    outputs = self.model(X_batch)\n",
    "                    preds = (outputs.squeeze() > 0.5).int()\n",
    "                    val_preds.extend(preds.cpu().numpy())\n",
    "                    val_targets.extend(y_batch.squeeze().cpu().numpy())\n",
    "            val_f1 = f1_score(val_targets, val_preds)\n",
    "\n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                best_epoch = epoch\n",
    "                best_model_state = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "            # 可根据需要打印每个 epoch 的训练过程，这里注释掉了\n",
    "            # print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "            #       f\"Validation Loss: {valid_loss:.4f}, Validation F1: {val_f1:.4f}\")\n",
    "\n",
    "        # 恢复验证集上 F1 最佳的模型参数\n",
    "        if best_model_state is not None:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "            print(f\"Loaded best model from epoch {best_epoch+1} with Validation F1: {best_f1:.4f}\")\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in data_loader:\n",
    "                outputs = self.model(X_batch)\n",
    "                loss = self.criterion(outputs, y_batch)\n",
    "                total_loss += loss.item()\n",
    "        return total_loss / len(data_loader)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, X_data):\n",
    "        self.model.eval()\n",
    "        X_tensor = torch.tensor(X_data.values, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "        predicted_classes = (outputs.squeeze() > 0.5).int()\n",
    "        return predicted_classes\n",
    "\n",
    "    def predict_with_uncertainty(self, X_sample, n_iter=100):\n",
    "        # 启用 dropout 进行不确定性估计\n",
    "        self.model.train()\n",
    "        X_sample = torch.tensor(X_sample.values, dtype=torch.float32)\n",
    "        predictions = torch.zeros(n_iter, X_sample.size(0))\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            outputs = self.model(X_sample)\n",
    "            predictions[i] = outputs.squeeze()\n",
    "\n",
    "        mean_prediction = predictions.mean(dim=0)\n",
    "        uncertainty = predictions.std(dim=0)\n",
    "        predicted_classes = (mean_prediction > 0.5).int()\n",
    "\n",
    "        return predicted_classes, uncertainty\n",
    "\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    def compute_similarity_and_uncertainty(self, X_data, n_iter=30, device='cuda'):\n",
    "        self.model.to(device)  # 确保模型在 GPU 上\n",
    "        self.model.eval()  # 进入评估模式\n",
    "    \n",
    "        X_tensor = torch.tensor(X_data.values, dtype=torch.float32, device=device)\n",
    "    \n",
    "        # 计算隐藏层表示\n",
    "        with torch.no_grad():\n",
    "            hidden_vectors = self.model.get_hidden_representation(X_tensor)\n",
    "    \n",
    "        similarity_matrix = torch.mm(hidden_vectors, hidden_vectors.t())  # 计算余弦相似度\n",
    "    \n",
    "        # 启用 dropout 进行不确定性估计\n",
    "        self.model.train()\n",
    "    \n",
    "        # 初始化预测结果张量，形状为 (n_iter, n_samples)\n",
    "        predictions = torch.zeros(n_iter, X_tensor.size(0), device=device)\n",
    "    \n",
    "        for i in range(n_iter):\n",
    "            outputs = self.model(X_tensor)\n",
    "            predictions[i] = outputs.squeeze()\n",
    "    \n",
    "        # 计算均值和标准差\n",
    "        mean_prediction = predictions.mean(dim=0)\n",
    "        uncertainty = predictions.std(dim=0)\n",
    "    \n",
    "        # 计算预测类别\n",
    "        predicted_classes = (mean_prediction > 0.5).int()\n",
    "    \n",
    "        # 计算不确定性矩阵（优化，不使用循环）\n",
    "        pred_match = (predicted_classes.unsqueeze(1) == predicted_classes.unsqueeze(0)).float()\n",
    "        uncertainty_matrix = pred_match * (uncertainty.unsqueeze(1) + uncertainty.unsqueeze(0))\n",
    "        \n",
    "        # 归一化similarity_matrix中的值，确保相似度时范围在 [0,1]\n",
    "        # 计算 similarity_matrix 的最小值和最大值\n",
    "        sim_min = similarity_matrix.min()\n",
    "        sim_max = similarity_matrix.max()\n",
    "        \n",
    "        # 归一化到 [0,1]\n",
    "        similarity_matrix = (similarity_matrix - sim_min) / (sim_max - sim_min + 1e-8)  # 避免除零\n",
    "\n",
    "        return similarity_matrix, uncertainty_matrix\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3. 随机挑选参数组合进行 Grid Search\n",
    "# ----------------------------\n",
    "# 定义超参数空间\n",
    "param_grid = {\n",
    "    'hidden_size': list(range(64, 513, 32)),\n",
    "    'num_layers': [2, 3, 4, 5],\n",
    "    'dropout_rate': [0.2, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6],\n",
    "    'learning_rate': [0.01, 0.001, 0.005, 0.0005]\n",
    "}\n",
    "\n",
    "# 生成所有可能的参数组合\n",
    "all_combinations = list(itertools.product(param_grid['hidden_size'],\n",
    "                                          param_grid['num_layers'],\n",
    "                                          param_grid['dropout_rate'],\n",
    "                                          param_grid['learning_rate']))\n",
    "print(f\"共有 {len(all_combinations)} 种参数组合。\")\n",
    "\n",
    "# 随机挑选部分组合进行尝试，比如随机挑选 100 个组合\n",
    "n_iter = 10\n",
    "selected_combinations = random.sample(all_combinations, n_iter)\n",
    "print(\"随机挑选的参数组合为：{}个。\".format(len(selected_combinations)))\n",
    "for combo in selected_combinations:\n",
    "    print(f\"  hidden_size={combo[0]}, num_layers={combo[1]}, dropout_rate={combo[2]}, learning_rate={combo[3]}\")\n",
    "\n",
    "grid_search_results = []\n",
    "\n",
    "# 增加打印正在训练第多少个参数组合\n",
    "for idx, (hidden_size, num_layers, dropout_rate, learning_rate) in enumerate(selected_combinations):\n",
    "    print(\"==========================================\")\n",
    "    print(f\"Training combination {idx+1}/{len(selected_combinations)}: hidden_size={hidden_size}, num_layers={num_layers}, dropout_rate={dropout_rate}, learning_rate={learning_rate}\")\n",
    "    \n",
    "    # 根据当前参数组合初始化分类器\n",
    "    classifier = BinaryClassifier(\n",
    "        input_dim=X.shape[1],\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        dropout_rate=dropout_rate,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    \n",
    "    # 训练模型（例如 30 个 epochs，batch_size 为 64）\n",
    "    classifier.train(X_train, y_train, X_valid, y_valid, epochs=30, batch_size=64)\n",
    "    \n",
    "    # 在验证集上计算 F1 分数\n",
    "    val_preds = classifier.predict(X_valid)\n",
    "    val_f1 = f1_score(y_valid, val_preds)\n",
    "    print(f\"Validation F1 for this model: {val_f1:.4f}\")\n",
    "    \n",
    "    grid_search_results.append({\n",
    "        'model': copy.deepcopy(classifier),  # 保存该模型\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'learning_rate': learning_rate,\n",
    "        'val_f1': val_f1\n",
    "    })\n",
    "\n",
    "# 选择验证集 F1 分数最高的模型\n",
    "best_result = max(grid_search_results, key=lambda x: x['val_f1'])\n",
    "best_classifier = best_result['model']\n",
    "\n",
    "print(\"==========================================\")\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(f\"  hidden_size: {best_result['hidden_size']}\")\n",
    "print(f\"  num_layers: {best_result['num_layers']}\")\n",
    "print(f\"  dropout_rate: {best_result['dropout_rate']}\")\n",
    "print(f\"  learning_rate: {best_result['learning_rate']}\")\n",
    "print(f\"Best Validation F1 Score: {best_result['val_f1']:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4. 在测试集上评估最佳模型\n",
    "# ----------------------------\n",
    "test_predictions = best_classifier.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "test_recall = recall_score(y_test, test_predictions)\n",
    "test_f1 = f1_score(y_test, test_predictions)\n",
    "test_precision = precision_score(y_test, test_predictions)\n",
    "\n",
    "print(\"--------- Test Metrics ---------\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Recall:   {test_recall:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Test Precision:{test_precision:.4f}\")\n",
    "\n",
    "print(\"--------- Classification Report ---------\")\n",
    "print(classification_report(y_test, test_predictions))\n",
    "\n",
    "# ----------------------------\n",
    "# 5. 示例：对单个样本进行预测（并估计不确定性）\n",
    "# ----------------------------\n",
    "sample = X_test.iloc[[0]]\n",
    "print(\"Sample to Predict:\")\n",
    "print(sample)\n",
    "\n",
    "predicted_class, uncertainty = best_classifier.predict_with_uncertainty(sample)\n",
    "print(\"Predicted Class:\", predicted_class.item())\n",
    "print(\"Uncertainty:\", uncertainty.item())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有 1680 种参数组合。\n",
      "随机挑选的参数组合为：10个。\n",
      "  hidden_size=192, num_layers=3, dropout_rate=0.3, learning_rate=0.0005\n",
      "  hidden_size=64, num_layers=4, dropout_rate=0.35, learning_rate=0.01\n",
      "  hidden_size=160, num_layers=3, dropout_rate=0.45, learning_rate=0.0005\n",
      "  hidden_size=256, num_layers=5, dropout_rate=0.4, learning_rate=0.005\n",
      "  hidden_size=384, num_layers=3, dropout_rate=0.5, learning_rate=0.01\n",
      "  hidden_size=288, num_layers=4, dropout_rate=0.6, learning_rate=0.0005\n",
      "  hidden_size=512, num_layers=5, dropout_rate=0.45, learning_rate=0.0005\n",
      "  hidden_size=352, num_layers=4, dropout_rate=0.6, learning_rate=0.01\n",
      "  hidden_size=416, num_layers=4, dropout_rate=0.2, learning_rate=0.005\n",
      "  hidden_size=384, num_layers=5, dropout_rate=0.4, learning_rate=0.001\n",
      "==========================================\n",
      "Training combination 1/10: hidden_size=192, num_layers=3, dropout_rate=0.3, learning_rate=0.0005\n",
      "Loaded best model from epoch 8 with Validation F1: 0.6316\n",
      "Validation F1 for this model: 0.6316\n",
      "==========================================\n",
      "Training combination 2/10: hidden_size=64, num_layers=4, dropout_rate=0.35, learning_rate=0.01\n",
      "Loaded best model from epoch 4 with Validation F1: 0.6429\n",
      "Validation F1 for this model: 0.6429\n",
      "==========================================\n",
      "Training combination 3/10: hidden_size=160, num_layers=3, dropout_rate=0.45, learning_rate=0.0005\n",
      "Loaded best model from epoch 7 with Validation F1: 0.6909\n",
      "Validation F1 for this model: 0.6909\n",
      "==========================================\n",
      "Training combination 4/10: hidden_size=256, num_layers=5, dropout_rate=0.4, learning_rate=0.005\n",
      "Loaded best model from epoch 2 with Validation F1: 0.6122\n",
      "Validation F1 for this model: 0.6122\n",
      "==========================================\n",
      "Training combination 5/10: hidden_size=384, num_layers=3, dropout_rate=0.5, learning_rate=0.01\n",
      "Loaded best model from epoch 3 with Validation F1: 0.6545\n",
      "Validation F1 for this model: 0.6545\n",
      "==========================================\n",
      "Training combination 6/10: hidden_size=288, num_layers=4, dropout_rate=0.6, learning_rate=0.0005\n",
      "Loaded best model from epoch 6 with Validation F1: 0.6786\n",
      "Validation F1 for this model: 0.6786\n",
      "==========================================\n",
      "Training combination 7/10: hidden_size=512, num_layers=5, dropout_rate=0.45, learning_rate=0.0005\n",
      "Loaded best model from epoch 3 with Validation F1: 0.6792\n",
      "Validation F1 for this model: 0.6792\n",
      "==========================================\n",
      "Training combination 8/10: hidden_size=352, num_layers=4, dropout_rate=0.6, learning_rate=0.01\n",
      "Loaded best model from epoch 5 with Validation F1: 0.6333\n",
      "Validation F1 for this model: 0.6333\n",
      "==========================================\n",
      "Training combination 9/10: hidden_size=416, num_layers=4, dropout_rate=0.2, learning_rate=0.005\n",
      "Loaded best model from epoch 3 with Validation F1: 0.6667\n",
      "Validation F1 for this model: 0.6667\n",
      "==========================================\n",
      "Training combination 10/10: hidden_size=384, num_layers=5, dropout_rate=0.4, learning_rate=0.001\n",
      "Loaded best model from epoch 3 with Validation F1: 0.6786\n",
      "Validation F1 for this model: 0.6786\n",
      "==========================================\n",
      "Best Hyperparameters:\n",
      "  hidden_size: 160\n",
      "  num_layers: 3\n",
      "  dropout_rate: 0.45\n",
      "  learning_rate: 0.0005\n",
      "Best Validation F1 Score: 0.6909\n",
      "--------- Test Metrics ---------\n",
      "Test Accuracy: 0.7450\n",
      "Test Recall:   0.4000\n",
      "Test F1 Score: 0.4848\n",
      "Test Precision:0.6154\n",
      "--------- Classification Report ---------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.89      0.83       140\n",
      "           1       0.62      0.40      0.48        60\n",
      "\n",
      "    accuracy                           0.74       200\n",
      "   macro avg       0.70      0.65      0.66       200\n",
      "weighted avg       0.73      0.74      0.73       200\n",
      "\n",
      "Sample to Predict:\n",
      "   laufkont  laufzeit     moral     verw     hoehe  sparkont   beszeit  \\\n",
      "0 -0.490296 -1.294443 -0.579758  0.66335 -1.009094 -0.721009 -1.067797   \n",
      "\n",
      "       rate    famges    buerge  wohnzeit      verm     alter  weitkred  \\\n",
      "0  0.008981  1.724882 -0.320641 -1.585393 -0.357607 -1.099408  0.444788   \n",
      "\n",
      "       wohn  bishkred     beruf      pers    telef   gastarb  \n",
      "0  0.135954  -0.66218 -1.408326  0.377964 -0.84226  0.175863  \n",
      "Predicted Class: 0\n",
      "Uncertainty: 0.03154999762773514\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "864e960c-6ae4-4b0a-8431-2b6857a2f8f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T20:56:22.705306Z",
     "start_time": "2025-02-07T20:56:22.350869Z"
    }
   },
   "source": [
    "# Predict a single sample\n",
    "\n",
    "sample = X_test.iloc[[0]]\n",
    "print(\"Sample to Predict:\", sample)\n",
    "\n",
    "predicted_classes, uncertainty = classifier.predict_with_uncertainty(sample)\n",
    "\n",
    "print(\"Predicted Class:\", predicted_classes.item())\n",
    "print(\"Uncertainty:\", uncertainty.item())\n",
    "\n",
    "# Compute similarity and uncertainty matrices\n",
    "similarity_matrix, uncertainty_matrix = classifier.compute_similarity_and_uncertainty(pd.concat([X_train, X_valid, X_test]))\n",
    "print(\"Similarity Matrix:\", similarity_matrix)\n",
    "print(\"Uncertainty Matrix:\", uncertainty_matrix)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample to Predict:    laufkont  laufzeit     moral     verw     hoehe  sparkont   beszeit  \\\n",
      "0 -0.490296 -1.294443 -0.579758  0.66335 -1.009094 -0.721009 -1.067797   \n",
      "\n",
      "       rate    famges    buerge  wohnzeit      verm     alter  weitkred  \\\n",
      "0  0.008981  1.724882 -0.320641 -1.585393 -0.357607 -1.099408  0.444788   \n",
      "\n",
      "       wohn  bishkred     beruf      pers    telef   gastarb  \n",
      "0  0.135954  -0.66218 -1.408326  0.377964 -0.84226  0.175863  \n",
      "Predicted Class: 0\n",
      "Uncertainty: 0.04254193231463432\n",
      "Similarity Matrix: tensor([[0.1640, 0.1559, 0.1615,  ..., 0.1613, 0.1577, 0.1577],\n",
      "        [0.1559, 0.1612, 0.1571,  ..., 0.1518, 0.1639, 0.1567],\n",
      "        [0.1615, 0.1571, 0.1652,  ..., 0.1587, 0.1600, 0.1598],\n",
      "        ...,\n",
      "        [0.1613, 0.1518, 0.1587,  ..., 0.1665, 0.1485, 0.1555],\n",
      "        [0.1577, 0.1639, 0.1600,  ..., 0.1485, 0.1723, 0.1589],\n",
      "        [0.1577, 0.1567, 0.1598,  ..., 0.1555, 0.1589, 0.1603]],\n",
      "       device='cuda:0')\n",
      "Uncertainty Matrix: tensor([[0.0718, 0.0000, 0.0807,  ..., 0.0659, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0754, 0.0000,  ..., 0.0000, 0.0713, 0.0678],\n",
      "        [0.0807, 0.0000, 0.0896,  ..., 0.0748, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0659, 0.0000, 0.0748,  ..., 0.0601, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0713, 0.0000,  ..., 0.0000, 0.0672, 0.0637],\n",
      "        [0.0000, 0.0678, 0.0000,  ..., 0.0000, 0.0637, 0.0601]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T20:56:25.217522Z",
     "start_time": "2025-02-07T20:56:25.201174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 计算邻接矩阵\n",
    "adj_matrix = classifier.compute_adjacency_matrix(similarity_matrix, uncertainty_matrix, threshold=0.8, epsilon=0.3)\n",
    "\n",
    "# 打印邻接矩阵形状\n",
    "print(\"Adjacency matrix shape:\", adj_matrix.shape)\n",
    "\n",
    "# 计算并打印非零元素个数\n",
    "num_nonzero = torch.count_nonzero(adj_matrix).item()\n",
    "print(\"Number of nonzero elements:\", num_nonzero)\n",
    "similarity_matrix"
   ],
   "id": "347eded983370a54",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BinaryClassifier' object has no attribute 'compute_adjacency_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 计算邻接矩阵\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m adj_matrix \u001B[38;5;241m=\u001B[39m \u001B[43mclassifier\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_adjacency_matrix\u001B[49m(similarity_matrix, uncertainty_matrix, threshold\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.8\u001B[39m, epsilon\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# 打印邻接矩阵形状\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAdjacency matrix shape:\u001B[39m\u001B[38;5;124m\"\u001B[39m, adj_matrix\u001B[38;5;241m.\u001B[39mshape)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'BinaryClassifier' object has no attribute 'compute_adjacency_matrix'"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GPU implement\n",
   "id": "476d0ba7ad50d21e"
  },
  {
   "cell_type": "code",
   "id": "fd694c3d-3b31-4947-b299-d042a1e71a4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T20:57:51.504490Z",
     "start_time": "2025-02-07T20:57:41.399822Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import copy\n",
    "import itertools\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, classification_report\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----------------------------\n",
    "# 1. 构造数据集\n",
    "# 请确保 load_data_SGER_RAW() 已定义，返回以下变量：\n",
    "# X, y, X_train, X_valid, X_test, y_train, y_valid, y_test, train_mask, val_mask, test_mask\n",
    "X, y, X_train, X_valid, X_test, y_train, y_valid, y_test, train_mask, val_mask, test_mask = load_data_SGER_RAW()\n",
    "\n",
    "# ----------------------------\n",
    "# 2. 定义残差网络模型\n",
    "# ----------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dropout_rate):\n",
    "        \"\"\"\n",
    "        一个残差块，计算：\n",
    "            out = dropout( ReLU( Linear(x) ) )\n",
    "        并添加（可能经过投影）的残差连接。\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # 当输入输出维度不同时，对输入做投影\n",
    "        if in_dim != out_dim:\n",
    "            self.residual_transform = nn.Linear(in_dim, out_dim)\n",
    "        else:\n",
    "            self.residual_transform = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.linear(x))\n",
    "        out = self.dropout(out)\n",
    "        if self.residual_transform is not None:\n",
    "            residual = self.residual_transform(x)\n",
    "        else:\n",
    "            residual = x\n",
    "        out = out + residual\n",
    "        return out\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, num_layers, dropout_rate):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: 输入特征数。\n",
    "            hidden_size: 第一层隐藏层神经元个数。\n",
    "            num_layers: 总层数，包括输入层与后续残差块。（例如 num_layers=4 表示输入层+3 个残差块）\n",
    "            dropout_rate: 每个块的 dropout 概率。\n",
    "        \"\"\"\n",
    "        super(ResidualMLP, self).__init__()\n",
    "        # 输入层：投影 input_dim -> hidden_size\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_size)\n",
    "        \n",
    "        # 构造若干个残差块，每个块使隐藏层维度降低 3/4\n",
    "        self.hidden_blocks = nn.ModuleList()\n",
    "        current_dim = hidden_size\n",
    "        for i in range(num_layers - 1):  # 输入层已计入，总共构造 num_layers-1 个块\n",
    "            new_dim = max(1, int(current_dim * 0.75))\n",
    "            block = ResidualBlock(in_dim=current_dim, out_dim=new_dim, dropout_rate=dropout_rate)\n",
    "            self.hidden_blocks.append(block)\n",
    "            current_dim = new_dim\n",
    "        \n",
    "        # 输出层：从最后一个隐藏层维度投影到 1\n",
    "        self.output_layer = nn.Linear(current_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.input_layer(x))\n",
    "        for block in self.hidden_blocks:\n",
    "            out = block(out)\n",
    "        out = torch.sigmoid(self.output_layer(out))\n",
    "        return out\n",
    "\n",
    "    def get_hidden_representation(self, x):\n",
    "        \"\"\"\n",
    "        返回经过输入层和残差块后的隐藏层表示（即输出层之前的结果）。\n",
    "        \"\"\"\n",
    "        out = F.relu(self.input_layer(x))\n",
    "        for block in self.hidden_blocks:\n",
    "            out = block(out)\n",
    "        return out\n",
    "\n",
    "# 定义分类器类，包含训练、评估、预测及不确定性估计等方法\n",
    "class BinaryClassifier:\n",
    "    def __init__(self, input_dim, hidden_size=64, num_layers=3, dropout_rate=0.5, learning_rate=0.001):\n",
    "        # 自动检测是否有GPU可用\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = ResidualMLP(input_dim, hidden_size, num_layers, dropout_rate).to(self.device)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def train(self, X_train, y_train, X_valid, y_valid, epochs=50, batch_size=32):\n",
    "        train_dataset = TensorDataset(torch.tensor(X_train.values, dtype=torch.float32),\n",
    "                                      torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1))\n",
    "        valid_dataset = TensorDataset(torch.tensor(X_valid.values, dtype=torch.float32),\n",
    "                                      torch.tensor(y_valid.values, dtype=torch.float32).unsqueeze(1))\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "        \n",
    "        best_acc = 0.0\n",
    "        best_model_state = None\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(self.device)\n",
    "                y_batch = y_batch.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(X_batch)\n",
    "                loss = self.criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            valid_loss = self.evaluate(valid_loader)\n",
    "\n",
    "            # 计算验证集上的 Accuracy\n",
    "            self.model.eval()\n",
    "            val_preds = []\n",
    "            val_targets = []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in valid_loader:\n",
    "                    X_batch = X_batch.to(self.device)\n",
    "                    y_batch = y_batch.to(self.device)\n",
    "                    outputs = self.model(X_batch)\n",
    "                    preds = (outputs.squeeze() > 0.5).int()\n",
    "                    val_preds.extend(preds.cpu().numpy())\n",
    "                    val_targets.extend(y_batch.squeeze().cpu().numpy())\n",
    "            val_acc = accuracy_score(val_targets, val_preds)\n",
    "\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                best_epoch = epoch\n",
    "                best_model_state = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "            # 可根据需要打印每个 epoch 的训练过程\n",
    "            # print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "            #       f\"Validation Loss: {valid_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        # 恢复验证集上 Accuracy 最佳的模型参数\n",
    "        if best_model_state is not None:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "            print(f\"Loaded best model from epoch {best_epoch+1} with Validation Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in data_loader:\n",
    "                X_batch = X_batch.to(self.device)\n",
    "                y_batch = y_batch.to(self.device)\n",
    "                outputs = self.model(X_batch)\n",
    "                loss = self.criterion(outputs, y_batch)\n",
    "                total_loss += loss.item()\n",
    "        return total_loss / len(data_loader)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, X_data):\n",
    "        self.model.eval()\n",
    "        X_tensor = torch.tensor(X_data.values, dtype=torch.float32).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "        predicted_classes = (outputs.squeeze() > 0.5).int()\n",
    "        return predicted_classes.cpu()  # 返回 CPU 上的数据\n",
    "\n",
    "    def predict_with_uncertainty(self, X_sample, n_iter=100):\n",
    "        # 启用 dropout 进行不确定性估计\n",
    "        self.model.train()  # 保持 dropout\n",
    "        X_sample = torch.tensor(X_sample.values, dtype=torch.float32).to(self.device)\n",
    "        predictions = torch.zeros(n_iter, X_sample.size(0)).to(self.device)\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            outputs = self.model(X_sample)\n",
    "            predictions[i] = outputs.squeeze()\n",
    "\n",
    "        mean_prediction = predictions.mean(dim=0)\n",
    "        uncertainty = predictions.std(dim=0)\n",
    "        predicted_classes = (mean_prediction > 0.5).int()\n",
    "\n",
    "        return predicted_classes.cpu(), uncertainty.cpu()\n",
    "\n",
    "    def compute_similarity_and_uncertainty(self, X_data, n_iter=30, device='cuda'):\n",
    "        self.model.to(device)  # 确保模型在 GPU 上\n",
    "        self.model.eval()  # 进入评估模式\n",
    "    \n",
    "        X_tensor = torch.tensor(X_data.values, dtype=torch.float32, device=device)\n",
    "    \n",
    "        # 计算隐藏层表示\n",
    "        with torch.no_grad():\n",
    "            hidden_vectors = self.model.get_hidden_representation(X_tensor)\n",
    "    \n",
    "        # 归一化隐藏向量，确保计算余弦相似度时范围在 [0,1]\n",
    "        hidden_vectors = F.normalize(hidden_vectors, p=2, dim=1)  # L2 归一化\n",
    "        similarity_matrix = torch.mm(hidden_vectors, hidden_vectors.t())  # 计算余弦相似度\n",
    "    \n",
    "        # 启用 dropout 进行不确定性估计\n",
    "        self.model.train()\n",
    "    \n",
    "        # 初始化预测结果张量，形状为 (n_iter, n_samples)\n",
    "        predictions = torch.zeros(n_iter, X_tensor.size(0), device=device)\n",
    "    \n",
    "        for i in range(n_iter):\n",
    "            outputs = self.model(X_tensor)\n",
    "            predictions[i] = outputs.squeeze()\n",
    "    \n",
    "        # 计算均值和标准差\n",
    "        mean_prediction = predictions.mean(dim=0)\n",
    "        uncertainty = predictions.std(dim=0)\n",
    "    \n",
    "        # 计算预测类别\n",
    "        predicted_classes = (mean_prediction > 0.5).int()\n",
    "    \n",
    "        # 计算不确定性矩阵（优化，不使用循环）\n",
    "        pred_match = (predicted_classes.unsqueeze(1) == predicted_classes.unsqueeze(0)).float()\n",
    "        uncertainty_matrix = pred_match * (uncertainty.unsqueeze(1) + uncertainty.unsqueeze(0))\n",
    "        \n",
    "        # 计算 similarity_matrix 的最小值和最大值\n",
    "        sim_min = similarity_matrix.min()\n",
    "        sim_max = similarity_matrix.max()\n",
    "        \n",
    "        # 归一化到 [0,1]\n",
    "        similarity_matrix = (similarity_matrix - sim_min) / (sim_max - sim_min + 1e-8)  # 避免除零\n",
    "    \n",
    "        return similarity_matrix, uncertainty_matrix\n",
    "    \n",
    "    \n",
    "    def compute_adjacency_matrix(self, similarity_matrix, uncertainty_matrix, threshold=0.5, epsilon=0.1, device='cuda'):\n",
    "        \"\"\"\n",
    "        基于相似度矩阵和不确定性矩阵计算邻接矩阵，并进行 GPU 加速优化。\n",
    "    \n",
    "        参数：\n",
    "            similarity_matrix (torch.Tensor): 相似度矩阵，形状为 (n_samples, n_samples)\n",
    "            uncertainty_matrix (torch.Tensor): 不确定性矩阵，形状为 (n_samples, n_samples)\n",
    "            threshold (float): 设定相似度的阈值，大于此值的样本对可以相连\n",
    "            epsilon (float): 设定不确定性的上限，小于此值的样本对可以相连\n",
    "            device (str): 计算设备 ('cuda' or 'cpu')\n",
    "    \n",
    "        返回：\n",
    "            adj (torch.Tensor): 计算得到的邻接矩阵，形状为 (n_samples, n_samples)\n",
    "        \"\"\"\n",
    "        # 迁移到 GPU（如果可用）\n",
    "        similarity_matrix = similarity_matrix.to(device)\n",
    "        uncertainty_matrix = uncertainty_matrix.to(device)\n",
    "    \n",
    "        # 创建全零矩阵\n",
    "        adj = torch.zeros_like(similarity_matrix, device=device)\n",
    "    \n",
    "        # 使用布尔索引（避免循环）\n",
    "        mask = (similarity_matrix > threshold) & (uncertainty_matrix > 0) & (uncertainty_matrix < epsilon)\n",
    "    \n",
    "        # 直接赋值，减少计算量\n",
    "        adj[mask] = 1\n",
    "    \n",
    "        return adj\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3. 随机挑选参数组合进行 Grid Search\n",
    "# ----------------------------\n",
    "# 定义超参数空间\n",
    "param_grid = {\n",
    "    'hidden_size': list(range(64, 513, 32)),\n",
    "    'num_layers': [2, 3, 4, 5],\n",
    "    'dropout_rate': [0.2, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6],\n",
    "    'learning_rate': [0.01, 0.001, 0.005, 0.0005]\n",
    "}\n",
    "\n",
    "# 生成所有可能的参数组合\n",
    "all_combinations = list(itertools.product(param_grid['hidden_size'],\n",
    "                                          param_grid['num_layers'],\n",
    "                                          param_grid['dropout_rate'],\n",
    "                                          param_grid['learning_rate']))\n",
    "print(f\"共有 {len(all_combinations)} 种参数组合。\")\n",
    "\n",
    "# 随机挑选部分组合进行尝试，比如随机挑选 100 个组合\n",
    "n_iter = 10\n",
    "selected_combinations = random.sample(all_combinations, n_iter)\n",
    "print(\"随机挑选的参数组合为：{}个。\".format(len(selected_combinations)))\n",
    "for combo in selected_combinations:\n",
    "    print(f\"  hidden_size={combo[0]}, num_layers={combo[1]}, dropout_rate={combo[2]}, learning_rate={combo[3]}\")\n",
    "\n",
    "grid_search_results = []\n",
    "\n",
    "# 增加打印正在训练第多少个参数组合\n",
    "for idx, (hidden_size, num_layers, dropout_rate, learning_rate) in enumerate(selected_combinations):\n",
    "    print(\"==========================================\")\n",
    "    print(f\"Training combination {idx+1}/{len(selected_combinations)}: hidden_size={hidden_size}, num_layers={num_layers}, dropout_rate={dropout_rate}, learning_rate={learning_rate}\")\n",
    "    \n",
    "    # 根据当前参数组合初始化分类器\n",
    "    classifier = BinaryClassifier(\n",
    "        input_dim=X.shape[1],\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        dropout_rate=dropout_rate,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    \n",
    "    # 训练模型（例如 30 个 epochs，batch_size 为 64）\n",
    "    classifier.train(X_train, y_train, X_valid, y_valid, epochs=30, batch_size=64)\n",
    "    \n",
    "    # 在验证集上计算 Accuracy\n",
    "    val_preds = classifier.predict(X_valid)\n",
    "    val_acc = accuracy_score(y_valid, val_preds)\n",
    "    print(f\"Validation Accuracy for this model: {val_acc:.4f}\")\n",
    "    \n",
    "    grid_search_results.append({\n",
    "        'model': copy.deepcopy(classifier),  # 保存该模型\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'learning_rate': learning_rate,\n",
    "        'val_acc': val_acc\n",
    "    })\n",
    "\n",
    "# 选择验证集 Accuracy 最高的模型\n",
    "best_result = max(grid_search_results, key=lambda x: x['val_acc'])\n",
    "best_classifier = best_result['model']\n",
    "\n",
    "print(\"==========================================\")\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(f\"  hidden_size: {best_result['hidden_size']}\")\n",
    "print(f\"  num_layers: {best_result['num_layers']}\")\n",
    "print(f\"  dropout_rate: {best_result['dropout_rate']}\")\n",
    "print(f\"  learning_rate: {best_result['learning_rate']}\")\n",
    "print(f\"Best Validation Accuracy: {best_result['val_acc']:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4. 在测试集上评估最佳模型\n",
    "# ----------------------------\n",
    "test_predictions = best_classifier.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "test_recall = recall_score(y_test, test_predictions)\n",
    "test_f1 = f1_score(y_test, test_predictions)\n",
    "test_precision = precision_score(y_test, test_predictions)\n",
    "\n",
    "print(\"--------- Test Metrics ---------\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Recall:   {test_recall:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Test Precision:{test_precision:.4f}\")\n",
    "\n",
    "print(\"--------- Classification Report ---------\")\n",
    "print(classification_report(y_test, test_predictions))\n",
    "\n",
    "# ----------------------------\n",
    "# 5. 示例：对单个样本进行预测（并估计不确定性）\n",
    "# ----------------------------\n",
    "sample = X_test.iloc[[0]]\n",
    "print(\"Sample to Predict:\")\n",
    "print(sample)\n",
    "\n",
    "predicted_class, uncertainty = best_classifier.predict_with_uncertainty(sample)\n",
    "print(\"Predicted Class:\", predicted_class.item())\n",
    "print(\"Uncertainty:\", uncertainty.item())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有 1680 种参数组合。\n",
      "随机挑选的参数组合为：10个。\n",
      "  hidden_size=64, num_layers=5, dropout_rate=0.4, learning_rate=0.001\n",
      "  hidden_size=64, num_layers=2, dropout_rate=0.2, learning_rate=0.01\n",
      "  hidden_size=64, num_layers=4, dropout_rate=0.4, learning_rate=0.001\n",
      "  hidden_size=384, num_layers=5, dropout_rate=0.3, learning_rate=0.0005\n",
      "  hidden_size=96, num_layers=5, dropout_rate=0.5, learning_rate=0.001\n",
      "  hidden_size=352, num_layers=2, dropout_rate=0.35, learning_rate=0.005\n",
      "  hidden_size=192, num_layers=5, dropout_rate=0.6, learning_rate=0.001\n",
      "  hidden_size=160, num_layers=3, dropout_rate=0.6, learning_rate=0.001\n",
      "  hidden_size=448, num_layers=2, dropout_rate=0.3, learning_rate=0.001\n",
      "  hidden_size=96, num_layers=2, dropout_rate=0.6, learning_rate=0.01\n",
      "==========================================\n",
      "Training combination 1/10: hidden_size=64, num_layers=5, dropout_rate=0.4, learning_rate=0.001\n",
      "Loaded best model from epoch 9 with Validation Accuracy: 0.8200\n",
      "Validation Accuracy for this model: 0.8200\n",
      "==========================================\n",
      "Training combination 2/10: hidden_size=64, num_layers=2, dropout_rate=0.2, learning_rate=0.01\n",
      "Loaded best model from epoch 2 with Validation Accuracy: 0.8000\n",
      "Validation Accuracy for this model: 0.8000\n",
      "==========================================\n",
      "Training combination 3/10: hidden_size=64, num_layers=4, dropout_rate=0.4, learning_rate=0.001\n",
      "Loaded best model from epoch 7 with Validation Accuracy: 0.8300\n",
      "Validation Accuracy for this model: 0.8300\n",
      "==========================================\n",
      "Training combination 4/10: hidden_size=384, num_layers=5, dropout_rate=0.3, learning_rate=0.0005\n",
      "Loaded best model from epoch 3 with Validation Accuracy: 0.8200\n",
      "Validation Accuracy for this model: 0.8200\n",
      "==========================================\n",
      "Training combination 5/10: hidden_size=96, num_layers=5, dropout_rate=0.5, learning_rate=0.001\n",
      "Loaded best model from epoch 3 with Validation Accuracy: 0.8200\n",
      "Validation Accuracy for this model: 0.8200\n",
      "==========================================\n",
      "Training combination 6/10: hidden_size=352, num_layers=2, dropout_rate=0.35, learning_rate=0.005\n",
      "Loaded best model from epoch 12 with Validation Accuracy: 0.8300\n",
      "Validation Accuracy for this model: 0.8300\n",
      "==========================================\n",
      "Training combination 7/10: hidden_size=192, num_layers=5, dropout_rate=0.6, learning_rate=0.001\n",
      "Loaded best model from epoch 5 with Validation Accuracy: 0.8100\n",
      "Validation Accuracy for this model: 0.8100\n",
      "==========================================\n",
      "Training combination 8/10: hidden_size=160, num_layers=3, dropout_rate=0.6, learning_rate=0.001\n",
      "Loaded best model from epoch 2 with Validation Accuracy: 0.8600\n",
      "Validation Accuracy for this model: 0.8600\n",
      "==========================================\n",
      "Training combination 9/10: hidden_size=448, num_layers=2, dropout_rate=0.3, learning_rate=0.001\n",
      "Loaded best model from epoch 1 with Validation Accuracy: 0.8300\n",
      "Validation Accuracy for this model: 0.8300\n",
      "==========================================\n",
      "Training combination 10/10: hidden_size=96, num_layers=2, dropout_rate=0.6, learning_rate=0.01\n",
      "Loaded best model from epoch 1 with Validation Accuracy: 0.8200\n",
      "Validation Accuracy for this model: 0.8200\n",
      "==========================================\n",
      "Best Hyperparameters:\n",
      "  hidden_size: 160\n",
      "  num_layers: 3\n",
      "  dropout_rate: 0.6\n",
      "  learning_rate: 0.001\n",
      "Best Validation Accuracy: 0.8600\n",
      "--------- Test Metrics ---------\n",
      "Test Accuracy: 0.7200\n",
      "Test Recall:   0.3500\n",
      "Test F1 Score: 0.4286\n",
      "Test Precision:0.5526\n",
      "--------- Classification Report ---------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.88      0.81       140\n",
      "           1       0.55      0.35      0.43        60\n",
      "\n",
      "    accuracy                           0.72       200\n",
      "   macro avg       0.66      0.61      0.62       200\n",
      "weighted avg       0.70      0.72      0.70       200\n",
      "\n",
      "Sample to Predict:\n",
      "   laufkont  laufzeit     moral     verw     hoehe  sparkont   beszeit  \\\n",
      "0 -0.490296 -1.294443 -0.579758  0.66335 -1.009094 -0.721009 -1.067797   \n",
      "\n",
      "       rate    famges    buerge  wohnzeit      verm     alter  weitkred  \\\n",
      "0  0.008981  1.724882 -0.320641 -1.585393 -0.357607 -1.099408  0.444788   \n",
      "\n",
      "       wohn  bishkred     beruf      pers    telef   gastarb  \n",
      "0  0.135954  -0.66218 -1.408326  0.377964 -0.84226  0.175863  \n",
      "Predicted Class: 0\n",
      "Uncertainty: 0.04538155719637871\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T20:44:51.807546Z",
     "start_time": "2025-02-07T20:44:51.706554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample = X_test.iloc[[0]]\n",
    "print(\"Sample to Predict:\", sample)\n",
    "\n",
    "predicted_classes, uncertainty = classifier.predict_with_uncertainty(sample)\n",
    "\n",
    "print(\"Predicted Class:\", predicted_classes.item())\n",
    "print(\"Uncertainty:\", uncertainty.item())\n",
    "\n",
    "# Compute similarity and uncertainty matrices\n",
    "similarity_matrix, uncertainty_matrix = classifier.compute_similarity_and_uncertainty(pd.concat([X_train, X_valid, X_test]))\n",
    "print(\"Similarity Matrix:\", similarity_matrix)\n",
    "print(\"Uncertainty Matrix:\", uncertainty_matrix)\n",
    "\n",
    "# 计算邻接矩阵\n",
    "adj_matrix = classifier.compute_adjacency_matrix(similarity_matrix, uncertainty_matrix, threshold=0.1, epsilon=0.1)\n",
    "\n",
    "# 打印邻接矩阵形状\n",
    "print(\"Adjacency matrix shape:\", adj_matrix.shape)\n",
    "\n",
    "# 计算并打印非零元素个数\n",
    "num_nonzero = torch.count_nonzero(adj_matrix).item()\n",
    "print(\"Number of nonzero elements:\", num_nonzero)\n",
    "similarity_matrix"
   ],
   "id": "15843e5e7bc47c10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample to Predict:    laufkont  laufzeit     moral     verw     hoehe  sparkont   beszeit  \\\n",
      "0 -0.490296 -1.294443 -0.579758  0.66335 -1.009094 -0.721009 -1.067797   \n",
      "\n",
      "       rate    famges    buerge  wohnzeit      verm     alter  weitkred  \\\n",
      "0  0.008981  1.724882 -0.320641 -1.585393 -0.357607 -1.099408  0.444788   \n",
      "\n",
      "       wohn  bishkred     beruf      pers    telef   gastarb  \n",
      "0  0.135954  -0.66218 -1.408326  0.377964 -0.84226  0.175863  \n",
      "Predicted Class: 0\n",
      "Uncertainty: 0.0\n",
      "Similarity Matrix: tensor([[1.0000, 1.0000, 0.9988,  ..., 1.0000, 0.9999, 0.9999],\n",
      "        [1.0000, 1.0000, 0.9990,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [0.9988, 0.9990, 1.0000,  ..., 0.9990, 0.9992, 0.9993],\n",
      "        ...,\n",
      "        [1.0000, 1.0000, 0.9990,  ..., 0.9999, 1.0000, 0.9999],\n",
      "        [0.9999, 1.0000, 0.9992,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [0.9999, 1.0000, 0.9993,  ..., 0.9999, 1.0000, 1.0000]],\n",
      "       device='cuda:0')\n",
      "Uncertainty Matrix: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n",
      "Adjacency matrix shape: torch.Size([1000, 1000])\n",
      "Number of nonzero elements: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 0.9988,  ..., 1.0000, 0.9999, 0.9999],\n",
       "        [1.0000, 1.0000, 0.9990,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.9988, 0.9990, 1.0000,  ..., 0.9990, 0.9992, 0.9993],\n",
       "        ...,\n",
       "        [1.0000, 1.0000, 0.9990,  ..., 0.9999, 1.0000, 0.9999],\n",
       "        [0.9999, 1.0000, 0.9992,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.9999, 1.0000, 0.9993,  ..., 0.9999, 1.0000, 1.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T20:45:09.106769Z",
     "start_time": "2025-02-07T20:45:09.099869Z"
    }
   },
   "cell_type": "code",
   "source": "adj_matrix",
   "id": "c4d093792eaaa039",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "82df3f0b1ba79442"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
