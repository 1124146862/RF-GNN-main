{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T13:29:39.142072Z",
     "start_time": "2025-02-12T13:29:39.077773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### util function\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "import torch\n",
    "from sklearn.semi_supervised.tests.test_self_training import X_test\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Standardize the input data\n",
    "def standard_input(X):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    # 保留原有索引，便于后续处理\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "    return X_scaled_df\n",
    "\n",
    "\n",
    "def load_data_DEF(random_state=42):\n",
    "    \"\"\"\n",
    "    从 CSV 文件中加载数据，并划分训练、验证、测试集，同时构造节点 mask\n",
    "    \"\"\"\n",
    "    # CSV 文件路径（请根据实际情况修改）\n",
    "    path = '/home/gehongfei/project/TabGNN/dataset/DEF.csv'\n",
    "    df = pd.read_csv(path, sep=',')\n",
    "    \n",
    "    target_col = 'label'\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"Error: '{target_col}' column not found in the dataset.\")\n",
    "        return None, None, None, None, None, None, None, None, None, None, None\n",
    "    \n",
    "    y = df[target_col]\n",
    "    if \"ID\" in df.columns:\n",
    "        X = df.drop(columns=[\"ID\", target_col])\n",
    "    else:\n",
    "        X = df.drop(columns=[target_col])\n",
    "    \n",
    "    # 划分训练、验证和测试集（采用 stratify 保证标签分布均衡）\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=random_state, stratify=y\n",
    "    )\n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=2/3, random_state=random_state, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # 创建节点 mask（假设每一行数据代表图中的一个节点）\n",
    "    num_nodes = len(df)\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask   = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask  = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[X_train.index] = True\n",
    "    val_mask[X_valid.index]   = True\n",
    "    test_mask[X_test.index]   = True\n",
    "    \n",
    "    # 标准化数据\n",
    "    X = standard_input(X)\n",
    "    X_train = standard_input(X_train)\n",
    "    X_valid = standard_input(X_valid)\n",
    "    X_test  = standard_input(X_test)\n",
    "    \n",
    "    return X, y, X_train, X_valid, X_test, y_train, y_valid, y_test, train_mask, val_mask, test_mask"
   ],
   "id": "bd21cc33cc2be496",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T20:38:27.120957Z",
     "start_time": "2025-02-12T20:38:18.323630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import itertools, random\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# -------------------- Focal Loss 实现 --------------------\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss 实现：\n",
    "      focal_loss = alpha * (1 - p_t)^gamma * CE_loss\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, reduction=\"mean\"):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # 计算每个样本的 CrossEntropyLoss，不做 reduction\n",
    "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-ce_loss)  # p_t\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# -------------------- 灵活的 MLP 模型 --------------------\n",
    "class FlexibleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, activation, dropout_rate, num_classes):\n",
    "        \"\"\"\n",
    "        构造一个多层 MLP：\n",
    "          - 第一层：输入 -> hidden_dim\n",
    "          - 后续 (num_layers - 1) 层：hidden_dim -> hidden_dim\n",
    "          - 输出层：hidden_dim -> num_classes\n",
    "        每层均采用 BatchNorm、指定的激活函数和 Dropout。\n",
    "        \"\"\"\n",
    "        super(FlexibleMLP, self).__init__()\n",
    "        layers = []\n",
    "        # 第一层\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "        layers.append(activation())\n",
    "        layers.append(nn.Dropout(dropout_rate))\n",
    "        # 如果要求多层隐藏层，则叠加若干层\n",
    "        for i in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(activation())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "        # 输出层\n",
    "        layers.append(nn.Linear(hidden_dim, num_classes))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -------------------- 网格搜索函数 --------------------\n",
    "def grid_search_mlp():\n",
    "    # -------------------- 数据加载与转换 --------------------\n",
    "    X, y, X_train, X_valid, X_test, y_train, y_valid, y_test, _, _, _ = load_data_DEF(random_state=42)\n",
    "    \n",
    "    # 转换为 torch.tensor\n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "    X_valid_tensor = torch.tensor(X_valid.values, dtype=torch.float32)\n",
    "    y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.long)\n",
    "    X_test_tensor  = torch.tensor(X_test.values,  dtype=torch.float32)\n",
    "    y_test_tensor  = torch.tensor(y_test.values,  dtype=torch.long)\n",
    "    \n",
    "    # 设备设置\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_train_tensor, y_train_tensor = X_train_tensor.to(device), y_train_tensor.to(device)\n",
    "    X_valid_tensor, y_valid_tensor = X_valid_tensor.to(device), y_valid_tensor.to(device)\n",
    "    X_test_tensor, y_test_tensor   = X_test_tensor.to(device), y_test_tensor.to(device)\n",
    "    \n",
    "    input_dim = X_train_tensor.shape[1]\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    \n",
    "    # -------------------- 定义超参数搜索空间 --------------------\n",
    "    # 注意：对于 focal_alpha 和 focal_gamma，只有当 loss 选择 \"focal\" 时才会生效\n",
    "    param_grid = {\n",
    "        \"num_layers\": [1, 2],\n",
    "        \"hidden_dim\": [128, 256],\n",
    "        \"learning_rate\": [0.001, 0.0005],\n",
    "        \"activation\": [nn.ReLU, nn.LeakyReLU],\n",
    "        \"loss\": [\"crossentropy\", \"focal\"],\n",
    "        \"focal_alpha\": [0.25, 0.5],\n",
    "        \"focal_gamma\": [1, 2],\n",
    "        \"dropout_rate\": [0.3, 0.5]\n",
    "    }\n",
    "    \n",
    "    # 生成所有组合\n",
    "    all_combinations = list(itertools.product(*param_grid.values()))\n",
    "    # 为节约时间，这里随机选取部分组合进行试验（也可遍历所有组合）\n",
    "    n_trials = min(5, len(all_combinations))\n",
    "    sampled_combinations = random.sample(all_combinations, n_trials)\n",
    "    \n",
    "    best_val_f1 = 0.0\n",
    "    best_params = None\n",
    "    best_model_state = None\n",
    "    num_epochs = 100  # 可根据需要调整训练轮数\n",
    "\n",
    "    # -------------------- 开始搜索 --------------------\n",
    "    for trial_idx, comb in enumerate(sampled_combinations):\n",
    "        # 解包超参数组合\n",
    "        num_layers, hidden_dim, learning_rate, activation_func, loss_type, focal_alpha, focal_gamma, dropout_rate = comb\n",
    "        print(f\"================ Trial {trial_idx+1}/{n_trials} ================\")\n",
    "        print(f\"Params: num_layers={num_layers}, hidden_dim={hidden_dim}, \"\n",
    "              f\"learning_rate={learning_rate}, activation={activation_func.__name__}, \"\n",
    "              f\"loss={loss_type}, focal_alpha={focal_alpha}, focal_gamma={focal_gamma}, \"\n",
    "              f\"dropout_rate={dropout_rate}\")\n",
    "        \n",
    "        # 构建模型（每个 trial 都重新初始化模型）\n",
    "        model = FlexibleMLP(input_dim, hidden_dim, num_layers, activation_func, dropout_rate, num_classes)\n",
    "        model.to(device)\n",
    "        \n",
    "        # 选择损失函数\n",
    "        if loss_type == \"focal\":\n",
    "            criterion = FocalLoss(alpha=focal_alpha, gamma=focal_gamma)\n",
    "        else:\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        best_trial_f1 = 0.0\n",
    "        best_trial_state = None\n",
    "        \n",
    "        # 训练过程\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train_tensor)\n",
    "            loss = criterion(outputs, y_train_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 验证评估\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(X_valid_tensor)\n",
    "                val_loss = criterion(val_outputs, y_valid_tensor)\n",
    "                val_pred = torch.argmax(val_outputs, dim=1)\n",
    "                val_f1 = f1_score(y_valid_tensor.cpu().numpy(), val_pred.cpu().numpy(), average=\"macro\")\n",
    "            \n",
    "            # 记录该 trial 中最优的模型状态\n",
    "            if val_f1 > best_trial_f1:\n",
    "                best_trial_f1 = val_f1\n",
    "                best_trial_state = model.state_dict()\n",
    "            \n",
    "            # 每 10 个 epoch 打印一次\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}]  Val Loss: {val_loss.item():.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        print(f\"--> Best Val F1 for Trial {trial_idx+1}: {best_trial_f1:.4f}\\n\")\n",
    "        \n",
    "        # 若当前 trial 的最优效果优于之前的最优效果，则更新全局最优\n",
    "        if best_trial_f1 > best_val_f1:\n",
    "            best_val_f1 = best_trial_f1\n",
    "            best_params = {\n",
    "                \"num_layers\": num_layers,\n",
    "                \"hidden_dim\": hidden_dim,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"activation\": activation_func.__name__,\n",
    "                \"loss\": loss_type,\n",
    "                \"focal_alpha\": focal_alpha,\n",
    "                \"focal_gamma\": focal_gamma,\n",
    "                \"dropout_rate\": dropout_rate\n",
    "            }\n",
    "            best_model_state = best_trial_state\n",
    "    \n",
    "    print(\"#######################\")\n",
    "    print(\"Best Hyperparameters Found:\")\n",
    "    print(best_params)\n",
    "    print(f\"Best Validation F1: {best_val_f1:.4f}\")\n",
    "    \n",
    "    # -------------------- 用最优模型在测试集上评估 --------------------\n",
    "    # 重新构建模型（注意激活函数名称需要转回类，可以用 getattr(nn, ...)）\n",
    "    best_activation_class = getattr(nn, best_params[\"activation\"])\n",
    "    model = FlexibleMLP(input_dim, best_params[\"hidden_dim\"], best_params[\"num_layers\"],\n",
    "                        best_activation_class, best_params[\"dropout_rate\"], num_classes)\n",
    "    model.to(device)\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor)\n",
    "        test_pred = torch.argmax(test_outputs, dim=1)\n",
    "    test_pred_np = test_pred.cpu().numpy()\n",
    "    y_test_np = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    print(\"\\nTest Classification Report (Best Model):\")\n",
    "    print(classification_report(y_test_np, test_pred_np))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    grid_search_mlp()\n"
   ],
   "id": "c7e2562bc2ee8f3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Trial 1/5 ================\n",
      "Params: num_layers=2, hidden_dim=256, learning_rate=0.0005, activation=LeakyReLU, loss=crossentropy, focal_alpha=0.5, focal_gamma=2, dropout_rate=0.3\n",
      "Epoch [10/100]  Val Loss: 0.6267, Val F1: 0.6507\n",
      "Epoch [20/100]  Val Loss: 0.5176, Val F1: 0.6855\n",
      "Epoch [30/100]  Val Loss: 0.4676, Val F1: 0.6809\n",
      "Epoch [40/100]  Val Loss: 0.4545, Val F1: 0.6763\n",
      "Epoch [50/100]  Val Loss: 0.4481, Val F1: 0.6749\n",
      "Epoch [60/100]  Val Loss: 0.4454, Val F1: 0.6762\n",
      "Epoch [70/100]  Val Loss: 0.4450, Val F1: 0.6819\n",
      "Epoch [80/100]  Val Loss: 0.4437, Val F1: 0.6825\n",
      "Epoch [90/100]  Val Loss: 0.4437, Val F1: 0.6861\n",
      "Epoch [100/100]  Val Loss: 0.4431, Val F1: 0.6856\n",
      "--> Best Val F1 for Trial 1: 0.6864\n",
      "\n",
      "================ Trial 2/5 ================\n",
      "Params: num_layers=1, hidden_dim=128, learning_rate=0.001, activation=LeakyReLU, loss=crossentropy, focal_alpha=0.25, focal_gamma=2, dropout_rate=0.5\n",
      "Epoch [10/100]  Val Loss: 0.6039, Val F1: 0.6093\n",
      "Epoch [20/100]  Val Loss: 0.5201, Val F1: 0.5541\n",
      "Epoch [30/100]  Val Loss: 0.5080, Val F1: 0.5874\n",
      "Epoch [40/100]  Val Loss: 0.4996, Val F1: 0.6168\n",
      "Epoch [50/100]  Val Loss: 0.4888, Val F1: 0.6226\n",
      "Epoch [60/100]  Val Loss: 0.4810, Val F1: 0.6297\n",
      "Epoch [70/100]  Val Loss: 0.4764, Val F1: 0.6294\n",
      "Epoch [80/100]  Val Loss: 0.4731, Val F1: 0.6316\n",
      "Epoch [90/100]  Val Loss: 0.4706, Val F1: 0.6297\n",
      "Epoch [100/100]  Val Loss: 0.4683, Val F1: 0.6294\n",
      "--> Best Val F1 for Trial 2: 0.6329\n",
      "\n",
      "================ Trial 3/5 ================\n",
      "Params: num_layers=1, hidden_dim=128, learning_rate=0.001, activation=ReLU, loss=focal, focal_alpha=0.5, focal_gamma=1, dropout_rate=0.5\n",
      "Epoch [10/100]  Val Loss: 0.1351, Val F1: 0.5744\n",
      "Epoch [20/100]  Val Loss: 0.1313, Val F1: 0.6364\n",
      "Epoch [30/100]  Val Loss: 0.1265, Val F1: 0.6268\n",
      "Epoch [40/100]  Val Loss: 0.1236, Val F1: 0.6243\n",
      "Epoch [50/100]  Val Loss: 0.1222, Val F1: 0.6273\n",
      "Epoch [60/100]  Val Loss: 0.1214, Val F1: 0.6316\n",
      "Epoch [70/100]  Val Loss: 0.1206, Val F1: 0.6254\n",
      "Epoch [80/100]  Val Loss: 0.1200, Val F1: 0.6248\n",
      "Epoch [90/100]  Val Loss: 0.1195, Val F1: 0.6348\n",
      "Epoch [100/100]  Val Loss: 0.1191, Val F1: 0.6361\n",
      "--> Best Val F1 for Trial 3: 0.6379\n",
      "\n",
      "================ Trial 4/5 ================\n",
      "Params: num_layers=1, hidden_dim=128, learning_rate=0.001, activation=LeakyReLU, loss=focal, focal_alpha=0.25, focal_gamma=1, dropout_rate=0.3\n",
      "Epoch [10/100]  Val Loss: 0.0655, Val F1: 0.5196\n",
      "Epoch [20/100]  Val Loss: 0.0620, Val F1: 0.6156\n",
      "Epoch [30/100]  Val Loss: 0.0602, Val F1: 0.6301\n",
      "Epoch [40/100]  Val Loss: 0.0594, Val F1: 0.6339\n",
      "Epoch [50/100]  Val Loss: 0.0589, Val F1: 0.6402\n",
      "Epoch [60/100]  Val Loss: 0.0585, Val F1: 0.6383\n",
      "Epoch [70/100]  Val Loss: 0.0584, Val F1: 0.6415\n",
      "Epoch [80/100]  Val Loss: 0.0582, Val F1: 0.6411\n",
      "Epoch [90/100]  Val Loss: 0.0580, Val F1: 0.6436\n",
      "Epoch [100/100]  Val Loss: 0.0579, Val F1: 0.6464\n",
      "--> Best Val F1 for Trial 4: 0.6464\n",
      "\n",
      "================ Trial 5/5 ================\n",
      "Params: num_layers=2, hidden_dim=256, learning_rate=0.001, activation=LeakyReLU, loss=crossentropy, focal_alpha=0.25, focal_gamma=2, dropout_rate=0.5\n",
      "Epoch [10/100]  Val Loss: 0.6455, Val F1: 0.6356\n",
      "Epoch [20/100]  Val Loss: 0.5013, Val F1: 0.6905\n",
      "Epoch [30/100]  Val Loss: 0.4703, Val F1: 0.6804\n",
      "Epoch [40/100]  Val Loss: 0.4560, Val F1: 0.6702\n",
      "Epoch [50/100]  Val Loss: 0.4518, Val F1: 0.6777\n",
      "Epoch [60/100]  Val Loss: 0.4508, Val F1: 0.6820\n",
      "Epoch [70/100]  Val Loss: 0.4489, Val F1: 0.6833\n",
      "Epoch [80/100]  Val Loss: 0.4482, Val F1: 0.6834\n",
      "Epoch [90/100]  Val Loss: 0.4480, Val F1: 0.6848\n",
      "Epoch [100/100]  Val Loss: 0.4466, Val F1: 0.6874\n",
      "--> Best Val F1 for Trial 5: 0.6905\n",
      "\n",
      "#######################\n",
      "Best Hyperparameters Found:\n",
      "{'num_layers': 2, 'hidden_dim': 256, 'learning_rate': 0.001, 'activation': 'LeakyReLU', 'loss': 'crossentropy', 'focal_alpha': 0.25, 'focal_gamma': 2, 'dropout_rate': 0.5}\n",
      "Best Validation F1: 0.6905\n",
      "\n",
      "Test Classification Report (Best Model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.93      0.89      4673\n",
      "           1       0.62      0.42      0.50      1327\n",
      "\n",
      "    accuracy                           0.81      6000\n",
      "   macro avg       0.73      0.67      0.69      6000\n",
      "weighted avg       0.80      0.81      0.80      6000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "### 可能的采样优化 交叉",
   "id": "4c72dfd730487c8d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T13:27:31.688894Z",
     "start_time": "2025-02-12T13:27:31.682813Z"
    }
   },
   "cell_type": "code",
   "source": "X_test.shape",
   "id": "55af7549f45aa83a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 23)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "576de34efa53f20d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
