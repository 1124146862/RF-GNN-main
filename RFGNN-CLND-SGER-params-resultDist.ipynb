{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839f95dd4785f5af",
   "metadata": {},
   "source": [
    "### util function"
   ]
  },
  {
   "cell_type": "code",
   "id": "2569111fcf584f28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T21:53:06.231102Z",
     "start_time": "2025-02-21T21:53:04.941477Z"
    }
   },
   "source": [
    "# util function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.sparse import csr_matrix\n",
    "import torch\n",
    "\n",
    "\n",
    "def standard_input(X):\n",
    "    # 标准化输入\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "\n",
    "\n",
    "def load_data_SGER(mode='raw',random_state=41, aug_pct=100):\n",
    "    \"\"\"\n",
    "    加载数据并进行预处理，返回以下变量：\n",
    "    X, y, X_train, X_valid, X_test, y_train, y_valid, y_test, train_mask, valid_mask, test_mask\n",
    "\n",
    "    参数：\n",
    "        mode (str): 数据读取模式，可选 'raw' 或 'aug'\n",
    "            - 'raw': 仅使用 raw 数据（SGER1000.csv），逻辑与原 load_data_SGER_RAW 一致\n",
    "            - 'aug': raw 数据按原逻辑划分，且读取 LLM 数据（SGERLLM.csv）并将其按 90%/10%分别追加到\n",
    "                     训练集和验证集中；测试集仍使用 raw 数据划分出的部分\n",
    "        aug_pct (int 或 float): 使用的增强数据比例，取值范围 0~100，默认 100 表示使用全部 LLM 数据。\n",
    "                                  当 mode 为 'aug' 时有效。\n",
    "    \"\"\"\n",
    "    # 定义文件路径\n",
    "    path_raw = '/home/gehongfei/project/TabGNN/dataset/SGER1000.csv'\n",
    "    path_llm = '/home/gehongfei/project/TabGNN/dataset/SGERLLM.csv'\n",
    "    \n",
    "    # ---------------------------\n",
    "    # 1. 处理 raw 数据\n",
    "    # ---------------------------\n",
    "    try:\n",
    "        df_raw = pd.read_csv(path_raw, sep='\\s+')\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading raw file {path_raw}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    if 'kredit' not in df_raw.columns:\n",
    "        print(\"Error: 'kredit' 列未在 raw 数据中找到。\")\n",
    "        return None\n",
    "\n",
    "    # 分离特征和目标变量\n",
    "    y_raw = df_raw['kredit']\n",
    "    X_raw = df_raw.drop(columns=['kredit'])\n",
    "    \n",
    "    # 划分 raw 数据：70% 为训练集，30% 为临时集合\n",
    "    X_train_raw, X_temp, y_train_raw, y_temp = train_test_split(\n",
    "        X_raw, y_raw, test_size=0.3, random_state=random_state, stratify=y_raw\n",
    "    )\n",
    "    # 将临时集合再划分为验证集和测试集，其中 1/3（约 10%）为验证集，2/3（约 20%）为测试集\n",
    "    X_valid_raw, X_test_raw, y_valid_raw, y_test_raw = train_test_split(\n",
    "        X_temp, y_temp, test_size=2/3, random_state=random_state, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # ---------------------------\n",
    "    # 2. 根据 mode 处理数据\n",
    "    # ---------------------------\n",
    "    if mode == 'raw':\n",
    "        # 直接使用 raw 数据划分结果\n",
    "        X_train_final = X_train_raw.reset_index(drop=True)\n",
    "        y_train_final = y_train_raw.reset_index(drop=True)\n",
    "        X_valid_final = X_valid_raw.reset_index(drop=True)\n",
    "        y_valid_final = y_valid_raw.reset_index(drop=True)\n",
    "        X_test_final  = X_test_raw.reset_index(drop=True)\n",
    "        y_test_final  = y_test_raw.reset_index(drop=True)\n",
    "        \n",
    "    elif mode == 'aug':\n",
    "        # 读取 LLM 数据，并将数据追加到 raw 的训练集和验证集中\n",
    "        try:\n",
    "            df_llm = pd.read_csv(path_llm, sep='\\s+')\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading LLM file {path_llm}: {e}\")\n",
    "            return None\n",
    "        \n",
    "        if 'kredit' not in df_llm.columns:\n",
    "            print(\"Error: 'kredit' 列未在 LLM 数据中找到。\")\n",
    "            return None\n",
    "        \n",
    "        # 根据 aug_pct 参数调整使用的增强数据比例\n",
    "        if aug_pct < 100:\n",
    "            df_llm = df_llm.sample(frac=aug_pct/100, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        # 分离 LLM 数据中的特征和目标变量\n",
    "        y_llm = df_llm['kredit']\n",
    "        X_llm = df_llm.drop(columns=['kredit'])\n",
    "        \n",
    "        # 划分 LLM 数据：90% 为训练，10% 为验证\n",
    "        X_llm_train, X_llm_valid, y_llm_train, y_llm_valid = train_test_split(\n",
    "            X_llm, y_llm, test_size=0.1, random_state=42, stratify=y_llm\n",
    "        )\n",
    "        \n",
    "        # 重置索引，防止后续合并时出现冲突\n",
    "        X_train_raw = X_train_raw.reset_index(drop=True)\n",
    "        y_train_raw = y_train_raw.reset_index(drop=True)\n",
    "        X_valid_raw = X_valid_raw.reset_index(drop=True)\n",
    "        y_valid_raw = y_valid_raw.reset_index(drop=True)\n",
    "        X_llm_train = X_llm_train.reset_index(drop=True)\n",
    "        y_llm_train = y_llm_train.reset_index(drop=True)\n",
    "        X_llm_valid = X_llm_valid.reset_index(drop=True)\n",
    "        y_llm_valid = y_llm_valid.reset_index(drop=True)\n",
    "        X_test_raw  = X_test_raw.reset_index(drop=True)\n",
    "        y_test_raw  = y_test_raw.reset_index(drop=True)\n",
    "        \n",
    "        # 追加 LLM 数据：训练集增加 LLM 训练数据，验证集增加 LLM 验证数据\n",
    "        X_train_final = pd.concat([X_train_raw, X_llm_train], axis=0).reset_index(drop=True)\n",
    "        y_train_final = pd.concat([y_train_raw, y_llm_train], axis=0).reset_index(drop=True)\n",
    "        X_valid_final = pd.concat([X_valid_raw, X_llm_valid], axis=0).reset_index(drop=True)\n",
    "        y_valid_final = pd.concat([y_valid_raw, y_llm_valid], axis=0).reset_index(drop=True)\n",
    "        # 测试集仍使用 raw 划分出的数据\n",
    "        X_test_final  = X_test_raw\n",
    "        y_test_final  = y_test_raw\n",
    "    else:\n",
    "        print(\"Error: mode 必须为 'raw' 或 'aug'\")\n",
    "        return None\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3. 合并所有数据，并构造掩码\n",
    "    # ---------------------------\n",
    "    # 合并训练、验证、测试数据为完整数据集\n",
    "    X = pd.concat([X_train_final, X_valid_final, X_test_final], axis=0).reset_index(drop=True)\n",
    "    y = pd.concat([y_train_final, y_valid_final, y_test_final], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # 构造掩码，保证返回的变量名与要求一致\n",
    "    num_total = len(X)\n",
    "    train_mask = torch.zeros(num_total, dtype=torch.bool)\n",
    "    valid_mask = torch.zeros(num_total, dtype=torch.bool)\n",
    "    test_mask  = torch.zeros(num_total, dtype=torch.bool)\n",
    "    \n",
    "    # 训练集占合并数据的前部分\n",
    "    len_train = len(X_train_final)\n",
    "    train_mask[:len_train] = True\n",
    "    # 验证集紧随训练集之后\n",
    "    len_valid = len(X_valid_final)\n",
    "    valid_mask[len_train:len_train+len_valid] = True\n",
    "    # 测试集为剩余部分\n",
    "    test_mask[len_train+len_valid:] = True\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4. 标准化数据\n",
    "    # ---------------------------\n",
    "    X = standard_input(X)\n",
    "    X_train = standard_input(X_train_final)\n",
    "    X_valid = standard_input(X_valid_final)\n",
    "    X_test  = standard_input(X_test_final)\n",
    "    \n",
    "    return X, y, X_train, X_valid, X_test, \\\n",
    "           y_train_final, y_valid_final, y_test_final, \\\n",
    "           train_mask, valid_mask, test_mask\n",
    "\n",
    "# 使用示例：\n",
    "\n",
    "\n",
    "\n",
    "# 训练Random Forest并计算相似性\n",
    "# Function to compute adjacency matrices for train, validation, and test data\n",
    "def compute_adjacency_matrix(X_train, X_valid, X_test, y_train, y_valid, n_estimators=200, max_depth=None, threshold=0.15, random_state=42):\n",
    "    # 合并训练、验证和测试数据\n",
    "    X_combined = pd.concat([X_train, X_valid, X_test], axis=0)\n",
    "    num_samples = X_combined.shape[0]\n",
    "    # 训练Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n",
    "    rf.fit(pd.concat([X_train, X_valid], axis=0), pd.concat([y_train, y_valid], axis=0))\n",
    "    # 获取每棵树的叶子索引\n",
    "    leaf_indices = rf.apply(X_combined)\n",
    "    # 计算相似性矩阵\n",
    "    adjacency_matrix = np.zeros((num_samples, num_samples))\n",
    "    for tree_idx in range(leaf_indices.shape[1]):  # 遍历每棵树\n",
    "        leaf_to_samples = {}\n",
    "        for sample_idx, leaf_id in enumerate(leaf_indices[:, tree_idx]):\n",
    "            if leaf_id not in leaf_to_samples:\n",
    "                leaf_to_samples[leaf_id] = []\n",
    "            leaf_to_samples[leaf_id].append(sample_idx)\n",
    "        # 更新相似性矩阵\n",
    "        for sample_list in leaf_to_samples.values():\n",
    "            for i in sample_list:\n",
    "                for j in sample_list:\n",
    "                    if i != j:\n",
    "                        adjacency_matrix[i, j] += 1\n",
    "    # 归一化相似性\n",
    "    adjacency_matrix /= adjacency_matrix.max()\n",
    "    # 应用阈值，转换为二值矩阵\n",
    "    adjacency_matrix = (adjacency_matrix > threshold).astype(int)\n",
    "    # 转换为稀疏矩阵\n",
    "    adjacency_matrix_sparse = csr_matrix(adjacency_matrix)\n",
    "    return adjacency_matrix_sparse\n",
    "\n",
    "# 从稀疏邻接矩阵提取边索引\n",
    "def adjacency_to_edge_index(adj_matrix):\n",
    "    coo_matrix = adj_matrix.tocoo()  # 转换为COO格式\n",
    "    edge_index = torch.tensor(np.vstack((coo_matrix.row, coo_matrix.col)), dtype=torch.long)\n",
    "    return edge_index\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### add unsupervised contrastive loss NodeDropout",
   "id": "a82b8ccb0e8ffed"
  },
  {
   "cell_type": "code",
   "id": "31226259996f9dea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T21:59:37.382834Z",
     "start_time": "2025-02-09T21:59:34.709786Z"
    }
   },
   "source": [
    "# 稳定跑到0.80\n",
    "import itertools\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv, GATConv\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#########################################\n",
    "# 1. 定义模型\n",
    "#########################################\n",
    "# GraphSAGE 模型（包含残差结构和 dropout，每层隐藏单元数递减至上一层的3/4）\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout_rate=0.5):\n",
    "        \"\"\"\n",
    "        参数说明：\n",
    "          in_channels: 输入特征维度\n",
    "          hidden_channels: 第一层的隐藏单元数\n",
    "          out_channels: 输出类别数\n",
    "          num_layers: 图卷积层的总层数（至少为 1）\n",
    "          dropout_rate: dropout 概率\n",
    "        \"\"\"\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.residuals = torch.nn.ModuleList()\n",
    "        # 第一层：从 in_channels 到 hidden_channels\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        if in_channels != hidden_channels:\n",
    "            self.residuals.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        else:\n",
    "            self.residuals.append(torch.nn.Identity())\n",
    "        current_hidden = hidden_channels\n",
    "        # 后续每一层的隐藏单元数为上一层的 3/4（向下取整，最小为 1）\n",
    "        for _ in range(num_layers - 1):\n",
    "            next_hidden = max(1, int(current_hidden * 3 / 4))\n",
    "            self.convs.append(SAGEConv(current_hidden, next_hidden))\n",
    "            if current_hidden != next_hidden:\n",
    "                self.residuals.append(torch.nn.Linear(current_hidden, next_hidden))\n",
    "            else:\n",
    "                self.residuals.append(torch.nn.Identity())\n",
    "            current_hidden = next_hidden\n",
    "        # 全连接层：将最后一层的隐藏向量映射到输出类别\n",
    "        self.fc = torch.nn.Linear(current_hidden, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        \"\"\"提取节点表示：依次通过图卷积层、残差连接、ReLU 和 dropout\"\"\"\n",
    "        for conv, res in zip(self.convs, self.residuals):\n",
    "            out = conv(x, edge_index)\n",
    "            res_x = res(x)\n",
    "            x = self.dropout(torch.relu(out + res_x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.encode(data.x, data.edge_index)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# GAT 模型（包含残差结构和 dropout，每层隐藏单元数递减至上一层的3/4）\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout_rate=0.5, heads=1, concat=True):\n",
    "        \"\"\"\n",
    "        参数说明：\n",
    "          in_channels: 输入特征维度\n",
    "          hidden_channels: 第一层的隐藏单元数\n",
    "          out_channels: 输出类别数\n",
    "          num_layers: 图卷积层的总层数（至少为 1）\n",
    "          dropout_rate: dropout 概率（同时用于 GATConv 内部 dropout）\n",
    "          heads: 注意力头的数量\n",
    "          concat: 是否拼接多头输出（True）或取平均（False）\n",
    "        \"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.residuals = torch.nn.ModuleList()\n",
    "        # 第一层\n",
    "        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout_rate, concat=concat))\n",
    "        out_dim = hidden_channels * heads if concat else hidden_channels\n",
    "        if in_channels != out_dim:\n",
    "            self.residuals.append(torch.nn.Linear(in_channels, out_dim))\n",
    "        else:\n",
    "            self.residuals.append(torch.nn.Identity())\n",
    "        current_dim = out_dim\n",
    "        # 后续层，每层隐藏单元数为上一层的 3/4（向下取整，最小为 1）\n",
    "        for _ in range(num_layers - 1):\n",
    "            next_hidden = max(1, int(current_dim * 3 / 4))\n",
    "            self.convs.append(GATConv(current_dim, next_hidden, heads=heads, dropout=dropout_rate, concat=concat))\n",
    "            new_out_dim = next_hidden * heads if concat else next_hidden\n",
    "            if current_dim != new_out_dim:\n",
    "                self.residuals.append(torch.nn.Linear(current_dim, new_out_dim))\n",
    "            else:\n",
    "                self.residuals.append(torch.nn.Identity())\n",
    "            current_dim = new_out_dim\n",
    "        self.fc = torch.nn.Linear(current_dim, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        \"\"\"提取节点表示：依次通过 GAT 层、残差连接、ReLU 和 dropout\"\"\"\n",
    "        for conv, res in zip(self.convs, self.residuals):\n",
    "            out = conv(x, edge_index)\n",
    "            res_x = res(x)\n",
    "            x = self.dropout(torch.relu(out + res_x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.encode(data.x, data.edge_index)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "#########################################\n",
    "# 2. 定义损失函数\n",
    "#########################################\n",
    "# Focal Loss（用于处理类别不平衡）\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None, reduction=\"mean\"):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  \n",
    "        self.reduction = reduction\n",
    "        self.ce = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        if self.alpha is not None:\n",
    "            if isinstance(self.alpha, (list, np.ndarray)):\n",
    "                alpha = inputs.new_tensor(self.alpha)\n",
    "            else:\n",
    "                alpha = self.alpha\n",
    "            at = alpha.gather(0, targets.data)\n",
    "            ce_loss = at * ce_loss\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean() if self.reduction == \"mean\" else focal_loss.sum()\n",
    "\n",
    "# 修改后的普通对比学习损失（不使用 mask）\n",
    "class SupConLoss(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            temperature: 温度参数\n",
    "        \"\"\"\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: [batch_size, n_views, feature_dim]\n",
    "                      要求每个样本至少有两个视图，视图之间互为正样本，其余样本均为负样本。\n",
    "        Returns:\n",
    "            对比损失（InfoNCE Loss）\n",
    "        \"\"\"\n",
    "        device = features.device\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` 需要形状为 [batch_size, n_views, feature_dim]')\n",
    "        batch_size, n_views, feature_dim = features.shape\n",
    "\n",
    "        # 将多个视图拼接为 [batch_size*n_views, feature_dim]\n",
    "        features = features.view(batch_size * n_views, feature_dim)\n",
    "        # 对每个特征进行 L2 归一化\n",
    "        features = torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "\n",
    "        # 计算相似度矩阵，形状 [batch_size*n_views, batch_size*n_views]\n",
    "        similarity_matrix = torch.matmul(features, features.T) / self.temperature\n",
    "\n",
    "        # 构造正样本掩码：同一原始样本（即同一 batch 中的不同视图）的两两之间为正样本\n",
    "        labels = torch.arange(batch_size, device=device).repeat_interleave(n_views)\n",
    "        mask = torch.eq(labels.unsqueeze(1), labels.unsqueeze(0)).float()\n",
    "        # 去除自身对比（对角线置 0）\n",
    "        self_mask = torch.eye(mask.shape[0], device=device)\n",
    "        mask = mask - self_mask\n",
    "\n",
    "        # 计算 exp(similarity)\n",
    "        exp_sim = torch.exp(similarity_matrix) * (1 - self_mask)\n",
    "        # 对每个 anchor，分母为除自身外所有样本的 exp(sim)\n",
    "        denom = exp_sim.sum(dim=1, keepdim=True) + 1e-8\n",
    "\n",
    "        # 计算仅正样本对的对数概率\n",
    "        log_prob = similarity_matrix - torch.log(denom)\n",
    "        numerator = (mask * log_prob).sum(dim=1)\n",
    "        # 正样本个数（防止除 0）\n",
    "        pos_count = mask.sum(dim=1) + 1e-8\n",
    "        loss = - (numerator / pos_count)\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "#########################################\n",
    "# 3. 数据增强方法\n",
    "#########################################\n",
    "# 原始的特征扰动（用于 \"feature\" 增强方式）\n",
    "def perturb_features(features, noise_level=0.1):\n",
    "    \"\"\"对特征进行扰动，生成增强视图\"\"\"\n",
    "    noise = torch.randn_like(features) * noise_level\n",
    "    return features + noise\n",
    "\n",
    "# 节点丢弃\n",
    "def augment_node_drop(features, edge_index, drop_prob=0.1):\n",
    "    \"\"\"\n",
    "    节点丢弃：以一定概率丢弃节点（将被丢弃节点的特征置零，\n",
    "    同时删除其相关边，但保持节点的序号不变）。\n",
    "    \"\"\"\n",
    "    if isinstance(drop_prob, (list, tuple)):\n",
    "        drop_prob = float(drop_prob[0])\n",
    "    num_nodes = features.shape[0]\n",
    "    keep_mask = (torch.rand(num_nodes, device=features.device) > drop_prob)\n",
    "    features_aug = features * keep_mask.unsqueeze(1).float()\n",
    "    src, dst = edge_index\n",
    "    valid_edge_mask = keep_mask[src] & keep_mask[dst]\n",
    "    edge_index_aug = edge_index[:, valid_edge_mask]\n",
    "    return features_aug, edge_index_aug\n",
    "\n",
    "# 边丢弃\n",
    "def augment_edge_drop(features, edge_index, drop_prob=0.1):\n",
    "    \"\"\"\n",
    "    边丢弃：以一定概率删除边，但保留所有节点和原始特征。\n",
    "    \"\"\"\n",
    "    if isinstance(drop_prob, (list, tuple)):\n",
    "        drop_prob = float(drop_prob[0])\n",
    "    num_edges = edge_index.shape[1]\n",
    "    mask = (torch.rand(num_edges, device=edge_index.device) > drop_prob)\n",
    "    edge_index_aug = edge_index[:, mask]\n",
    "    return features, edge_index_aug\n",
    "\n",
    "# 边扰动\n",
    "def augment_edge_perturb(features, edge_index, drop_prob=0.1):\n",
    "    \"\"\"\n",
    "    边扰动：先以一定概率删除部分边，再随机添加一些新的边，\n",
    "    添加的新边数量与被删除边的数量相当。\n",
    "    \"\"\"\n",
    "    if isinstance(drop_prob, (list, tuple)):\n",
    "        drop_prob = float(drop_prob[0])\n",
    "    num_edges = edge_index.shape[1]\n",
    "    num_nodes = features.shape[0]\n",
    "    mask = (torch.rand(num_edges, device=edge_index.device) > drop_prob)\n",
    "    edge_index_dropped = edge_index[:, mask]\n",
    "    num_dropped = num_edges - mask.sum().item()\n",
    "    if num_dropped > 0:\n",
    "        new_edges = torch.randint(0, num_nodes, (2, num_dropped), device=features.device)\n",
    "        edge_index_aug = torch.cat([edge_index_dropped, new_edges], dim=1)\n",
    "    else:\n",
    "        edge_index_aug = edge_index_dropped\n",
    "    return features, edge_index_aug\n",
    "\n",
    "def augment_data(data, aug_method=\"feature\", aug_ratio=0.1):\n",
    "    \"\"\"\n",
    "    根据指定的增强方式对图数据进行增强，返回增强后的节点特征和 edge_index。\n",
    "    参数:\n",
    "      aug_method: \"feature\"（特征扰动）, \"node_drop\", \"edge_drop\", \"edge_perturb\"\n",
    "      aug_ratio: 控制增强强度（例如噪声水平或丢弃比例）\n",
    "    \"\"\"\n",
    "    if aug_method == \"feature\":\n",
    "        x_aug = perturb_features(data.x, noise_level=aug_ratio)\n",
    "        edge_index_aug = data.edge_index  # 图结构不变\n",
    "        return x_aug, edge_index_aug\n",
    "    elif aug_method == \"node_drop\":\n",
    "        return augment_node_drop(data.x, data.edge_index, drop_prob=aug_ratio)\n",
    "    elif aug_method == \"edge_drop\":\n",
    "        return augment_edge_drop(data.x, data.edge_index, drop_prob=aug_ratio)\n",
    "    elif aug_method == \"edge_perturb\":\n",
    "        return augment_edge_perturb(data.x, data.edge_index, drop_prob=aug_ratio)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown augmentation method: {aug_method}\")\n",
    "\n",
    "#########################################\n",
    "# 4. 训练函数（预训练 + 微调）\n",
    "#########################################\n",
    "def pretrain_model(data, model, optimizer, criterion_contrast, num_epochs=200, aug_method=\"feature\", aug_ratio=0.1):\n",
    "    \"\"\"\n",
    "    预训练阶段：仅使用对比损失训练模型（不计算 Focal Loss）。\n",
    "    对输入数据使用指定的增强方式生成两个视图，然后通过模型提取节点表示，\n",
    "    计算对比损失（仅对训练集节点计算）。\n",
    "    \"\"\"\n",
    "    best_f1 = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 生成两个增强视图\n",
    "        x_aug1, edge_index1 = augment_data(data, aug_method, aug_ratio)\n",
    "        x_aug2, edge_index2 = augment_data(data, aug_method, aug_ratio)\n",
    "\n",
    "        embedding_aug1 = model.encode(x_aug1, edge_index1)\n",
    "        embedding_aug2 = model.encode(x_aug2, edge_index2)\n",
    "\n",
    "        # 构造形状为 [N, 2, feature_dim] 的张量\n",
    "        features_aug = torch.stack([embedding_aug1, embedding_aug2], dim=1)\n",
    "\n",
    "        # 计算对比损失（仅对训练集节点计算）\n",
    "        loss_contrast = criterion_contrast(features_aug[data.train_mask])\n",
    "        loss = loss_contrast\n",
    "        \n",
    "        # print('epoch:', epoch, ', contrast Loss:', loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 验证阶段（使用分类任务评估模型效果）\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(data)\n",
    "            val_preds = val_out[data.val_mask].argmax(dim=1)\n",
    "            val_true = data.y[data.val_mask]\n",
    "            val_f1 = f1_score(val_true.cpu(), val_preds.cpu(), average=\"macro\")\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "def fine_tune_model(data, model, optimizer, criterion_focal, num_epochs=50):\n",
    "    \"\"\"\n",
    "    微调阶段：仅使用 Focal Loss 进行训练（不计算对比损失）。\n",
    "    \"\"\"\n",
    "    best_f1 = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(data)\n",
    "        loss = criterion_focal(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(data)\n",
    "            val_preds = val_out[data.val_mask].argmax(dim=1)\n",
    "            val_true = data.y[data.val_mask]\n",
    "            val_f1 = f1_score(val_true.cpu(), val_preds.cpu(), average=\"macro\")\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "def two_stage_train_model(data, model, optimizer, optimizer_ft, criterion_focal, criterion_contrast,\n",
    "                          pretrain_epochs, finetune_epochs, aug_method=\"feature\", aug_ratio=0.1):\n",
    "    \"\"\"\n",
    "    两阶段训练：\n",
    "      第一阶段：预训练（仅用对比损失）；\n",
    "      第二阶段：微调（仅用分类损失）。\n",
    "    \"\"\"\n",
    "    print(\"========== 开始预训练阶段 ==========\")\n",
    "    best_pretrain_state = pretrain_model(data, model, optimizer, criterion_contrast,\n",
    "                                         num_epochs=pretrain_epochs,\n",
    "                                         aug_method=aug_method, aug_ratio=aug_ratio)\n",
    "    model.load_state_dict(best_pretrain_state)\n",
    "\n",
    "    print(\"========== 开始微调阶段 ==========\")\n",
    "    best_finetune_state = fine_tune_model(data, model, optimizer_ft,\n",
    "                                          criterion_focal, num_epochs=finetune_epochs)\n",
    "    return best_finetune_state\n",
    "\n",
    "#########################################\n",
    "# 5. 封装随机采样超参数组合的函数\n",
    "#########################################\n",
    "def get_random_hyperparameter_combinations(n_iter):\n",
    "    \"\"\"\n",
    "    随机生成 n_iter 个超参数组合，每个组合包含：\n",
    "      (threshold, random_state, num_layers, hidden_channels, finetune_lr, pretrain_lr,\n",
    "       gamma, alpha_value, aug_method, aug_ratio, pretrain_epochs, temperature,\n",
    "       model_type, dropout_rate)\n",
    "    \n",
    "    其中以下超参数从均匀分布中采样：\n",
    "      - threshold: [0.1, 0.2]\n",
    "      - gamma: [2, 4]\n",
    "      - alpha_value: [0.3, 0.6]\n",
    "      - aug_ratio: [0.05, 0.25]\n",
    "      - temperature: [0.05, 0.1]\n",
    "      - dropout_rate: [0.2, 0.7]\n",
    "    \n",
    "    其余离散变量使用 random.choice 从候选列表中选择，\n",
    "    并随机选择模型类型：\"GraphSAGE\" 或 \"GAT\"。\n",
    "    \"\"\"\n",
    "    combinations = []\n",
    "    # 离散变量候选列表\n",
    "    random_states = [30]\n",
    "    num_layers_list = [2]\n",
    "    hidden_channels_list = [220]\n",
    "    finetune_lrs = [0.01]\n",
    "    pretrain_lrs = [0.001]\n",
    "    aug_methods = [\"feature\"]  # \"node_drop\", \"edge_drop\", \"edge_perturb\"\n",
    "    pretrain_epochs_list = [200]\n",
    "    model_types = [\"GraphSAGE\"]\n",
    "    dropout_rates = [0.2]\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # 连续变量，从均匀分布中采样\n",
    "        threshold = 0.2\n",
    "        gamma = 3.7\n",
    "        alpha_value = 0.3\n",
    "        aug_ratio = 0.22\n",
    "        temperature = 3.7\n",
    "        \n",
    "        \n",
    "        # 离散变量随机选择\n",
    "        dropout_rate = random.choice(dropout_rates)\n",
    "        random_state = random.choice(random_states)\n",
    "        num_layers = random.choice(num_layers_list)\n",
    "        hidden_channels = random.choice(hidden_channels_list)\n",
    "        finetune_lr = random.choice(finetune_lrs)\n",
    "        pretrain_lr = random.choice(pretrain_lrs)\n",
    "        aug_method = random.choice(aug_methods)\n",
    "        pretrain_epochs = random.choice(pretrain_epochs_list)\n",
    "        model_type = random.choice(model_types)\n",
    "        \n",
    "        combinations.append((threshold, random_state, num_layers, hidden_channels,\n",
    "                             finetune_lr, pretrain_lr, gamma, alpha_value,\n",
    "                             aug_method, aug_ratio, pretrain_epochs, temperature,\n",
    "                             model_type, dropout_rate))\n",
    "    return combinations\n",
    "\n",
    "#########################################\n",
    "# 6. 随机搜索超参数并评估模型\n",
    "#########################################\n",
    "def grid_search(X, y, X_train, X_valid, X_test, y_train, y_valid,\n",
    "                train_mask, valid_mask, test_mask, n_iter):\n",
    "    best_acc = 0.0\n",
    "    best_overall_model_state = None\n",
    "    best_overall_params = None\n",
    "\n",
    "    print(\"Start random search with {} combinations...\".format(n_iter))\n",
    "    hyperparam_combos = get_random_hyperparameter_combinations(n_iter)\n",
    "    for i, (threshold, random_state, num_layers, hidden_channels, finetune_lr,\n",
    "            pretrain_lr, gamma, alpha_value, aug_method, aug_ratio, pretrain_epochs, temperature,\n",
    "            model_type, dropout_rate) in enumerate(hyperparam_combos):\n",
    "        print(f\"\\nTesting combination {i+1}: threshold={threshold:.4f}, random_state={random_state}, \"\n",
    "              f\"layers={num_layers}, hidden_channels={hidden_channels}, finetune_lr={finetune_lr}, \"\n",
    "              f\"pretrain_lr={pretrain_lr}, gamma={gamma:.4f}, alpha={alpha_value:.4f}, aug_method={aug_method}, \"\n",
    "              f\"aug_ratio={aug_ratio:.4f}, pretrain_epochs={pretrain_epochs}, temperature={temperature:.4f}, \"\n",
    "              f\"model_type={model_type}, dropout_rate={dropout_rate:.4f}\")\n",
    "\n",
    "        # 计算邻接矩阵，并转换为 edge_index（假设 compute_adjacency_matrix 与 adjacency_to_edge_index 已实现）\n",
    "        adj_matrix = compute_adjacency_matrix(X_train, X_valid, X_test, y_train, y_valid,\n",
    "                                              threshold=threshold, random_state=random_state)\n",
    "        edge_index = adjacency_to_edge_index(adj_matrix).to(device)\n",
    "        # 如有需要，可构造 dense tensor\n",
    "        adj_tensor = torch.tensor(adj_matrix.toarray(), dtype=torch.float).to(device)\n",
    "\n",
    "        X_tensor = torch.tensor(X.values, dtype=torch.float)\n",
    "        y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "        data = Data(x=X_tensor, y=y_tensor, edge_index=edge_index,\n",
    "                    train_mask=train_mask, val_mask=valid_mask, test_mask=test_mask).to(device)\n",
    "\n",
    "        # 根据 model_type 选择模型，并传入 dropout_rate 参数\n",
    "        if model_type == \"GraphSAGE\":\n",
    "            model = GraphSAGE(in_channels=X.shape[1], hidden_channels=hidden_channels,\n",
    "                              out_channels=len(np.unique(y)), num_layers=num_layers, dropout_rate=dropout_rate).to(device)\n",
    "        elif model_type == \"GAT\":\n",
    "            model = GAT(in_channels=X.shape[1], hidden_channels=hidden_channels,\n",
    "                        out_channels=len(np.unique(y)), num_layers=num_layers, dropout_rate=dropout_rate).to(device)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=pretrain_lr, weight_decay=5e-4)\n",
    "        optimizer_ft = torch.optim.Adam(model.parameters(), lr=finetune_lr, weight_decay=5e-4)\n",
    "\n",
    "        alpha_list = [1 - alpha_value, alpha_value]\n",
    "        alpha_tensor = torch.tensor(alpha_list, dtype=torch.float).to(device)\n",
    "        criterion_focal = FocalLoss(gamma=gamma, alpha=alpha_tensor, reduction=\"mean\")\n",
    "        # 使用随机采样的 temperature 参数\n",
    "        criterion_contrast = SupConLoss(temperature=temperature)\n",
    "\n",
    "        best_model_epoch = two_stage_train_model(data, model, optimizer, optimizer_ft, criterion_focal,\n",
    "                                                 criterion_contrast, pretrain_epochs=pretrain_epochs, finetune_epochs=100,\n",
    "                                                 aug_method=aug_method, aug_ratio=aug_ratio)\n",
    "        model.load_state_dict(best_model_epoch)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_out = model(data)\n",
    "            preds = test_out[data.test_mask].argmax(dim=1)\n",
    "            test_acc = accuracy_score(data.y[data.test_mask].cpu(), preds.cpu())\n",
    "        print(f\"Test Accuracy for current combination: {test_acc:.4f}\")\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_overall_model_state = best_model_epoch\n",
    "            best_overall_params = (threshold, random_state, num_layers, hidden_channels,\n",
    "                                   finetune_lr, pretrain_lr, gamma, alpha_value, aug_method, aug_ratio,\n",
    "                                   pretrain_epochs, temperature, model_type, dropout_rate)\n",
    "\n",
    "    return best_overall_params, best_overall_model_state\n",
    "\n",
    "#########################################\n",
    "# 7. 主程序：加载数据、随机搜索超参数、加载最佳模型并评估\n",
    "#########################################\n",
    "# 假设 load_data_SGER 返回：\n",
    "# X, y, X_train, X_valid, X_test, y_train, y_valid, y_test,\n",
    "# train_mask, valid_mask, test_mask\n",
    "X, y, X_train, X_valid, X_test, y_train, y_valid, y_test, train_mask, valid_mask, test_mask = load_data_SGER()\n",
    "\n",
    "# 随机搜索（例如搜索 100 个超参数组合）\n",
    "best_params, best_model_state = grid_search(X, y, X_train, X_valid, X_test,\n",
    "                                            y_train, y_valid, train_mask, valid_mask, test_mask, n_iter=200)\n",
    "print(\"\\nBest Hyperparameters:\", best_params)\n",
    "\n",
    "# 解包最佳超参数（注意增加了 temperature, model_type, dropout_rate 参数）\n",
    "(threshold, random_state, num_layers, hidden_channels,\n",
    " finetune_lr, pretrain_lr, gamma, alpha_value, aug_method, aug_ratio,\n",
    " pretrain_epochs, temperature, model_type, dropout_rate) = best_params\n",
    "\n",
    "adj_matrix = compute_adjacency_matrix(X_train, X_valid, X_test, y_train, y_valid,\n",
    "                                      threshold=threshold, random_state=random_state)\n",
    "edge_index = adjacency_to_edge_index(adj_matrix).to(device)\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float).to(device)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.long).to(device)\n",
    "data = Data(x=X_tensor, y=y_tensor, edge_index=edge_index,\n",
    "            train_mask=train_mask.to(device), val_mask=valid_mask.to(device), test_mask=test_mask.to(device))\n",
    "\n",
    "# 根据最终选出的模型类型构造模型\n",
    "if model_type == \"GraphSAGE\":\n",
    "    model = GraphSAGE(in_channels=X.shape[1], hidden_channels=hidden_channels,\n",
    "                      out_channels=len(np.unique(y)), num_layers=num_layers, dropout_rate=dropout_rate).to(device)\n",
    "elif model_type == \"GAT\":\n",
    "    model = GAT(in_channels=X.shape[1], hidden_channels=hidden_channels,\n",
    "                out_channels=len(np.unique(y)), num_layers=num_layers, dropout_rate=dropout_rate).to(device)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_out = model(data)\n",
    "    preds = test_out[data.test_mask].argmax(dim=1)\n",
    "    true_labels = data.y[data.test_mask]\n",
    "    report = classification_report(true_labels.cpu(), preds.cpu(), target_names=[\"Class 0\", \"Class 1\"], digits=4)\n",
    "    test_precision = precision_score(true_labels.cpu(), preds.cpu(), average=\"macro\")\n",
    "    test_recall = recall_score(true_labels.cpu(), preds.cpu(), average=\"macro\")\n",
    "    test_f1 = f1_score(true_labels.cpu(), preds.cpu(), average=\"macro\")\n",
    "    test_acc = accuracy_score(true_labels.cpu(), preds.cpu())\n",
    "    \n",
    "    print(\"\\nBest Model Classification Report on Test Set:\")\n",
    "    print(report)\n",
    "    print(\"Best Model Test Set Metrics:\")\n",
    "    print(f\"Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}, Accuracy: {test_acc:.4f}\")\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data_SGER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 499\u001B[0m\n\u001B[1;32m    491\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m best_overall_params, best_overall_model_state\n\u001B[1;32m    493\u001B[0m \u001B[38;5;66;03m#########################################\u001B[39;00m\n\u001B[1;32m    494\u001B[0m \u001B[38;5;66;03m# 7. 主程序：加载数据、随机搜索超参数、加载最佳模型并评估\u001B[39;00m\n\u001B[1;32m    495\u001B[0m \u001B[38;5;66;03m#########################################\u001B[39;00m\n\u001B[1;32m    496\u001B[0m \u001B[38;5;66;03m# 假设 load_data_SGER 返回：\u001B[39;00m\n\u001B[1;32m    497\u001B[0m \u001B[38;5;66;03m# X, y, X_train, X_valid, X_test, y_train, y_valid, y_test,\u001B[39;00m\n\u001B[1;32m    498\u001B[0m \u001B[38;5;66;03m# train_mask, valid_mask, test_mask\u001B[39;00m\n\u001B[0;32m--> 499\u001B[0m X, y, X_train, X_valid, X_test, y_train, y_valid, y_test, train_mask, valid_mask, test_mask \u001B[38;5;241m=\u001B[39m \u001B[43mload_data_SGER\u001B[49m()\n\u001B[1;32m    501\u001B[0m \u001B[38;5;66;03m# 随机搜索（例如搜索 100 个超参数组合）\u001B[39;00m\n\u001B[1;32m    502\u001B[0m best_params, best_model_state \u001B[38;5;241m=\u001B[39m grid_search(X, y, X_train, X_valid, X_test,\n\u001B[1;32m    503\u001B[0m                                             y_train, y_valid, train_mask, valid_mask, test_mask, n_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'load_data_SGER' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "TEST",
   "id": "8c278bdafa3b73c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T22:01:08.982503Z",
     "start_time": "2025-02-21T21:53:20.703477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 稳定跑到0.80\n",
    "import itertools\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv, GATConv\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#########################################\n",
    "# 1. 定义模型\n",
    "#########################################\n",
    "# GraphSAGE 模型（包含残差结构和 dropout，每层隐藏单元数递减至上一层的3/4）\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout_rate=0.5):\n",
    "        \"\"\"\n",
    "        参数说明：\n",
    "          in_channels: 输入特征维度\n",
    "          hidden_channels: 第一层的隐藏单元数\n",
    "          out_channels: 输出类别数\n",
    "          num_layers: 图卷积层的总层数（至少为 1）\n",
    "          dropout_rate: dropout 概率\n",
    "        \"\"\"\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.residuals = torch.nn.ModuleList()\n",
    "        # 第一层：从 in_channels 到 hidden_channels\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        if in_channels != hidden_channels:\n",
    "            self.residuals.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        else:\n",
    "            self.residuals.append(torch.nn.Identity())\n",
    "        current_hidden = hidden_channels\n",
    "        # 后续每一层的隐藏单元数为上一层的 3/4（向下取整，最小为 1）\n",
    "        for _ in range(num_layers - 1):\n",
    "            next_hidden = max(1, int(current_hidden * 3 / 4))\n",
    "            self.convs.append(SAGEConv(current_hidden, next_hidden))\n",
    "            if current_hidden != next_hidden:\n",
    "                self.residuals.append(torch.nn.Linear(current_hidden, next_hidden))\n",
    "            else:\n",
    "                self.residuals.append(torch.nn.Identity())\n",
    "            current_hidden = next_hidden\n",
    "        # 全连接层：将最后一层的隐藏向量映射到输出类别\n",
    "        self.fc = torch.nn.Linear(current_hidden, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        \"\"\"提取节点表示：依次通过图卷积层、残差连接、ReLU 和 dropout\"\"\"\n",
    "        for conv, res in zip(self.convs, self.residuals):\n",
    "            out = conv(x, edge_index)\n",
    "            res_x = res(x)\n",
    "            x = self.dropout(torch.relu(out + res_x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.encode(data.x, data.edge_index)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# GAT 模型（包含残差结构和 dropout，每层隐藏单元数递减至上一层的3/4）\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout_rate=0.5, heads=1, concat=True):\n",
    "        \"\"\"\n",
    "        参数说明：\n",
    "          in_channels: 输入特征维度\n",
    "          hidden_channels: 第一层的隐藏单元数\n",
    "          out_channels: 输出类别数\n",
    "          num_layers: 图卷积层的总层数（至少为 1）\n",
    "          dropout_rate: dropout 概率（同时用于 GATConv 内部 dropout）\n",
    "          heads: 注意力头的数量\n",
    "          concat: 是否拼接多头输出（True）或取平均（False）\n",
    "        \"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.residuals = torch.nn.ModuleList()\n",
    "        # 第一层\n",
    "        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout_rate, concat=concat))\n",
    "        out_dim = hidden_channels * heads if concat else hidden_channels\n",
    "        if in_channels != out_dim:\n",
    "            self.residuals.append(torch.nn.Linear(in_channels, out_dim))\n",
    "        else:\n",
    "            self.residuals.append(torch.nn.Identity())\n",
    "        current_dim = out_dim\n",
    "        # 后续层，每层隐藏单元数为上一层的 3/4（向下取整，最小为 1）\n",
    "        for _ in range(num_layers - 1):\n",
    "            next_hidden = max(1, int(current_dim * 3 / 4))\n",
    "            self.convs.append(GATConv(current_dim, next_hidden, heads=heads, dropout=dropout_rate, concat=concat))\n",
    "            new_out_dim = next_hidden * heads if concat else next_hidden\n",
    "            if current_dim != new_out_dim:\n",
    "                self.residuals.append(torch.nn.Linear(current_dim, new_out_dim))\n",
    "            else:\n",
    "                self.residuals.append(torch.nn.Identity())\n",
    "            current_dim = new_out_dim\n",
    "        self.fc = torch.nn.Linear(current_dim, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        \"\"\"提取节点表示：依次通过 GAT 层、残差连接、ReLU 和 dropout\"\"\"\n",
    "        for conv, res in zip(self.convs, self.residuals):\n",
    "            out = conv(x, edge_index)\n",
    "            res_x = res(x)\n",
    "            x = self.dropout(torch.relu(out + res_x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.encode(data.x, data.edge_index)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "#########################################\n",
    "# 2. 定义损失函数\n",
    "#########################################\n",
    "# Focal Loss（用于处理类别不平衡）\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None, reduction=\"mean\"):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  \n",
    "        self.reduction = reduction\n",
    "        self.ce = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        if self.alpha is not None:\n",
    "            if isinstance(self.alpha, (list, np.ndarray)):\n",
    "                alpha = inputs.new_tensor(self.alpha)\n",
    "            else:\n",
    "                alpha = self.alpha\n",
    "            at = alpha.gather(0, targets.data)\n",
    "            ce_loss = at * ce_loss\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean() if self.reduction == \"mean\" else focal_loss.sum()\n",
    "\n",
    "# 修改后的普通对比学习损失（不使用 mask）\n",
    "class SupConLoss(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            temperature: 温度参数\n",
    "        \"\"\"\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: [batch_size, n_views, feature_dim]\n",
    "                      要求每个样本至少有两个视图，视图之间互为正样本，其余样本均为负样本。\n",
    "        Returns:\n",
    "            对比损失（InfoNCE Loss）\n",
    "        \"\"\"\n",
    "        device = features.device\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` 需要形状为 [batch_size, n_views, feature_dim]')\n",
    "        batch_size, n_views, feature_dim = features.shape\n",
    "\n",
    "        # 将多个视图拼接为 [batch_size*n_views, feature_dim]\n",
    "        features = features.view(batch_size * n_views, feature_dim)\n",
    "        # 对每个特征进行 L2 归一化\n",
    "        features = torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "\n",
    "        # 计算相似度矩阵，形状 [batch_size*n_views, batch_size*n_views]\n",
    "        similarity_matrix = torch.matmul(features, features.T) / self.temperature\n",
    "\n",
    "        # 构造正样本掩码：同一原始样本（即同一 batch 中的不同视图）的两两之间为正样本\n",
    "        labels = torch.arange(batch_size, device=device).repeat_interleave(n_views)\n",
    "        mask = torch.eq(labels.unsqueeze(1), labels.unsqueeze(0)).float()\n",
    "        # 去除自身对比（对角线置 0）\n",
    "        self_mask = torch.eye(mask.shape[0], device=device)\n",
    "        mask = mask - self_mask\n",
    "\n",
    "        # 计算 exp(similarity)\n",
    "        exp_sim = torch.exp(similarity_matrix) * (1 - self_mask)\n",
    "        # 对每个 anchor，分母为除自身外所有样本的 exp(sim)\n",
    "        denom = exp_sim.sum(dim=1, keepdim=True) + 1e-8\n",
    "\n",
    "        # 计算仅正样本对的对数概率\n",
    "        log_prob = similarity_matrix - torch.log(denom)\n",
    "        numerator = (mask * log_prob).sum(dim=1)\n",
    "        # 正样本个数（防止除 0）\n",
    "        pos_count = mask.sum(dim=1) + 1e-8\n",
    "        loss = - (numerator / pos_count)\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "#########################################\n",
    "# 3. 数据增强方法\n",
    "#########################################\n",
    "# 原始的特征扰动（用于 \"feature\" 增强方式）\n",
    "def perturb_features(features, noise_level=0.1):\n",
    "    \"\"\"对特征进行扰动，生成增强视图\"\"\"\n",
    "    noise = torch.randn_like(features) * noise_level\n",
    "    return features + noise\n",
    "\n",
    "# 节点丢弃\n",
    "def augment_node_drop(features, edge_index, drop_prob=0.1):\n",
    "    \"\"\"\n",
    "    节点丢弃：以一定概率丢弃节点（将被丢弃节点的特征置零，\n",
    "    同时删除其相关边，但保持节点的序号不变）。\n",
    "    \"\"\"\n",
    "    if isinstance(drop_prob, (list, tuple)):\n",
    "        drop_prob = float(drop_prob[0])\n",
    "    num_nodes = features.shape[0]\n",
    "    keep_mask = (torch.rand(num_nodes, device=features.device) > drop_prob)\n",
    "    features_aug = features * keep_mask.unsqueeze(1).float()\n",
    "    src, dst = edge_index\n",
    "    valid_edge_mask = keep_mask[src] & keep_mask[dst]\n",
    "    edge_index_aug = edge_index[:, valid_edge_mask]\n",
    "    return features_aug, edge_index_aug\n",
    "\n",
    "# 边丢弃\n",
    "def augment_edge_drop(features, edge_index, drop_prob=0.1):\n",
    "    \"\"\"\n",
    "    边丢弃：以一定概率删除边，但保留所有节点和原始特征。\n",
    "    \"\"\"\n",
    "    if isinstance(drop_prob, (list, tuple)):\n",
    "        drop_prob = float(drop_prob[0])\n",
    "    num_edges = edge_index.shape[1]\n",
    "    mask = (torch.rand(num_edges, device=edge_index.device) > drop_prob)\n",
    "    edge_index_aug = edge_index[:, mask]\n",
    "    return features, edge_index_aug\n",
    "\n",
    "# 边扰动\n",
    "def augment_edge_perturb(features, edge_index, drop_prob=0.1):\n",
    "    \"\"\"\n",
    "    边扰动：先以一定概率删除部分边，再随机添加一些新的边，\n",
    "    添加的新边数量与被删除边的数量相当。\n",
    "    \"\"\"\n",
    "    if isinstance(drop_prob, (list, tuple)):\n",
    "        drop_prob = float(drop_prob[0])\n",
    "    num_edges = edge_index.shape[1]\n",
    "    num_nodes = features.shape[0]\n",
    "    mask = (torch.rand(num_edges, device=edge_index.device) > drop_prob)\n",
    "    edge_index_dropped = edge_index[:, mask]\n",
    "    num_dropped = num_edges - mask.sum().item()\n",
    "    if num_dropped > 0:\n",
    "        new_edges = torch.randint(0, num_nodes, (2, num_dropped), device=features.device)\n",
    "        edge_index_aug = torch.cat([edge_index_dropped, new_edges], dim=1)\n",
    "    else:\n",
    "        edge_index_aug = edge_index_dropped\n",
    "    return features, edge_index_aug\n",
    "\n",
    "def augment_data(data, aug_method=\"feature\", aug_ratio=0.1):\n",
    "    \"\"\"\n",
    "    根据指定的增强方式对图数据进行增强，返回增强后的节点特征和 edge_index。\n",
    "    参数:\n",
    "      aug_method: \"feature\"（特征扰动）, \"node_drop\", \"edge_drop\", \"edge_perturb\"\n",
    "      aug_ratio: 控制增强强度（例如噪声水平或丢弃比例）\n",
    "    \"\"\"\n",
    "    if aug_method == \"feature\":\n",
    "        x_aug = perturb_features(data.x, noise_level=aug_ratio)\n",
    "        edge_index_aug = data.edge_index  # 图结构不变\n",
    "        return x_aug, edge_index_aug\n",
    "    elif aug_method == \"node_drop\":\n",
    "        return augment_node_drop(data.x, data.edge_index, drop_prob=aug_ratio)\n",
    "    elif aug_method == \"edge_drop\":\n",
    "        return augment_edge_drop(data.x, data.edge_index, drop_prob=aug_ratio)\n",
    "    elif aug_method == \"edge_perturb\":\n",
    "        return augment_edge_perturb(data.x, data.edge_index, drop_prob=aug_ratio)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown augmentation method: {aug_method}\")\n",
    "\n",
    "#########################################\n",
    "# 4. 训练函数（预训练 + 微调）\n",
    "#########################################\n",
    "def pretrain_model(data, model, optimizer, criterion_contrast, num_epochs=200, aug_method=\"feature\", aug_ratio=0.1):\n",
    "    \"\"\"\n",
    "    预训练阶段：仅使用对比损失训练模型（不计算 Focal Loss）。\n",
    "    对输入数据使用指定的增强方式生成两个视图，然后通过模型提取节点表示，\n",
    "    计算对比损失（仅对训练集节点计算）。\n",
    "    \"\"\"\n",
    "    best_f1 = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 生成两个增强视图\n",
    "        x_aug1, edge_index1 = augment_data(data, aug_method, aug_ratio)\n",
    "        x_aug2, edge_index2 = augment_data(data, aug_method, aug_ratio)\n",
    "\n",
    "        embedding_aug1 = model.encode(x_aug1, edge_index1)\n",
    "        embedding_aug2 = model.encode(x_aug2, edge_index2)\n",
    "\n",
    "        # 构造形状为 [N, 2, feature_dim] 的张量\n",
    "        features_aug = torch.stack([embedding_aug1, embedding_aug2], dim=1)\n",
    "\n",
    "        # 计算对比损失（仅对训练集节点计算）\n",
    "        loss_contrast = criterion_contrast(features_aug[data.train_mask])\n",
    "        loss = loss_contrast\n",
    "        \n",
    "        # print('epoch:', epoch, ', contrast Loss:', loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 验证阶段（使用分类任务评估模型效果）\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(data)\n",
    "            val_preds = val_out[data.val_mask].argmax(dim=1)\n",
    "            val_true = data.y[data.val_mask]\n",
    "            val_f1 = f1_score(val_true.cpu(), val_preds.cpu(), average=\"macro\")\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "def fine_tune_model(data, model, optimizer, criterion_focal, num_epochs=50):\n",
    "    \"\"\"\n",
    "    微调阶段：仅使用 Focal Loss 进行训练（不计算对比损失）。\n",
    "    \"\"\"\n",
    "    best_f1 = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(data)\n",
    "        loss = criterion_focal(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(data)\n",
    "            val_preds = val_out[data.val_mask].argmax(dim=1)\n",
    "            val_true = data.y[data.val_mask]\n",
    "            val_f1 = f1_score(val_true.cpu(), val_preds.cpu(), average=\"macro\")\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "def two_stage_train_model(data, model, optimizer, optimizer_ft, criterion_focal, criterion_contrast,\n",
    "                          pretrain_epochs, finetune_epochs, aug_method=\"feature\", aug_ratio=0.1):\n",
    "    \"\"\"\n",
    "    两阶段训练：\n",
    "      第一阶段：预训练（仅用对比损失）；\n",
    "      第二阶段：微调（仅用分类损失）。\n",
    "    \"\"\"\n",
    "    print(\"========== 开始预训练阶段 ==========\")\n",
    "    best_pretrain_state = pretrain_model(data, model, optimizer, criterion_contrast,\n",
    "                                         num_epochs=pretrain_epochs,\n",
    "                                         aug_method=aug_method, aug_ratio=aug_ratio)\n",
    "    model.load_state_dict(best_pretrain_state)\n",
    "\n",
    "    print(\"========== 开始微调阶段 ==========\")\n",
    "    best_finetune_state = fine_tune_model(data, model, optimizer_ft,\n",
    "                                          criterion_focal, num_epochs=finetune_epochs)\n",
    "    return best_finetune_state\n",
    "\n",
    "#########################################\n",
    "# 5. 封装随机采样超参数组合的函数\n",
    "#########################################\n",
    "def get_random_hyperparameter_combinations(n_iter):\n",
    "    \"\"\"\n",
    "    随机生成 n_iter 个超参数组合，每个组合包含：\n",
    "      (threshold, random_state, num_layers, hidden_channels, finetune_lr, pretrain_lr,\n",
    "       gamma, alpha_value, aug_method, aug_ratio, pretrain_epochs, temperature,\n",
    "       model_type, dropout_rate)\n",
    "    \n",
    "    其中以下超参数从均匀分布中采样：\n",
    "      - threshold: [0.1, 0.2]\n",
    "      - gamma: [2, 4]\n",
    "      - alpha_value: [0.3, 0.6]\n",
    "      - aug_ratio: [0.05, 0.25]\n",
    "      - temperature: [0.05, 0.1]\n",
    "      - dropout_rate: [0.2, 0.7]\n",
    "    \n",
    "    其余离散变量使用 random.choice 从候选列表中选择，\n",
    "    并随机选择模型类型：\"GraphSAGE\" 或 \"GAT\"。\n",
    "    \"\"\"\n",
    "    combinations = []\n",
    "    # 离散变量候选列表\n",
    "    random_states = [30]\n",
    "    num_layers_list = [2]\n",
    "    hidden_channels_list = [220]\n",
    "    finetune_lrs = [0.01]\n",
    "    pretrain_lrs = [0.001]\n",
    "    aug_methods = [\"feature\"]  # \"node_drop\", \"edge_drop\", \"edge_perturb\"\n",
    "    pretrain_epochs_list = [200]\n",
    "    model_types = [\"GraphSAGE\"]\n",
    "    dropout_rates = [0.2]\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # 连续变量，从均匀分布中采样\n",
    "        threshold = 0.2\n",
    "        gamma = 3.7\n",
    "        alpha_value = 0.3\n",
    "        aug_ratio = 0.22\n",
    "        temperature = 3.7\n",
    "        \n",
    "        \n",
    "        # 离散变量随机选择\n",
    "        dropout_rate = random.choice(dropout_rates)\n",
    "        random_state = random.choice(random_states)\n",
    "        num_layers = random.choice(num_layers_list)\n",
    "        hidden_channels = random.choice(hidden_channels_list)\n",
    "        finetune_lr = random.choice(finetune_lrs)\n",
    "        pretrain_lr = random.choice(pretrain_lrs)\n",
    "        aug_method = random.choice(aug_methods)\n",
    "        pretrain_epochs = random.choice(pretrain_epochs_list)\n",
    "        model_type = random.choice(model_types)\n",
    "        \n",
    "        combinations.append((threshold, random_state, num_layers, hidden_channels,\n",
    "                             finetune_lr, pretrain_lr, gamma, alpha_value,\n",
    "                             aug_method, aug_ratio, pretrain_epochs, temperature,\n",
    "                             model_type, dropout_rate))\n",
    "    return combinations\n",
    "\n",
    "#########################################\n",
    "# 6. 随机搜索超参数并评估模型\n",
    "#########################################\n",
    "def grid_search(X, y, X_train, X_valid, X_test, y_train, y_valid,\n",
    "                train_mask, valid_mask, test_mask, n_iter):\n",
    "    best_acc = 0.0\n",
    "    best_overall_model_state = None\n",
    "    best_overall_params = None\n",
    "\n",
    "    print(\"Start random search with {} combinations...\".format(n_iter))\n",
    "    hyperparam_combos = get_random_hyperparameter_combinations(n_iter)\n",
    "    for i, (threshold, random_state, num_layers, hidden_channels, finetune_lr,\n",
    "            pretrain_lr, gamma, alpha_value, aug_method, aug_ratio, pretrain_epochs, temperature,\n",
    "            model_type, dropout_rate) in enumerate(hyperparam_combos):\n",
    "        print(f\"\\nTesting combination {i+1}: threshold={threshold:.4f}, random_state={random_state}, \"\n",
    "              f\"layers={num_layers}, hidden_channels={hidden_channels}, finetune_lr={finetune_lr}, \"\n",
    "              f\"pretrain_lr={pretrain_lr}, gamma={gamma:.4f}, alpha={alpha_value:.4f}, aug_method={aug_method}, \"\n",
    "              f\"aug_ratio={aug_ratio:.4f}, pretrain_epochs={pretrain_epochs}, temperature={temperature:.4f}, \"\n",
    "              f\"model_type={model_type}, dropout_rate={dropout_rate:.4f}\")\n",
    "\n",
    "        # 计算邻接矩阵，并转换为 edge_index（假设 compute_adjacency_matrix 与 adjacency_to_edge_index 已实现）\n",
    "        adj_matrix = compute_adjacency_matrix(X_train, X_valid, X_test, y_train, y_valid,\n",
    "                                              threshold=threshold, random_state=random_state)\n",
    "        edge_index = adjacency_to_edge_index(adj_matrix).to(device)\n",
    "        # 如有需要，可构造 dense tensor\n",
    "        adj_tensor = torch.tensor(adj_matrix.toarray(), dtype=torch.float).to(device)\n",
    "\n",
    "        X_tensor = torch.tensor(X.values, dtype=torch.float)\n",
    "        y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "        data = Data(x=X_tensor, y=y_tensor, edge_index=edge_index,\n",
    "                    train_mask=train_mask, val_mask=valid_mask, test_mask=test_mask).to(device)\n",
    "\n",
    "        # 根据 model_type 选择模型，并传入 dropout_rate 参数\n",
    "        if model_type == \"GraphSAGE\":\n",
    "            model = GraphSAGE(in_channels=X.shape[1], hidden_channels=hidden_channels,\n",
    "                              out_channels=len(np.unique(y)), num_layers=num_layers, dropout_rate=dropout_rate).to(device)\n",
    "        elif model_type == \"GAT\":\n",
    "            model = GAT(in_channels=X.shape[1], hidden_channels=hidden_channels,\n",
    "                        out_channels=len(np.unique(y)), num_layers=num_layers, dropout_rate=dropout_rate).to(device)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=pretrain_lr, weight_decay=5e-4)\n",
    "        optimizer_ft = torch.optim.Adam(model.parameters(), lr=finetune_lr, weight_decay=5e-4)\n",
    "\n",
    "        alpha_list = [1 - alpha_value, alpha_value]\n",
    "        alpha_tensor = torch.tensor(alpha_list, dtype=torch.float).to(device)\n",
    "        criterion_focal = FocalLoss(gamma=gamma, alpha=alpha_tensor, reduction=\"mean\")\n",
    "        # 使用随机采样的 temperature 参数\n",
    "        criterion_contrast = SupConLoss(temperature=temperature)\n",
    "\n",
    "        best_model_epoch = two_stage_train_model(data, model, optimizer, optimizer_ft, criterion_focal,\n",
    "                                                 criterion_contrast, pretrain_epochs=pretrain_epochs, finetune_epochs=100,\n",
    "                                                 aug_method=aug_method, aug_ratio=aug_ratio)\n",
    "        model.load_state_dict(best_model_epoch)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_out = model(data)\n",
    "            preds = test_out[data.test_mask].argmax(dim=1)\n",
    "            test_acc = accuracy_score(data.y[data.test_mask].cpu(), preds.cpu())\n",
    "        print(f\"Test Accuracy for current combination: {test_acc:.4f}\")\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_overall_model_state = best_model_epoch\n",
    "            best_overall_params = (threshold, random_state, num_layers, hidden_channels,\n",
    "                                   finetune_lr, pretrain_lr, gamma, alpha_value, aug_method, aug_ratio,\n",
    "                                   pretrain_epochs, temperature, model_type, dropout_rate)\n",
    "\n",
    "    return best_overall_params, best_overall_model_state\n",
    "\n",
    "#########################################\n",
    "# 7. 主程序：加载数据、随机搜索超参数、加载最佳模型并评估\n",
    "#########################################\n",
    "# 假设 load_data_SGER 返回：\n",
    "# X, y, X_train, X_valid, X_test, y_train, y_valid, y_test,\n",
    "# train_mask, valid_mask, test_mask\n",
    "X, y, X_train, X_valid, X_test, y_train, y_valid, y_test, train_mask, valid_mask, test_mask = load_data_SGER()\n",
    "\n",
    "# 随机搜索（例如搜索 100 个超参数组合）\n",
    "best_params, best_model_state = grid_search(X, y, X_train, X_valid, X_test,\n",
    "                                            y_train, y_valid, train_mask, valid_mask, test_mask, n_iter=100)\n",
    "print(\"\\nBest Hyperparameters:\", best_params)\n",
    "\n",
    "# 解包最佳超参数（注意增加了 temperature, model_type, dropout_rate 参数）\n",
    "(threshold, random_state, num_layers, hidden_channels,\n",
    " finetune_lr, pretrain_lr, gamma, alpha_value, aug_method, aug_ratio,\n",
    " pretrain_epochs, temperature, model_type, dropout_rate) = best_params\n",
    "\n",
    "adj_matrix = compute_adjacency_matrix(X_train, X_valid, X_test, y_train, y_valid,\n",
    "                                      threshold=threshold, random_state=random_state)\n",
    "edge_index = adjacency_to_edge_index(adj_matrix).to(device)\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float).to(device)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.long).to(device)\n",
    "data = Data(x=X_tensor, y=y_tensor, edge_index=edge_index,\n",
    "            train_mask=train_mask.to(device), val_mask=valid_mask.to(device), test_mask=test_mask.to(device))\n",
    "\n",
    "# 根据最终选出的模型类型构造模型\n",
    "if model_type == \"GraphSAGE\":\n",
    "    model = GraphSAGE(in_channels=X.shape[1], hidden_channels=hidden_channels,\n",
    "                      out_channels=len(np.unique(y)), num_layers=num_layers, dropout_rate=dropout_rate).to(device)\n",
    "elif model_type == \"GAT\":\n",
    "    model = GAT(in_channels=X.shape[1], hidden_channels=hidden_channels,\n",
    "                out_channels=len(np.unique(y)), num_layers=num_layers, dropout_rate=dropout_rate).to(device)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_out = model(data)\n",
    "    preds = test_out[data.test_mask].argmax(dim=1)\n",
    "    true_labels = data.y[data.test_mask]\n",
    "    report = classification_report(true_labels.cpu(), preds.cpu(), target_names=[\"Class 0\", \"Class 1\"], digits=4)\n",
    "    test_precision = precision_score(true_labels.cpu(), preds.cpu(), average=\"macro\")\n",
    "    test_recall = recall_score(true_labels.cpu(), preds.cpu(), average=\"macro\")\n",
    "    test_f1 = f1_score(true_labels.cpu(), preds.cpu(), average=\"macro\")\n",
    "    test_acc = accuracy_score(true_labels.cpu(), preds.cpu())\n",
    "    \n",
    "    print(\"\\nBest Model Classification Report on Test Set:\")\n",
    "    print(report)\n",
    "    print(\"Best Model Test Set Metrics:\")\n",
    "    print(f\"Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}, Accuracy: {test_acc:.4f}\")\n"
   ],
   "id": "85ad3da61f5de298",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start random search with 100 combinations...\n",
      "\n",
      "Testing combination 1: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7300\n",
      "\n",
      "Testing combination 2: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7550\n",
      "\n",
      "Testing combination 3: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7600\n",
      "\n",
      "Testing combination 4: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7450\n",
      "\n",
      "Testing combination 5: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7550\n",
      "\n",
      "Testing combination 6: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7650\n",
      "\n",
      "Testing combination 7: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7550\n",
      "\n",
      "Testing combination 8: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7400\n",
      "\n",
      "Testing combination 9: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7350\n",
      "\n",
      "Testing combination 10: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7550\n",
      "\n",
      "Testing combination 11: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7800\n",
      "\n",
      "Testing combination 12: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7400\n",
      "\n",
      "Testing combination 13: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7500\n",
      "\n",
      "Testing combination 14: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7400\n",
      "\n",
      "Testing combination 15: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7550\n",
      "\n",
      "Testing combination 16: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7600\n",
      "\n",
      "Testing combination 17: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7500\n",
      "\n",
      "Testing combination 18: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7650\n",
      "\n",
      "Testing combination 19: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7600\n",
      "\n",
      "Testing combination 20: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7600\n",
      "\n",
      "Testing combination 21: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7350\n",
      "\n",
      "Testing combination 22: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7600\n",
      "\n",
      "Testing combination 23: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7550\n",
      "\n",
      "Testing combination 24: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7600\n",
      "\n",
      "Testing combination 25: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7400\n",
      "\n",
      "Testing combination 26: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7550\n",
      "\n",
      "Testing combination 27: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7400\n",
      "\n",
      "Testing combination 28: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7700\n",
      "\n",
      "Testing combination 29: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7600\n",
      "\n",
      "Testing combination 30: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7450\n",
      "\n",
      "Testing combination 31: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7550\n",
      "\n",
      "Testing combination 32: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7400\n",
      "\n",
      "Testing combination 33: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7750\n",
      "\n",
      "Testing combination 34: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7600\n",
      "\n",
      "Testing combination 35: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7300\n",
      "\n",
      "Testing combination 36: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7800\n",
      "\n",
      "Testing combination 37: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7500\n",
      "\n",
      "Testing combination 38: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7600\n",
      "\n",
      "Testing combination 39: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7400\n",
      "\n",
      "Testing combination 40: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7350\n",
      "\n",
      "Testing combination 41: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7650\n",
      "\n",
      "Testing combination 42: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7500\n",
      "\n",
      "Testing combination 43: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7350\n",
      "\n",
      "Testing combination 44: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7650\n",
      "\n",
      "Testing combination 45: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7550\n",
      "\n",
      "Testing combination 46: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7600\n",
      "\n",
      "Testing combination 47: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7550\n",
      "\n",
      "Testing combination 48: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7750\n",
      "\n",
      "Testing combination 49: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7700\n",
      "\n",
      "Testing combination 50: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7550\n",
      "\n",
      "Testing combination 51: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7550\n",
      "\n",
      "Testing combination 52: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7500\n",
      "\n",
      "Testing combination 53: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7700\n",
      "\n",
      "Testing combination 54: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7450\n",
      "\n",
      "Testing combination 55: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7650\n",
      "\n",
      "Testing combination 56: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7500\n",
      "\n",
      "Testing combination 57: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7400\n",
      "\n",
      "Testing combination 58: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7550\n",
      "\n",
      "Testing combination 59: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7450\n",
      "\n",
      "Testing combination 60: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7550\n",
      "\n",
      "Testing combination 61: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7350\n",
      "\n",
      "Testing combination 62: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7400\n",
      "\n",
      "Testing combination 63: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7800\n",
      "\n",
      "Testing combination 64: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7600\n",
      "\n",
      "Testing combination 65: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7600\n",
      "\n",
      "Testing combination 66: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7450\n",
      "\n",
      "Testing combination 67: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7300\n",
      "\n",
      "Testing combination 68: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7700\n",
      "\n",
      "Testing combination 69: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7750\n",
      "\n",
      "Testing combination 70: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7400\n",
      "\n",
      "Testing combination 71: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7600\n",
      "\n",
      "Testing combination 72: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7400\n",
      "\n",
      "Testing combination 73: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7800\n",
      "\n",
      "Testing combination 74: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7700\n",
      "\n",
      "Testing combination 75: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7450\n",
      "\n",
      "Testing combination 76: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7400\n",
      "\n",
      "Testing combination 77: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7500\n",
      "\n",
      "Testing combination 78: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7550\n",
      "\n",
      "Testing combination 79: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7450\n",
      "\n",
      "Testing combination 80: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7500\n",
      "\n",
      "Testing combination 81: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7500\n",
      "\n",
      "Testing combination 82: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7700\n",
      "\n",
      "Testing combination 83: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7550\n",
      "\n",
      "Testing combination 84: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7450\n",
      "\n",
      "Testing combination 85: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7500\n",
      "\n",
      "Testing combination 86: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7350\n",
      "\n",
      "Testing combination 87: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7500\n",
      "\n",
      "Testing combination 88: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7600\n",
      "\n",
      "Testing combination 89: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7750\n",
      "\n",
      "Testing combination 90: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7550\n",
      "\n",
      "Testing combination 91: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7700\n",
      "\n",
      "Testing combination 92: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7300\n",
      "\n",
      "Testing combination 93: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7550\n",
      "\n",
      "Testing combination 94: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7550\n",
      "\n",
      "Testing combination 95: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7650\n",
      "\n",
      "Testing combination 96: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7450\n",
      "\n",
      "Testing combination 97: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7300\n",
      "\n",
      "Testing combination 98: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7600\n",
      "\n",
      "Testing combination 99: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7650\n",
      "\n",
      "Testing combination 100: threshold=0.2000, random_state=30, layers=2, hidden_channels=220, finetune_lr=0.01, pretrain_lr=0.001, gamma=3.7000, alpha=0.3000, aug_method=feature, aug_ratio=0.2200, pretrain_epochs=200, temperature=3.7000, model_type=GraphSAGE, dropout_rate=0.2000\n",
      "========== 开始预训练阶段 ==========\n",
      "========== 开始微调阶段 ==========\n",
      "Test Accuracy for current combination: 0.7500\n",
      "\n",
      "Best Hyperparameters: (0.2, 30, 2, 220, 0.01, 0.001, 3.7, 0.3, 'feature', 0.22, 200, 3.7, 'GraphSAGE', 0.2)\n",
      "\n",
      "Best Model Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0     0.8243    0.8714    0.8472       140\n",
      "     Class 1     0.6538    0.5667    0.6071        60\n",
      "\n",
      "    accuracy                         0.7800       200\n",
      "   macro avg     0.7391    0.7190    0.7272       200\n",
      "weighted avg     0.7732    0.7800    0.7752       200\n",
      "\n",
      "Best Model Test Set Metrics:\n",
      "Precision: 0.7391, Recall: 0.7190, F1: 0.7272, Accuracy: 0.7800\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 使用随机森林模型进行训练，并生成测试集上的分类报告\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# 输出测试集上的分类报告\n",
    "print(\"Test Set Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, digits=4))\n",
    "\n",
    "\n",
    "# 为了便于比较，将预测结果转换为与 y_test 相同的索引\n",
    "y_pred_rf_series = pd.Series(y_pred_rf, index=y_test.index)\n",
    "\n",
    "# 统计各类样本的索引集合\n",
    "tp_idx = y_test[(y_test == 1) & (y_pred_rf_series == 1)].index.tolist()\n",
    "tn_idx = y_test[(y_test == 0) & (y_pred_rf_series == 0)].index.tolist()\n",
    "fp_idx = y_test[(y_test == 0) & (y_pred_rf_series == 1)].index.tolist()\n",
    "fn_idx = y_test[(y_test == 1) & (y_pred_rf_series == 0)].index.tolist()\n",
    "\n",
    "print(\"True Positives (1被分为1):\", tp_idx)\n",
    "print(\"True Negatives (0被分为0):\", tn_idx)\n",
    "print(\"False Positives (0被分为1):\", fp_idx)\n",
    "print(\"False Negatives (1被分为0):\", fn_idx)\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# 将四个列表合并为一个列表\n",
    "error_idx_list = [tp_idx, tn_idx, fp_idx, fn_idx]\n",
    "\n",
    "# 定义要保存的文件名\n",
    "filename = \"SGER-RFGNN.json\"\n",
    "\n",
    "# 保存为 JSON 文件\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump(error_idx_list, f, indent=4)\n",
    "\n",
    "print(f\"索引集合已保存至 {filename}\")\n",
    "\n",
    "\n",
    "# 如果只需要错误样本（FP和FN）的索引集合，可以合并如下：\n",
    "error_idx = {\n",
    "    \"[0,1]\": fp_idx,  # 真实为0，但预测为1\n",
    "    \"[1,0]\": fn_idx   # 真实为1，但预测为0\n",
    "}\n",
    "print(\"错误样本的索引集合:\", error_idx)\n",
    "\n",
    "# 读取 JSON 文件\n",
    "with open(filename, \"r\") as f:\n",
    "    loaded_error_idx_list = json.load(f)\n",
    "\n",
    "print(\"加载的索引集合:\", loaded_error_idx_list)\n"
   ],
   "id": "9121982c0aa503ba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
