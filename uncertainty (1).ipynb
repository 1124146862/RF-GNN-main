{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92b3eea7-e868-4e45-bbbe-04e907049cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# util function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.sparse import csr_matrix\n",
    "import torch\n",
    "\n",
    "def standard_input(X):\n",
    "    # 标准化输入\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "def load_data_SGER_RAW(random_state=42):\n",
    "    # 读取以空格分隔的SGER CSV文件\n",
    "    path = '/home/gehongfei/project/TabGNN/dataset/SGER1000.csv'\n",
    "    df = pd.read_csv(path, sep='\\s+')\n",
    "    # 确保 'kredit' 列存在\n",
    "    if 'kredit' not in df.columns:\n",
    "        print(\"Error: 'kredit' column not found.\")\n",
    "        return None, None, None, None, None, None\n",
    "    # 目标变量和特征\n",
    "    y = df['kredit']\n",
    "    X = df.drop(columns=['kredit'])\n",
    "    # 划分训练集、验证集和测试集\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=random_state, stratify=y)\n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=2/3, random_state=random_state, stratify=y_temp)\n",
    "    # 计算节点数并创建 mask\n",
    "    num_nodes = len(df)\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    # 获取索引并设置 mask\n",
    "    train_mask[X_train.index] = True\n",
    "    val_mask[X_valid.index] = True\n",
    "    test_mask[X_test.index] = True\n",
    "    # 标准化输入\n",
    "    X = standard_input(X)\n",
    "    X_train = standard_input(X_train)\n",
    "    X_valid = standard_input(X_valid)\n",
    "    X_test = standard_input(X_test)\n",
    "    return X, y, X_train, X_valid, X_test, y_train, y_valid, y_test, train_mask, val_mask, test_mask\n",
    "X, y, X_train, X_valid, X_test, y_train, y_valid, y_test, train_mask, val_mask, test_mask = load_data_SGER_RAW()\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b08195c-9e5f-4d48-864d-557e8b3e5e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 0.6003749641505155, Validation Loss: 0.5305873602628708\n",
      "Epoch 2/30, Train Loss: 0.5224356055259705, Validation Loss: 0.4850348085165024\n",
      "Epoch 3/30, Train Loss: 0.4759216010570526, Validation Loss: 0.4665784388780594\n",
      "Epoch 4/30, Train Loss: 0.44562107595530426, Validation Loss: 0.4634212255477905\n",
      "Epoch 5/30, Train Loss: 0.43111248449845746, Validation Loss: 0.46111078560352325\n",
      "Epoch 6/30, Train Loss: 0.4162434122779153, Validation Loss: 0.4752398729324341\n",
      "Epoch 7/30, Train Loss: 0.4016761075366627, Validation Loss: 0.47323934733867645\n",
      "Epoch 8/30, Train Loss: 0.38446633924137463, Validation Loss: 0.47396431863307953\n",
      "Epoch 9/30, Train Loss: 0.3728475123643875, Validation Loss: 0.47851546108722687\n",
      "Epoch 10/30, Train Loss: 0.35500493916598236, Validation Loss: 0.47707679867744446\n",
      "Epoch 11/30, Train Loss: 0.33597189458933746, Validation Loss: 0.5017910897731781\n",
      "Epoch 12/30, Train Loss: 0.32580102709206665, Validation Loss: 0.5007322132587433\n",
      "Epoch 13/30, Train Loss: 0.3002569350329312, Validation Loss: 0.4997875690460205\n",
      "Epoch 14/30, Train Loss: 0.2767917066812515, Validation Loss: 0.5662721246480942\n",
      "Epoch 15/30, Train Loss: 0.2760636129162528, Validation Loss: 0.5646814405918121\n",
      "Epoch 16/30, Train Loss: 0.2524244798855348, Validation Loss: 0.552339643239975\n",
      "Epoch 17/30, Train Loss: 0.22700966217301108, Validation Loss: 0.5958784818649292\n",
      "Epoch 18/30, Train Loss: 0.21531475470824676, Validation Loss: 0.5896573066711426\n",
      "Epoch 19/30, Train Loss: 0.18516761335459622, Validation Loss: 0.6288511604070663\n",
      "Epoch 20/30, Train Loss: 0.16625806553797287, Validation Loss: 0.7071508467197418\n",
      "Epoch 21/30, Train Loss: 0.16787116771394556, Validation Loss: 0.7347796112298965\n",
      "Epoch 22/30, Train Loss: 0.15290879051793704, Validation Loss: 0.7431739866733551\n",
      "Epoch 23/30, Train Loss: 0.12066282399676063, Validation Loss: 0.8171650171279907\n",
      "Epoch 24/30, Train Loss: 0.11059752987189726, Validation Loss: 0.864555299282074\n",
      "Epoch 25/30, Train Loss: 0.10125031186775728, Validation Loss: 0.8570433855056763\n",
      "Epoch 26/30, Train Loss: 0.09014859118244865, Validation Loss: 0.9604093134403229\n",
      "Epoch 27/30, Train Loss: 0.07459384067492052, Validation Loss: 0.9820881485939026\n",
      "Epoch 28/30, Train Loss: 0.07232721590182999, Validation Loss: 0.9953352808952332\n",
      "Epoch 29/30, Train Loss: 0.05771046741442247, Validation Loss: 1.105198711156845\n",
      "Epoch 30/30, Train Loss: 0.05066314068707553, Validation Loss: 1.110133171081543\n",
      "Test Accuracy: 0.75\n",
      "Test Recall: 0.5166666666666667\n",
      "Test F1 Score: 0.5535714285714286\n",
      "Test Precision: 0.5961538461538461\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.83       140\n",
      "           1       0.60      0.52      0.55        60\n",
      "\n",
      "    accuracy                           0.75       200\n",
      "   macro avg       0.70      0.68      0.69       200\n",
      "weighted avg       0.74      0.75      0.74       200\n",
      "\n",
      "Sample to Predict:    laufkont  laufzeit     moral     verw     hoehe  sparkont   beszeit  \\\n",
      "0 -0.490296 -1.294443 -0.579758  0.66335 -1.009094 -0.721009 -1.067797   \n",
      "\n",
      "       rate    famges    buerge  wohnzeit      verm     alter  weitkred  \\\n",
      "0  0.008981  1.724882 -0.320641 -1.585393 -0.357607 -1.099408  0.444788   \n",
      "\n",
      "       wohn  bishkred     beruf      pers    telef   gastarb  \n",
      "0  0.135954  -0.66218 -1.408326  0.377964 -0.84226  0.175863  \n",
      "Predicted Class: 1\n",
      "Uncertainty: 0.1662192940711975\n",
      "Similarity Matrix: tensor([[2.0179e+01, 7.9946e+00, 2.3155e+01,  ..., 1.2484e+00, 4.8671e+00,\n",
      "         6.8724e+00],\n",
      "        [7.9946e+00, 1.1061e+01, 1.4126e+01,  ..., 5.8312e+00, 3.3073e+00,\n",
      "         4.3186e+00],\n",
      "        [2.3155e+01, 1.4126e+01, 6.4318e+01,  ..., 7.3198e-04, 8.3259e+00,\n",
      "         1.3590e+01],\n",
      "        ...,\n",
      "        [1.2484e+00, 5.8312e+00, 7.3198e-04,  ..., 4.6864e+01, 5.2591e+00,\n",
      "         2.0279e+00],\n",
      "        [4.8671e+00, 3.3073e+00, 8.3259e+00,  ..., 5.2591e+00, 4.0607e+00,\n",
      "         2.6002e+00],\n",
      "        [6.8724e+00, 4.3186e+00, 1.3590e+01,  ..., 2.0279e+00, 2.6002e+00,\n",
      "         7.4183e+00]])\n",
      "Uncertainty Matrix: tensor([[4.0938e-04, 0.0000e+00, 5.1848e-03,  ..., 8.8311e-02, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 4.5075e-01, 0.0000e+00,  ..., 0.0000e+00, 2.2987e-01,\n",
      "         2.3671e-01],\n",
      "        [5.1848e-03, 0.0000e+00, 9.9602e-03,  ..., 9.3087e-02, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [8.8311e-02, 0.0000e+00, 9.3087e-02,  ..., 1.7621e-01, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 2.2987e-01, 0.0000e+00,  ..., 0.0000e+00, 9.0031e-03,\n",
      "         1.5842e-02],\n",
      "        [0.0000e+00, 2.3671e-01, 0.0000e+00,  ..., 0.0000e+00, 1.5842e-02,\n",
      "         2.2680e-02]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, classification_report\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, num_layers, dropout_rate):\n",
    "        super(ResidualMLP, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_size)\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size) for _ in range(num_layers - 1)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.input_layer(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            residual = out\n",
    "            out = F.relu(layer(out))\n",
    "            out = self.dropout(out)\n",
    "            out += residual  # Add residual connection\n",
    "        out = torch.sigmoid(self.output_layer(out))\n",
    "        return out\n",
    "\n",
    "class BinaryClassifier:\n",
    "    def __init__(self, input_dim, hidden_size=64, num_layers=3, dropout_rate=0.5, learning_rate=0.001):\n",
    "        self.model = ResidualMLP(input_dim, hidden_size, num_layers, dropout_rate)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def train(self, X_train, y_train, X_valid, y_valid, epochs=20, batch_size=32):\n",
    "        train_dataset = TensorDataset(torch.tensor(X_train.values, dtype=torch.float32), \n",
    "                                      torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1))\n",
    "        valid_dataset = TensorDataset(torch.tensor(X_valid.values, dtype=torch.float32), \n",
    "                                      torch.tensor(y_valid.values, dtype=torch.float32).unsqueeze(1))\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(X_batch)\n",
    "                loss = self.criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            valid_loss = self.evaluate(valid_loader)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader)}, Validation Loss: {valid_loss}\")\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in data_loader:\n",
    "                outputs = self.model(X_batch)\n",
    "                loss = self.criterion(outputs, y_batch)\n",
    "                total_loss += loss.item()\n",
    "        return total_loss / len(data_loader)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict_with_uncertainty(self, X_sample, n_iter=100):\n",
    "        self.model.train()  # Enable dropout\n",
    "        X_sample = torch.tensor(X_sample.values, dtype=torch.float32)\n",
    "        predictions = torch.zeros(n_iter, X_sample.size(0))\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            outputs = self.model(X_sample)\n",
    "            predictions[i] = outputs.squeeze()\n",
    "\n",
    "        mean_prediction = predictions.mean(dim=0)\n",
    "        uncertainty = predictions.std(dim=0)\n",
    "        predicted_classes = (mean_prediction > 0.5).int()\n",
    "\n",
    "        return predicted_classes, uncertainty\n",
    "\n",
    "    def predict(self, X_data):\n",
    "        self.model.eval()\n",
    "        X_tensor = torch.tensor(X_data.values, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "        predicted_classes = (outputs.squeeze() > 0.5).int()\n",
    "        return predicted_classes\n",
    "\n",
    "    def compute_similarity_and_uncertainty(self, X_data, n_iter=100):\n",
    "        self.model.train()  # Enable dropout\n",
    "        X_tensor = torch.tensor(X_data.values, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_vectors = self.model.input_layer(X_tensor)\n",
    "            for layer in self.model.hidden_layers:\n",
    "                output_vectors = F.relu(layer(output_vectors))\n",
    "                output_vectors = self.model.dropout(output_vectors)\n",
    "\n",
    "        similarity_matrix = torch.mm(output_vectors, output_vectors.t())\n",
    "        \n",
    "        predictions = torch.zeros(n_iter, X_tensor.size(0))\n",
    "        for i in range(n_iter):\n",
    "            outputs = self.model(X_tensor)\n",
    "            predictions[i] = outputs.squeeze()\n",
    "\n",
    "        mean_prediction = predictions.mean(dim=0)\n",
    "        uncertainty = predictions.std(dim=0)\n",
    "        predicted_classes = (mean_prediction > 0.5).int()\n",
    "\n",
    "        uncertainty_matrix = torch.zeros(X_tensor.size(0), X_tensor.size(0))\n",
    "        for i in range(X_tensor.size(0)):\n",
    "            for j in range(X_tensor.size(0)):\n",
    "                if predicted_classes[i] == predicted_classes[j]:\n",
    "                    uncertainty_matrix[i, j] = uncertainty[i] + uncertainty[j]\n",
    "                else:\n",
    "                    uncertainty_matrix[i, j] = 0\n",
    "\n",
    "        return similarity_matrix, uncertainty_matrix\n",
    "\n",
    "# Initialize and train the model\n",
    "input_dim = X.shape[1]\n",
    "classifier = BinaryClassifier(input_dim=input_dim, hidden_size=128, num_layers=4, dropout_rate=0.3, learning_rate=0.001)\n",
    "classifier.train(X_train, y_train, X_valid, y_valid, epochs=30, batch_size=64)\n",
    "\n",
    "# Save the model\n",
    "classifier.save_model('residual_mlp_model.pth')\n",
    "\n",
    "# Load the model\n",
    "classifier.load_model('residual_mlp_model.pth')\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "test_accuracy = accuracy_score(y_test, predictions)\n",
    "test_recall = recall_score(y_test, predictions)\n",
    "test_f1 = f1_score(y_test, predictions)\n",
    "test_precision = precision_score(y_test, predictions)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Recall: {test_recall}\")\n",
    "print(f\"Test F1 Score: {test_f1}\")\n",
    "print(f\"Test Precision: {test_precision}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Predict a single sample\n",
    "sample = X_test.iloc[[0]]\n",
    "print(\"Sample to Predict:\", sample)\n",
    "\n",
    "predicted_classes, uncertainty = classifier.predict_with_uncertainty(sample)\n",
    "\n",
    "print(\"Predicted Class:\", predicted_classes.item())\n",
    "print(\"Uncertainty:\", uncertainty.item())\n",
    "\n",
    "# Compute similarity and uncertainty matrices\n",
    "similarity_matrix, uncertainty_matrix = classifier.compute_similarity_and_uncertainty(pd.concat([X_train, X_valid, X_test]))\n",
    "print(\"Similarity Matrix:\", similarity_matrix)\n",
    "print(\"Uncertainty Matrix:\", uncertainty_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864e960c-6ae4-4b0a-8431-2b6857a2f8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd694c3d-3b31-4947-b299-d042a1e71a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
