{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T21:26:54.318721Z",
     "start_time": "2025-02-21T21:26:48.951051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Standardize the input data\n",
    "def standard_input(X):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    return X_scaled_df\n",
    "\n",
    "def load_data_AUS():\n",
    "    # 数据文件路径\n",
    "    path = '/home/gehongfei/project/TabGNN/dataset/AUS.csv'\n",
    "    # 读取没有表头且以空白字符分隔的数据\n",
    "    df = pd.read_csv(path, sep='\\s+')\n",
    "    \n",
    "    # 获取列数，假设最后一列为标签\n",
    "    n_cols = df.shape[1]\n",
    "    # 为前 n_cols-1 列生成特征列名，最后一列命名为 'label'\n",
    "    feature_cols = [f'feature_{i}' for i in range(n_cols - 1)]\n",
    "    df.columns = feature_cols + ['label']\n",
    "    \n",
    "    # 分离特征 X 和标签 y\n",
    "    y = df['label']\n",
    "    X = df.drop(columns=['label'])\n",
    "    \n",
    "    # 标准化特征数据\n",
    "    X = standard_input(X)\n",
    "    return X, y\n",
    "\n",
    "# 调用函数加载数据\n",
    "X, y = load_data_AUS()\n",
    "\n",
    "\n",
    "# Split data into train (70%), validation (10%), and test (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=2/3, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"MLP\": MLPClassifier(max_iter=1000),  # Increase max_iter for convergence\n",
    "    \"LDA\": LinearDiscriminantAnalysis()\n",
    "}\n",
    "\n",
    "# Evaluate models on test set\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_test_pred)\n",
    "    prec = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    rec = recall_score(y_test, y_test_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-score': f1\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(classification_report(y_test, y_test_pred, digits=4))\n",
    "\n",
    "# Report summary for all models\n",
    "print(\"\\nTest Performance Summary:\")\n",
    "for model, metrics in results.items():\n",
    "    print(f\"{model}: {metrics}\")\n"
   ],
   "id": "d47e7ca4df16ddd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree...\n",
      "\n",
      "Decision Tree Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8506    0.9024    0.8757        82\n",
      "           1     0.8431    0.7679    0.8037        56\n",
      "\n",
      "    accuracy                         0.8478       138\n",
      "   macro avg     0.8469    0.8351    0.8397       138\n",
      "weighted avg     0.8476    0.8478    0.8465       138\n",
      "\n",
      "Training Logistic Regression...\n",
      "\n",
      "Logistic Regression Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8734    0.8415    0.8571        82\n",
      "           1     0.7797    0.8214    0.8000        56\n",
      "\n",
      "    accuracy                         0.8333       138\n",
      "   macro avg     0.8265    0.8314    0.8286       138\n",
      "weighted avg     0.8354    0.8333    0.8340       138\n",
      "\n",
      "Training SVM...\n",
      "\n",
      "SVM Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8846    0.8415    0.8625        82\n",
      "           1     0.7833    0.8393    0.8103        56\n",
      "\n",
      "    accuracy                         0.8406       138\n",
      "   macro avg     0.8340    0.8404    0.8364       138\n",
      "weighted avg     0.8435    0.8406    0.8413       138\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Random Forest Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8690    0.8902    0.8795        82\n",
      "           1     0.8333    0.8036    0.8182        56\n",
      "\n",
      "    accuracy                         0.8551       138\n",
      "   macro avg     0.8512    0.8469    0.8488       138\n",
      "weighted avg     0.8546    0.8551    0.8546       138\n",
      "\n",
      "Training Naive Bayes...\n",
      "\n",
      "Naive Bayes Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8043    0.9024    0.8506        82\n",
      "           1     0.8261    0.6786    0.7451        56\n",
      "\n",
      "    accuracy                         0.8116       138\n",
      "   macro avg     0.8152    0.7905    0.7978       138\n",
      "weighted avg     0.8132    0.8116    0.8078       138\n",
      "\n",
      "Training MLP...\n",
      "\n",
      "MLP Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7955    0.8537    0.8235        82\n",
      "           1     0.7600    0.6786    0.7170        56\n",
      "\n",
      "    accuracy                         0.7826       138\n",
      "   macro avg     0.7777    0.7661    0.7703       138\n",
      "weighted avg     0.7811    0.7826    0.7803       138\n",
      "\n",
      "Training LDA...\n",
      "\n",
      "LDA Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9420    0.7927    0.8609        82\n",
      "           1     0.7536    0.9286    0.8320        56\n",
      "\n",
      "    accuracy                         0.8478       138\n",
      "   macro avg     0.8478    0.8606    0.8465       138\n",
      "weighted avg     0.8656    0.8478    0.8492       138\n",
      "\n",
      "\n",
      "Test Performance Summary:\n",
      "Decision Tree: {'Accuracy': 0.8478260869565217, 'Precision': 0.8475566138499377, 'Recall': 0.8478260869565217, 'F1-score': 0.8465217150867138}\n",
      "Logistic Regression: {'Accuracy': 0.8333333333333334, 'Precision': 0.8353715225631123, 'Recall': 0.8333333333333334, 'F1-score': 0.8339544513457557}\n",
      "SVM: {'Accuracy': 0.8405797101449275, 'Precision': 0.8435154217762912, 'Recall': 0.8405797101449275, 'F1-score': 0.8413355822088956}\n",
      "Random Forest: {'Accuracy': 0.855072463768116, 'Precision': 0.8545548654244307, 'Recall': 0.855072463768116, 'F1-score': 0.8546279981586424}\n",
      "Naive Bayes: {'Accuracy': 0.8115942028985508, 'Precision': 0.8131695022054191, 'Recall': 0.8115942028985508, 'F1-score': 0.8077725842960873}\n",
      "MLP: {'Accuracy': 0.782608695652174, 'Precision': 0.7810671936758893, 'Recall': 0.782608695652174, 'F1-score': 0.7802924287024079}\n",
      "LDA: {'Accuracy': 0.8478260869565217, 'Precision': 0.865574459147238, 'Recall': 0.8478260869565217, 'F1-score': 0.849188597754103}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gehongfei/anaconda3/envs/TabGNN/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T22:10:25.821826Z",
     "start_time": "2025-02-21T22:10:25.652837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用随机森林模型进行训练，并生成测试集上的分类报告\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# 输出测试集上的分类报告\n",
    "print(\"Test Set Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, digits=4))\n",
    "\n",
    "\n",
    "# 为了便于比较，将预测结果转换为与 y_test 相同的索引\n",
    "y_pred_rf_series = pd.Series(y_pred_rf, index=y_test.index)\n",
    "\n",
    "# 统计各类样本的索引集合\n",
    "tp_idx = y_test[(y_test == 1) & (y_pred_rf_series == 1)].index.tolist()\n",
    "tn_idx = y_test[(y_test == 0) & (y_pred_rf_series == 0)].index.tolist()\n",
    "fp_idx = y_test[(y_test == 0) & (y_pred_rf_series == 1)].index.tolist()\n",
    "fn_idx = y_test[(y_test == 1) & (y_pred_rf_series == 0)].index.tolist()\n",
    "\n",
    "print(\"True Positives (1被分为1):\", tp_idx)\n",
    "print(\"True Negatives (0被分为0):\", tn_idx)\n",
    "print(\"False Positives (0被分为1):\", fp_idx)\n",
    "print(\"False Negatives (1被分为0):\", fn_idx)\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# 将四个列表合并为一个列表\n",
    "error_idx_list = [tp_idx, tn_idx, fp_idx, fn_idx]\n",
    "\n",
    "# 定义要保存的文件名\n",
    "filename = \"AUS-RF.json\"\n",
    "\n",
    "# 保存为 JSON 文件\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump(error_idx_list, f, indent=4)\n",
    "\n",
    "print(f\"索引集合已保存至 {filename}\")\n",
    "\n",
    "\n",
    "# 如果只需要错误样本（FP和FN）的索引集合，可以合并如下：\n",
    "error_idx = {\n",
    "    \"[0,1]\": fp_idx,  # 真实为0，但预测为1\n",
    "    \"[1,0]\": fn_idx   # 真实为1，但预测为0\n",
    "}\n",
    "print(\"错误样本的索引集合:\", error_idx)\n",
    "\n",
    "# 读取 JSON 文件\n",
    "with open(filename, \"r\") as f:\n",
    "    loaded_error_idx_list = json.load(f)\n",
    "\n",
    "print(\"加载的索引集合:\", loaded_error_idx_list)\n"
   ],
   "id": "71808eafd60f921e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8588    0.8902    0.8743        82\n",
      "           1     0.8302    0.7857    0.8073        56\n",
      "\n",
      "    accuracy                         0.8478       138\n",
      "   macro avg     0.8445    0.8380    0.8408       138\n",
      "weighted avg     0.8472    0.8478    0.8471       138\n",
      "\n",
      "True Positives (1被分为1): [497, 405, 28, 685, 65, 321, 182, 210, 278, 599, 601, 55, 11, 539, 204, 412, 56, 314, 264, 603, 621, 235, 671, 320, 120, 380, 39, 360, 220, 518, 292, 624, 29, 42, 145, 487, 662, 499, 174, 286, 227, 420, 158, 163]\n",
      "True Negatives (0被分为0): [431, 81, 164, 425, 24, 645, 18, 133, 501, 86, 131, 669, 629, 310, 327, 51, 208, 631, 135, 534, 356, 569, 432, 60, 677, 244, 281, 31, 335, 381, 527, 576, 611, 296, 652, 353, 218, 90, 199, 357, 72, 556, 250, 110, 595, 657, 354, 192, 493, 82, 212, 284, 289, 109, 165, 209, 377, 54, 552, 318, 136, 6, 515, 608, 92, 606, 640, 686, 260, 61, 554, 132, 404]\n",
      "False Positives (0被分为1): [522, 681, 341, 586, 396, 482, 43, 213, 467]\n",
      "False Negatives (1被分为0): [256, 390, 674, 338, 101, 211, 500, 526, 257, 155, 249, 572]\n",
      "索引集合已保存至 AUS-RF.json\n",
      "错误样本的索引集合: {'[0,1]': [522, 681, 341, 586, 396, 482, 43, 213, 467], '[1,0]': [256, 390, 674, 338, 101, 211, 500, 526, 257, 155, 249, 572]}\n",
      "加载的索引集合: [[497, 405, 28, 685, 65, 321, 182, 210, 278, 599, 601, 55, 11, 539, 204, 412, 56, 314, 264, 603, 621, 235, 671, 320, 120, 380, 39, 360, 220, 518, 292, 624, 29, 42, 145, 487, 662, 499, 174, 286, 227, 420, 158, 163], [431, 81, 164, 425, 24, 645, 18, 133, 501, 86, 131, 669, 629, 310, 327, 51, 208, 631, 135, 534, 356, 569, 432, 60, 677, 244, 281, 31, 335, 381, 527, 576, 611, 296, 652, 353, 218, 90, 199, 357, 72, 556, 250, 110, 595, 657, 354, 192, 493, 82, 212, 284, 289, 109, 165, 209, 377, 54, 552, 318, 136, 6, 515, 608, 92, 606, 640, 686, 260, 61, 554, 132, 404], [522, 681, 341, 586, 396, 482, 43, 213, 467], [256, 390, 674, 338, 101, 211, 500, 526, 257, 155, 249, 572]]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5acf4c05cdf9d284"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
